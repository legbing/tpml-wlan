{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>node_type</th>\n",
       "      <th>x(m)</th>\n",
       "      <th>y(m)</th>\n",
       "      <th>primary_channel</th>\n",
       "      <th>min_channel_allowed</th>\n",
       "      <th>max_channel_allowed</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>SINR</th>\n",
       "      <th>throughput</th>\n",
       "      <th>average_airtime</th>\n",
       "      <th>average_interference</th>\n",
       "      <th>wlan_code_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-58.226667</td>\n",
       "      <td>29.62</td>\n",
       "      <td>111.77</td>\n",
       "      <td>95.745</td>\n",
       "      <td>-107.399091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>10.8079</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-65.370000</td>\n",
       "      <td>29.36</td>\n",
       "      <td>5.79</td>\n",
       "      <td>95.745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9627</td>\n",
       "      <td>4.1427</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-65.350000</td>\n",
       "      <td>29.48</td>\n",
       "      <td>6.11</td>\n",
       "      <td>95.745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13.7849</td>\n",
       "      <td>16.7538</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-61.410000</td>\n",
       "      <td>31.97</td>\n",
       "      <td>6.91</td>\n",
       "      <td>95.745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6.7112</td>\n",
       "      <td>1.7487</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-63.520000</td>\n",
       "      <td>31.26</td>\n",
       "      <td>9.99</td>\n",
       "      <td>95.745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87904</th>\n",
       "      <td>87904</td>\n",
       "      <td>1</td>\n",
       "      <td>22.4444</td>\n",
       "      <td>46.9925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-68.170000</td>\n",
       "      <td>11.06</td>\n",
       "      <td>3.30</td>\n",
       "      <td>79.220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87905</th>\n",
       "      <td>87905</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0104</td>\n",
       "      <td>51.2173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-52.610000</td>\n",
       "      <td>32.17</td>\n",
       "      <td>43.24</td>\n",
       "      <td>79.220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87906</th>\n",
       "      <td>87906</td>\n",
       "      <td>1</td>\n",
       "      <td>34.1608</td>\n",
       "      <td>48.5987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-59.770000</td>\n",
       "      <td>24.86</td>\n",
       "      <td>37.02</td>\n",
       "      <td>79.220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87907</th>\n",
       "      <td>87907</td>\n",
       "      <td>1</td>\n",
       "      <td>31.8977</td>\n",
       "      <td>54.3313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-60.700000</td>\n",
       "      <td>23.68</td>\n",
       "      <td>36.86</td>\n",
       "      <td>79.220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87908</th>\n",
       "      <td>87908</td>\n",
       "      <td>1</td>\n",
       "      <td>29.5763</td>\n",
       "      <td>58.3586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-68.610000</td>\n",
       "      <td>16.89</td>\n",
       "      <td>31.41</td>\n",
       "      <td>79.220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87909 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  node_type     x(m)     y(m)  primary_channel  \\\n",
       "0               0          0  10.0000  10.0000                4   \n",
       "1               1          1   0.0713  10.8079                4   \n",
       "2               2          1   1.9627   4.1427                4   \n",
       "3               3          1  13.7849  16.7538                4   \n",
       "4               4          1   6.7112   1.7487                4   \n",
       "...           ...        ...      ...      ...              ...   \n",
       "87904       87904          1  22.4444  46.9925                0   \n",
       "87905       87905          1  32.0104  51.2173                0   \n",
       "87906       87906          1  34.1608  48.5987                0   \n",
       "87907       87907          1  31.8977  54.3313                0   \n",
       "87908       87908          1  29.5763  58.3586                0   \n",
       "\n",
       "       min_channel_allowed  max_channel_allowed       RSSI   SINR  throughput  \\\n",
       "0                        4                    5 -58.226667  29.62      111.77   \n",
       "1                        4                    5 -65.370000  29.36        5.79   \n",
       "2                        4                    5 -65.350000  29.48        6.11   \n",
       "3                        4                    5 -61.410000  31.97        6.91   \n",
       "4                        4                    5 -63.520000  31.26        9.99   \n",
       "...                    ...                  ...        ...    ...         ...   \n",
       "87904                    0                    7 -68.170000  11.06        3.30   \n",
       "87905                    0                    7 -52.610000  32.17       43.24   \n",
       "87906                    0                    7 -59.770000  24.86       37.02   \n",
       "87907                    0                    7 -60.700000  23.68       36.86   \n",
       "87908                    0                    7 -68.610000  16.89       31.41   \n",
       "\n",
       "       average_airtime  average_interference  wlan_code_index  \n",
       "0               95.745           -107.399091                0  \n",
       "1               95.745              0.000000                0  \n",
       "2               95.745              0.000000                0  \n",
       "3               95.745              0.000000                0  \n",
       "4               95.745              0.000000                0  \n",
       "...                ...                   ...              ...  \n",
       "87904           79.220              0.000000                9  \n",
       "87905           79.220              0.000000                9  \n",
       "87906           79.220              0.000000                9  \n",
       "87907           79.220              0.000000                9  \n",
       "87908           79.220              0.000000                9  \n",
       "\n",
       "[87909 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\\\PIL\\\\fin-dataset_2.csv\")\n",
    "df\n",
    "df.drop(['node_code'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.12.0+cu113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y=df['throughput'].values #throughput \n",
    "\n",
    "\n",
    "x=df[[  'x(m)', 'y(m)',\n",
    "       'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'RSSI',\n",
    "       'SINR', 'average_airtime', 'average_interference',\n",
    "       'wlan_code_index']].values\n",
    "       \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=101)\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70327, 10]) torch.Size([70327])\n",
      "torch.Size([17582, 10]) torch.Size([17582])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "# default collate is to allow me to send the batches to the gpu after dataloader since \n",
    "#doesnt have an attribute called to_device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#X_train.to_numpy()\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_train\n",
    "y_train = torch.squeeze(torch.from_numpy(y_train).float())\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.squeeze(torch.from_numpy(y_test).float())\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "y_test\n",
    "batch_size=512\n",
    "\n",
    "train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = True,collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the appropriate NN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_features):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1=nn.Linear(n_features,384)\n",
    "        self.fc2=nn.Linear(384,384)\n",
    "        self.fc3=nn.Linear(384,384)\n",
    "        self.fc4=nn.Linear(384,384)\n",
    "        self.fc5=nn.Linear(384,384)\n",
    "        self.fc6=nn.Linear(384,1)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        x=F.relu(self.fc4(x))\n",
    "        x=F.relu(self.fc5(x))\n",
    "        x=self.fc6(x)\n",
    "        return x\n",
    "net = Net(X_train.shape[1]) #instantiate the NN\n",
    "err_fn=nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on cuda for better performance\n",
    "\n",
    "#train_loader=train_loader.to(device)\n",
    "#test_loader=test_loader.to(device)\n",
    "net=net.to(device)\n",
    "err_fn=err_fn.to(device)\n",
    "#optimizer=optimizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 967.793091  [    0/70327]\n",
      "loss: 471.563293  [51200/70327]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([183])) that is different to the input size (torch.Size([183, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1045.117432  [    0/70327]\n",
      "loss: 740.569946  [51200/70327]\n",
      "loss: 841.907166  [    0/70327]\n",
      "loss: 959.958008  [51200/70327]\n",
      "loss: 793.673035  [    0/70327]\n",
      "loss: 822.802490  [51200/70327]\n",
      "loss: 781.035767  [    0/70327]\n",
      "loss: 874.222046  [51200/70327]\n",
      "loss: 561.860718  [    0/70327]\n",
      "loss: 789.432495  [51200/70327]\n",
      "loss: 564.615967  [    0/70327]\n",
      "loss: 1571.572632  [51200/70327]\n",
      "loss: 563.163452  [    0/70327]\n",
      "loss: 856.534180  [51200/70327]\n",
      "loss: 1089.685059  [    0/70327]\n",
      "loss: 787.177795  [51200/70327]\n",
      "loss: 986.074951  [    0/70327]\n",
      "loss: 995.072266  [51200/70327]\n",
      "loss: 1496.528564  [    0/70327]\n",
      "loss: 357.177612  [51200/70327]\n",
      "loss: 771.809326  [    0/70327]\n",
      "loss: 598.800781  [51200/70327]\n",
      "loss: 861.836304  [    0/70327]\n",
      "loss: 433.789001  [51200/70327]\n",
      "loss: 1144.667847  [    0/70327]\n",
      "loss: 777.454956  [51200/70327]\n",
      "loss: 647.253662  [    0/70327]\n",
      "loss: 973.617188  [51200/70327]\n",
      "loss: 1002.931396  [    0/70327]\n",
      "loss: 769.452637  [51200/70327]\n",
      "loss: 745.326477  [    0/70327]\n",
      "loss: 603.919128  [51200/70327]\n",
      "loss: 1004.677856  [    0/70327]\n",
      "loss: 1125.999634  [51200/70327]\n",
      "loss: 605.314514  [    0/70327]\n",
      "loss: 898.873901  [51200/70327]\n",
      "loss: 892.570740  [    0/70327]\n",
      "loss: 1205.928833  [51200/70327]\n",
      "loss: 609.347046  [    0/70327]\n",
      "loss: 1006.282654  [51200/70327]\n",
      "loss: 759.267334  [    0/70327]\n",
      "loss: 685.364014  [51200/70327]\n",
      "loss: 700.401367  [    0/70327]\n",
      "loss: 845.116699  [51200/70327]\n",
      "loss: 730.465210  [    0/70327]\n",
      "loss: 503.228180  [51200/70327]\n",
      "loss: 1041.090332  [    0/70327]\n",
      "loss: 662.811523  [51200/70327]\n",
      "loss: 697.079834  [    0/70327]\n",
      "loss: 654.585205  [51200/70327]\n",
      "loss: 491.391663  [    0/70327]\n",
      "loss: 565.485474  [51200/70327]\n",
      "loss: 702.845581  [    0/70327]\n",
      "loss: 676.749084  [51200/70327]\n",
      "loss: 553.115845  [    0/70327]\n",
      "loss: 670.016357  [51200/70327]\n",
      "loss: 753.224365  [    0/70327]\n",
      "loss: 545.873779  [51200/70327]\n",
      "loss: 742.878967  [    0/70327]\n",
      "loss: 839.476929  [51200/70327]\n",
      "loss: 682.694885  [    0/70327]\n",
      "loss: 533.968445  [51200/70327]\n",
      "loss: 977.966858  [    0/70327]\n",
      "loss: 1135.135742  [51200/70327]\n",
      "loss: 823.935547  [    0/70327]\n",
      "loss: 896.697449  [51200/70327]\n",
      "loss: 712.680969  [    0/70327]\n",
      "loss: 447.531830  [51200/70327]\n",
      "loss: 875.486694  [    0/70327]\n",
      "loss: 983.163086  [51200/70327]\n",
      "loss: 1064.791992  [    0/70327]\n",
      "loss: 745.429810  [51200/70327]\n",
      "loss: 637.692383  [    0/70327]\n",
      "loss: 773.680298  [51200/70327]\n",
      "loss: 1107.710449  [    0/70327]\n",
      "loss: 721.829468  [51200/70327]\n",
      "loss: 771.731750  [    0/70327]\n",
      "loss: 943.657349  [51200/70327]\n",
      "loss: 1243.621460  [    0/70327]\n",
      "loss: 768.347107  [51200/70327]\n",
      "loss: 636.829468  [    0/70327]\n",
      "loss: 815.065186  [51200/70327]\n",
      "loss: 787.556030  [    0/70327]\n",
      "loss: 595.086182  [51200/70327]\n",
      "loss: 684.128235  [    0/70327]\n",
      "loss: 729.090210  [51200/70327]\n",
      "loss: 864.408691  [    0/70327]\n",
      "loss: 717.772888  [51200/70327]\n",
      "loss: 790.546021  [    0/70327]\n",
      "loss: 704.017822  [51200/70327]\n",
      "loss: 784.970337  [    0/70327]\n",
      "loss: 1144.135010  [51200/70327]\n",
      "loss: 699.632324  [    0/70327]\n",
      "loss: 471.486816  [51200/70327]\n",
      "loss: 762.028931  [    0/70327]\n",
      "loss: 632.250122  [51200/70327]\n",
      "loss: 668.939392  [    0/70327]\n",
      "loss: 1027.281006  [51200/70327]\n",
      "loss: 859.275391  [    0/70327]\n",
      "loss: 886.662170  [51200/70327]\n",
      "loss: 1124.854858  [    0/70327]\n",
      "loss: 908.441162  [51200/70327]\n",
      "loss: 700.465759  [    0/70327]\n",
      "loss: 851.736511  [51200/70327]\n",
      "loss: 744.299744  [    0/70327]\n",
      "loss: 607.740601  [51200/70327]\n",
      "loss: 635.375183  [    0/70327]\n",
      "loss: 609.541992  [51200/70327]\n",
      "loss: 796.663940  [    0/70327]\n",
      "loss: 503.920532  [51200/70327]\n",
      "loss: 777.949219  [    0/70327]\n",
      "loss: 790.897278  [51200/70327]\n",
      "loss: 991.494507  [    0/70327]\n",
      "loss: 862.426025  [51200/70327]\n",
      "loss: 666.047241  [    0/70327]\n",
      "loss: 1051.935425  [51200/70327]\n",
      "loss: 676.622314  [    0/70327]\n",
      "loss: 1043.040405  [51200/70327]\n",
      "loss: 688.648926  [    0/70327]\n",
      "loss: 587.912964  [51200/70327]\n",
      "loss: 863.893433  [    0/70327]\n",
      "loss: 643.826965  [51200/70327]\n",
      "loss: 790.570679  [    0/70327]\n",
      "loss: 798.666748  [51200/70327]\n",
      "loss: 917.141052  [    0/70327]\n",
      "loss: 982.903870  [51200/70327]\n",
      "loss: 1155.124878  [    0/70327]\n",
      "loss: 711.383911  [51200/70327]\n",
      "loss: 683.026978  [    0/70327]\n",
      "loss: 721.462341  [51200/70327]\n",
      "loss: 1091.422363  [    0/70327]\n",
      "loss: 1053.840332  [51200/70327]\n",
      "loss: 553.936707  [    0/70327]\n",
      "loss: 856.081848  [51200/70327]\n",
      "loss: 976.357300  [    0/70327]\n",
      "loss: 749.265503  [51200/70327]\n",
      "loss: 539.781616  [    0/70327]\n",
      "loss: 1006.567871  [51200/70327]\n",
      "loss: 551.861450  [    0/70327]\n",
      "loss: 1138.601440  [51200/70327]\n",
      "loss: 614.412720  [    0/70327]\n",
      "loss: 751.117798  [51200/70327]\n",
      "loss: 1362.535400  [    0/70327]\n",
      "loss: 653.273438  [51200/70327]\n",
      "loss: 493.511108  [    0/70327]\n",
      "loss: 818.156677  [51200/70327]\n",
      "loss: 721.616333  [    0/70327]\n",
      "loss: 733.263123  [51200/70327]\n",
      "loss: 964.502991  [    0/70327]\n",
      "loss: 897.728760  [51200/70327]\n",
      "loss: 964.939575  [    0/70327]\n",
      "loss: 1313.270508  [51200/70327]\n",
      "loss: 685.800964  [    0/70327]\n",
      "loss: 834.688843  [51200/70327]\n",
      "loss: 1005.901245  [    0/70327]\n",
      "loss: 424.752289  [51200/70327]\n",
      "loss: 912.487488  [    0/70327]\n",
      "loss: 816.909546  [51200/70327]\n",
      "loss: 797.331787  [    0/70327]\n",
      "loss: 622.917847  [51200/70327]\n",
      "loss: 666.470459  [    0/70327]\n",
      "loss: 699.493835  [51200/70327]\n",
      "loss: 821.251221  [    0/70327]\n",
      "loss: 823.249878  [51200/70327]\n",
      "loss: 606.772583  [    0/70327]\n",
      "loss: 610.533447  [51200/70327]\n",
      "loss: 814.955078  [    0/70327]\n",
      "loss: 445.685883  [51200/70327]\n",
      "loss: 612.152527  [    0/70327]\n",
      "loss: 823.933228  [51200/70327]\n",
      "loss: 649.178955  [    0/70327]\n",
      "loss: 1206.020142  [51200/70327]\n",
      "loss: 694.641113  [    0/70327]\n",
      "loss: 783.656067  [51200/70327]\n",
      "loss: 950.781128  [    0/70327]\n",
      "loss: 774.971802  [51200/70327]\n",
      "loss: 1095.770996  [    0/70327]\n",
      "loss: 530.334351  [51200/70327]\n",
      "loss: 650.796753  [    0/70327]\n",
      "loss: 976.661926  [51200/70327]\n",
      "loss: 902.150391  [    0/70327]\n",
      "loss: 925.449707  [51200/70327]\n",
      "loss: 923.543945  [    0/70327]\n",
      "loss: 769.697510  [51200/70327]\n",
      "loss: 1018.346863  [    0/70327]\n",
      "loss: 862.817749  [51200/70327]\n",
      "loss: 690.821716  [    0/70327]\n",
      "loss: 1008.452942  [51200/70327]\n",
      "loss: 923.224548  [    0/70327]\n",
      "loss: 596.602539  [51200/70327]\n",
      "loss: 556.565796  [    0/70327]\n",
      "loss: 1409.874390  [51200/70327]\n",
      "loss: 778.650391  [    0/70327]\n",
      "loss: 620.714172  [51200/70327]\n",
      "loss: 755.089966  [    0/70327]\n",
      "loss: 839.287354  [51200/70327]\n",
      "loss: 1125.618408  [    0/70327]\n",
      "loss: 811.337036  [51200/70327]\n",
      "loss: 740.133545  [    0/70327]\n",
      "loss: 1285.560059  [51200/70327]\n",
      "loss: 853.835571  [    0/70327]\n",
      "loss: 758.707581  [51200/70327]\n",
      "loss: 848.901367  [    0/70327]\n",
      "loss: 820.815430  [51200/70327]\n",
      "loss: 781.583374  [    0/70327]\n",
      "loss: 783.311157  [51200/70327]\n",
      "loss: 840.856628  [    0/70327]\n",
      "loss: 920.115112  [51200/70327]\n",
      "loss: 702.582214  [    0/70327]\n",
      "loss: 933.293335  [51200/70327]\n",
      "loss: 544.211548  [    0/70327]\n",
      "loss: 770.296875  [51200/70327]\n",
      "loss: 696.322876  [    0/70327]\n",
      "loss: 1037.738281  [51200/70327]\n",
      "loss: 704.423950  [    0/70327]\n",
      "loss: 564.375366  [51200/70327]\n",
      "loss: 1170.512451  [    0/70327]\n",
      "loss: 623.006714  [51200/70327]\n",
      "loss: 1045.845215  [    0/70327]\n",
      "loss: 754.715454  [51200/70327]\n",
      "loss: 759.954834  [    0/70327]\n",
      "loss: 907.258911  [51200/70327]\n",
      "loss: 588.485352  [    0/70327]\n",
      "loss: 703.804565  [51200/70327]\n",
      "loss: 952.456970  [    0/70327]\n",
      "loss: 578.458862  [51200/70327]\n",
      "loss: 878.326599  [    0/70327]\n",
      "loss: 487.523804  [51200/70327]\n",
      "loss: 369.335663  [    0/70327]\n",
      "loss: 672.430908  [51200/70327]\n",
      "loss: 763.295776  [    0/70327]\n",
      "loss: 780.962402  [51200/70327]\n",
      "loss: 607.239746  [    0/70327]\n",
      "loss: 902.585327  [51200/70327]\n",
      "loss: 607.911316  [    0/70327]\n",
      "loss: 1014.875671  [51200/70327]\n",
      "loss: 612.288940  [    0/70327]\n",
      "loss: 855.402344  [51200/70327]\n",
      "loss: 666.693848  [    0/70327]\n",
      "loss: 560.670898  [51200/70327]\n",
      "loss: 601.768677  [    0/70327]\n",
      "loss: 508.287048  [51200/70327]\n",
      "loss: 982.496948  [    0/70327]\n",
      "loss: 757.905518  [51200/70327]\n",
      "loss: 702.975342  [    0/70327]\n",
      "loss: 1296.838989  [51200/70327]\n",
      "loss: 828.319885  [    0/70327]\n",
      "loss: 601.797791  [51200/70327]\n",
      "loss: 930.597412  [    0/70327]\n",
      "loss: 631.435608  [51200/70327]\n",
      "loss: 923.889526  [    0/70327]\n",
      "loss: 1183.148193  [51200/70327]\n",
      "loss: 807.505737  [    0/70327]\n",
      "loss: 531.556763  [51200/70327]\n",
      "loss: 983.830444  [    0/70327]\n",
      "loss: 851.674561  [51200/70327]\n",
      "loss: 794.487122  [    0/70327]\n",
      "loss: 695.554321  [51200/70327]\n",
      "loss: 753.511597  [    0/70327]\n",
      "loss: 823.810974  [51200/70327]\n",
      "loss: 672.178833  [    0/70327]\n",
      "loss: 554.796021  [51200/70327]\n",
      "loss: 713.272949  [    0/70327]\n",
      "loss: 841.962036  [51200/70327]\n",
      "loss: 533.871765  [    0/70327]\n",
      "loss: 743.606445  [51200/70327]\n",
      "loss: 1093.320923  [    0/70327]\n",
      "loss: 657.213501  [51200/70327]\n",
      "loss: 692.834961  [    0/70327]\n",
      "loss: 597.312134  [51200/70327]\n",
      "loss: 1020.076721  [    0/70327]\n",
      "loss: 1347.037842  [51200/70327]\n",
      "loss: 705.651978  [    0/70327]\n",
      "loss: 888.175659  [51200/70327]\n",
      "loss: 879.551147  [    0/70327]\n",
      "loss: 904.542603  [51200/70327]\n",
      "loss: 570.388062  [    0/70327]\n",
      "loss: 836.270874  [51200/70327]\n",
      "loss: 809.679810  [    0/70327]\n",
      "loss: 826.013428  [51200/70327]\n",
      "loss: 689.415466  [    0/70327]\n",
      "loss: 694.998596  [51200/70327]\n",
      "loss: 834.558960  [    0/70327]\n",
      "loss: 901.017456  [51200/70327]\n",
      "loss: 633.570618  [    0/70327]\n",
      "loss: 1138.386108  [51200/70327]\n",
      "loss: 818.452942  [    0/70327]\n",
      "loss: 827.423462  [51200/70327]\n",
      "loss: 769.397949  [    0/70327]\n",
      "loss: 686.144470  [51200/70327]\n",
      "loss: 578.306885  [    0/70327]\n",
      "loss: 884.893005  [51200/70327]\n",
      "loss: 539.860962  [    0/70327]\n",
      "loss: 580.934692  [51200/70327]\n",
      "loss: 380.324585  [    0/70327]\n",
      "loss: 769.493896  [51200/70327]\n",
      "loss: 858.614807  [    0/70327]\n",
      "loss: 692.063354  [51200/70327]\n",
      "loss: 669.196472  [    0/70327]\n",
      "loss: 608.777405  [51200/70327]\n",
      "loss: 733.170532  [    0/70327]\n",
      "loss: 826.679688  [51200/70327]\n",
      "loss: 864.980164  [    0/70327]\n",
      "loss: 892.355103  [51200/70327]\n",
      "loss: 737.267822  [    0/70327]\n",
      "loss: 1008.026733  [51200/70327]\n",
      "loss: 732.590210  [    0/70327]\n",
      "loss: 832.397522  [51200/70327]\n",
      "loss: 734.598877  [    0/70327]\n",
      "loss: 704.713379  [51200/70327]\n",
      "loss: 612.664124  [    0/70327]\n",
      "loss: 662.601746  [51200/70327]\n",
      "loss: 968.268677  [    0/70327]\n",
      "loss: 829.917053  [51200/70327]\n",
      "loss: 737.907776  [    0/70327]\n",
      "loss: 590.278687  [51200/70327]\n",
      "loss: 1371.243408  [    0/70327]\n",
      "loss: 927.640503  [51200/70327]\n",
      "loss: 749.079834  [    0/70327]\n",
      "loss: 650.858398  [51200/70327]\n",
      "loss: 767.425903  [    0/70327]\n",
      "loss: 937.525757  [51200/70327]\n",
      "loss: 786.130493  [    0/70327]\n",
      "loss: 551.963928  [51200/70327]\n",
      "loss: 495.761505  [    0/70327]\n",
      "loss: 719.656738  [51200/70327]\n",
      "loss: 1043.979004  [    0/70327]\n",
      "loss: 662.059143  [51200/70327]\n",
      "loss: 827.310791  [    0/70327]\n",
      "loss: 1037.782227  [51200/70327]\n",
      "loss: 824.686157  [    0/70327]\n",
      "loss: 1027.990723  [51200/70327]\n",
      "loss: 601.828979  [    0/70327]\n",
      "loss: 711.850159  [51200/70327]\n",
      "loss: 771.636475  [    0/70327]\n",
      "loss: 961.805542  [51200/70327]\n",
      "loss: 808.728271  [    0/70327]\n",
      "loss: 777.925659  [51200/70327]\n",
      "loss: 848.978271  [    0/70327]\n",
      "loss: 1026.740967  [51200/70327]\n",
      "loss: 755.789673  [    0/70327]\n",
      "loss: 476.475769  [51200/70327]\n",
      "loss: 654.367859  [    0/70327]\n",
      "loss: 640.414551  [51200/70327]\n",
      "loss: 628.631104  [    0/70327]\n",
      "loss: 461.653503  [51200/70327]\n",
      "loss: 1061.182007  [    0/70327]\n",
      "loss: 749.807556  [51200/70327]\n",
      "loss: 759.673401  [    0/70327]\n",
      "loss: 1336.243042  [51200/70327]\n",
      "loss: 930.710144  [    0/70327]\n",
      "loss: 782.668701  [51200/70327]\n",
      "loss: 869.982605  [    0/70327]\n",
      "loss: 1026.500610  [51200/70327]\n",
      "loss: 644.575256  [    0/70327]\n",
      "loss: 693.508179  [51200/70327]\n",
      "loss: 846.631592  [    0/70327]\n",
      "loss: 856.313965  [51200/70327]\n",
      "loss: 675.829224  [    0/70327]\n",
      "loss: 1023.309570  [51200/70327]\n",
      "loss: 839.025513  [    0/70327]\n",
      "loss: 781.014160  [51200/70327]\n",
      "loss: 720.706970  [    0/70327]\n",
      "loss: 828.850525  [51200/70327]\n",
      "loss: 649.567383  [    0/70327]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shrey\\OneDrive\\Documents\\.PES\\PIL\\Model\\NN\\trial2.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000006?line=5'>6</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000006?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000006?line=7'>8</a>\u001b[0m      \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000006?line=8'>9</a>\u001b[0m           pred\u001b[39m=\u001b[39mnet(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000006?line=9'>10</a>\u001b[0m           loss\u001b[39m=\u001b[39merr_fn(pred,y)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32mc:\\Users\\shrey\\OneDrive\\Documents\\.PES\\PIL\\Model\\NN\\trial2.ipynb Cell 4'\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000003?line=16'>17</a>\u001b[0m train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000003?line=17'>18</a>\u001b[0m test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(X_test, y_test)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000003?line=18'>19</a>\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train, batch_size \u001b[39m=\u001b[39m batch_size, shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mtuple\u001b[39m(x_\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m default_collate(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shrey/OneDrive/Documents/.PES/PIL/Model/NN/trial2.ipynb#ch0000003?line=19'>20</a>\u001b[0m test_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(test, batch_size \u001b[39m=\u001b[39m batch_size, shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,collate_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mtuple\u001b[39m(x_\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m default_collate(x)))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m    142\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    143\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs=1000\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "size = len(train_loader.dataset)\n",
    "for epoch in range(num_epochs):\n",
    "     for batch, (X, y) in enumerate(train_loader):\n",
    "          pred=net(X)\n",
    "          loss=err_fn(pred,y)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          loss_list.append(loss.item())\n",
    "          iteration_list.append(count)\n",
    "          if batch % 100 == 0:\n",
    "               loss, current = loss.item(), batch * len(X)\n",
    "              # tstsize = len(test_loader.dataset)\n",
    "              # num_batches = len(test_loader)\n",
    "              # test_loss, correct = 0, 0\n",
    "              # for X, y in test_loader:\n",
    "            #\n",
    "              #      pred = net(X)\n",
    "              #      test_loss += err_fn(pred, y).item()\n",
    "              #      for i in range(len(y)):\n",
    "              #           correct += (((abs(pred[i] - y[i])/y[i]) * 100) < 10).type(torch.float).item()\n",
    "              # test_loss /= num_batches\n",
    "              # correct /= tstsize          \n",
    "            \n",
    "               \n",
    "               print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "               #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm\n",
    "tstsize = len(test_loader.dataset)\n",
    "num_batches = len(test_loader)\n",
    "test_loss, correct = 0, 0\n",
    "for X, y in test_loader:\n",
    "\n",
    "     pred = net(X)\n",
    "     test_loss += err_fn(pred, y).item()\n",
    "     for i in range(len(y)):\n",
    "          correct += (((abs(pred[i] - y[i])/y[i]) * 100) < 10).type(torch.float).item()\n",
    "test_loss /= num_batches\n",
    "correct /= tstsize          \n",
    "print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
