{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\\\PIL\\\\sta-int_map.csv\")\n",
    "df.iloc[75310,1:13].astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_map(dep):\n",
    "    ap_index = {}\n",
    "    df_1 = pd.read_csv(\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\\\PIL\\\\sta-int_map.csv\")\n",
    "    dep = df_1.loc[df_1['deployment'] == dep]\n",
    "    dep_reset = dep.reset_index(drop=True)\n",
    "    aps_in_dep = dep_reset[dep_reset[\"node_type\"] == 0]\n",
    "    ap_index = {}\n",
    "    for i in range(len(aps_in_dep)):\n",
    "        ap_index[i] = aps_in_dep.index[i]\n",
    "    aps = dep[dep[\"node_type\"] == 0]\n",
    "    #print(dep, aps, ap_index)\n",
    "    \n",
    "    #print(ap_index)\n",
    "    #print(\"Lwngth: \", len(dep_reset.loc[ap_index[i], \"int_map\"]))\n",
    "    for i in range(len(aps)-1):\n",
    "        #print(\"i: \", i)\n",
    "        for j in range(1, aps.index[i+1] - aps.index[i]):\n",
    "            #print(\"j: \", j)\n",
    "            for k in range(1, dep_reset.iloc[ap_index[i],1:13].astype(bool).sum(axis=0)):\n",
    "                # STA interference for columns where currently value = 0: df.iloc[i, j] = df.iloc[i-1, j]*distance of i from j/distance of i from i-1\n",
    "                # finding distance - with x (m) and y (m). Then, distance = sqrt(x^2 + y^2). distance(i, j) = sqrt((xj-xi)^2 + (yj-yi)^2)\n",
    "                \n",
    "                if(dep_reset.iloc[ap_index[i] + j, k] == 0):\n",
    "                    # print(\"k: \", k)\n",
    "                    # print(dep_reset.iloc[ap_index[i], k])\n",
    "                    # print(dep_reset.iloc[ap_index[i] + j, 17]) \n",
    "                    # print(dep_reset.iloc[ap_index[i] + j, 18])\n",
    "                    # print(dep_reset[\"x(m)\"][ap_index[k-1]])\n",
    "                    \n",
    "                    #df.iloc[out2.index[i] + j, k] = df.iloc[out2.index[i], k]*math.sqrt((df.iloc[out2.index[i] + j, 17] - out[\"x(m)\"][i+k])**2 + (df.iloc[out2.index[i] + j, 17] - out[\"y(m)\"][i+k])**2)/math.sqrt((out2[\"x(m)\"][i] - out[\"x(m)\"][i+k])**2 + (out2[\"y(m)\"][i] - out[\"y(m)\"][i+k])**2)\n",
    "                    dep_reset.iloc[ap_index[i] + j, k] = dep_reset.iloc[ap_index[i], k]*((dep_reset.iloc[ap_index[i] + j, 17] - dep_reset[\"x(m)\"][ap_index[0] + ap_index[k-1]])**2 + \n",
    "                    (dep_reset.iloc[ap_index[i] + j, 18] - dep_reset[\"y(m)\"][ap_index[0] + ap_index[k-1]])**2)/((dep_reset.iloc[ap_index[i], 17] - dep_reset[\"x(m)\"][ap_index[0] + ap_index[k-1]])**2 + (dep_reset.iloc[ap_index[i], 18] - dep_reset[\"y(m)\"][ap_index[0] + ap_index[k-1]])**2)\n",
    "\n",
    "    i = len(aps) - 1\n",
    "    for j in range(1, dep.index[len(dep)-1] - aps.index[i]+1):\n",
    "            #print(\"j: \", j)\n",
    "            for k in range(1, dep_reset.iloc[ap_index[i],1:13].astype(bool).sum(axis=0)):\n",
    "                # STA interference for columns where currently value = 0: df.iloc[i, j] = df.iloc[i-1, j]*distance of i from j/distance of i from i-1\n",
    "                # finding distance - with x (m) and y (m). Then, distance = sqrt(x^2 + y^2). distance(i, j) = sqrt((xj-xi)^2 + (yj-yi)^2)\n",
    "                \n",
    "                if(dep_reset.iloc[ap_index[i] + j, k] == 0):\n",
    "                    # print(\"Last part: \")\n",
    "                    # print(\"k: \", k)\n",
    "                    # print(dep_reset.iloc[ap_index[i], k])\n",
    "                    # print(dep_reset.iloc[ap_index[i] + j, 17]) \n",
    "                    # print(dep_reset.iloc[ap_index[i] + j, 18])\n",
    "                    # print(dep_reset[\"x(m)\"][ap_index[0] + ap_index[k-1]])\n",
    "                    # print(dep_reset.iloc[ap_index[i], 17] - dep_reset[\"x(m)\"][ap_index[0] + ap_index[k-1]])\n",
    "                    \n",
    "                    #df.iloc[out2.index[i] + j, k] = df.iloc[out2.index[i], k]*math.sqrt((df.iloc[out2.index[i] + j, 17] - out[\"x(m)\"][i+k])**2 + (df.iloc[out2.index[i] + j, 17] - out[\"y(m)\"][i+k])**2)/math.sqrt((out2[\"x(m)\"][i] - out[\"x(m)\"][i+k])**2 + (out2[\"y(m)\"][i] - out[\"y(m)\"][i+k])**2)\n",
    "                    dep_reset.iloc[ap_index[i] + j, k] = dep_reset.iloc[ap_index[i], k]*math.sqrt((dep_reset.iloc[ap_index[i] + j, 17] - dep_reset[\"x(m)\"][ap_index[0] + ap_index[k-1]])**2 + \n",
    "                    (dep_reset.iloc[ap_index[i] + j, 18] - dep_reset[\"y(m)\"][ap_index[0] + ap_index[k-1]])**2)/math.sqrt((dep_reset.iloc[ap_index[i], 17] - dep_reset[\"x(m)\"][ap_index[0] + ap_index[k-1]])**2 + (dep_reset.iloc[ap_index[i], 18] - dep_reset[\"y(m)\"][ap_index[0] + ap_index[k-1]])**2)\n",
    "\n",
    "    return dep_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = get_int_map(799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep.to_csv(\"trial2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating individual graphs\n",
    "# This assumes all APs and STAs are connected to each other\n",
    "def create_graph(split, split_y, deployment):\n",
    "    dep = get_int_map(deployment)\n",
    "    \n",
    "    dep_y = dep[\"throughput\"]\n",
    "    dep_x = dep[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)',\n",
    "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'RSSI', 'node_type',\n",
    "            'SINR', 'average_airtime', 'deployment']]\n",
    "    #print(dep_x)\n",
    "    dep_reset = dep.reset_index(drop=True)\n",
    "    ap_index = {}\n",
    "    out = dep_reset[dep_reset[\"node_type\"] == 0]\n",
    "    for i in range(len(out)):\n",
    "        ap_index[out.index[i]] = i\n",
    "    #print(ap_index)\n",
    "    node_features = dep_x.iloc[:,12:].values\n",
    "    #edge_features = dep.iloc[:,:12].values - here each node has been given an edge feature\n",
    "    # need to give each edge an edge feature\n",
    "    node_targets = dep_y.values\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    print(node_features.shape)\n",
    "    #edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
    "    node_targets = torch.tensor(node_targets, dtype=torch.float)\n",
    "    # Add edges here for each deployment\n",
    "    edges = []\n",
    "    edge_features = []\n",
    "    edge_index = []\n",
    "    for i in range(len(dep)):\n",
    "        for j in range(len(dep)):\n",
    "            if (i != j and (dep[\"node_type\"].iloc[i] == 0 and dep[\"node_type\"].iloc[j] == 0)) or (i !=j and (dep[\"node_type\"].iloc[i] == 1 and dep[\"node_type\"].iloc[j] == 0)):\n",
    "                edges.append([i,j])\n",
    "    #print(edges)\n",
    "    edges = torch.tensor(edges, dtype=torch.float)\n",
    "    #print(\"Edges: \", edges, edges.shape)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "    edge_index = edge_index.t().contiguous()\n",
    "    #print(edges.detach(), edges.shape)\n",
    "    \n",
    "    for i in range(edges.shape[0]):\n",
    "        #print(i)\n",
    "        edge_features.append(dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]])\n",
    "    edge_features = torch.tensor(edge_features, dtype=torch.float).unsqueeze(1)\n",
    "    #print(edge_features, edge_features.shape)\n",
    "    graph = {\n",
    "        \"edges\": edges,\n",
    "        \"edge_index\": edge_index,\n",
    "        \"node_features\": node_features,\n",
    "        \"edge_features\": edge_features,\n",
    "        \"node_targets\": node_targets\n",
    "    }\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 11])\n",
      "{'edges': tensor([[  0.,  15.],\n",
      "        [  0.,  27.],\n",
      "        [  0.,  45.],\n",
      "        ...,\n",
      "        [176., 135.],\n",
      "        [176., 150.],\n",
      "        [176., 163.]]), 'edge_index': tensor([[  0,   0,   0,  ..., 176, 176, 176],\n",
      "        [ 15,  27,  45,  ..., 135, 150, 163]]), 'node_features': tensor([[0.0000e+00, 1.0000e+01, 1.0000e+01,  ..., 2.9620e+01, 9.5745e+01,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 7.1300e-02, 1.0808e+01,  ..., 2.9360e+01, 9.5745e+01,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.9627e+00, 4.1427e+00,  ..., 2.9480e+01, 9.5745e+01,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [1.1000e+01, 6.6595e+01, 5.5642e+01,  ..., 3.5400e+01, 9.4855e+01,\n",
      "         0.0000e+00],\n",
      "        [1.1000e+01, 6.3075e+01, 4.3863e+01,  ..., 2.9680e+01, 9.4855e+01,\n",
      "         0.0000e+00],\n",
      "        [1.1000e+01, 6.8264e+01, 5.0447e+01,  ..., 5.0690e+01, 9.4855e+01,\n",
      "         0.0000e+00]]), 'edge_features': tensor([[10000.0000],\n",
      "        [  -79.3400],\n",
      "        [ -103.9600],\n",
      "        ...,\n",
      "        [ -119.4342],\n",
      "        [ -102.3334],\n",
      "        [  -75.2229]]), 'node_targets': tensor([1.1177e+02, 5.7900e+00, 6.1100e+00, 6.9100e+00, 9.9900e+00, 9.4400e+00,\n",
      "        7.2500e+00, 5.8800e+00, 7.3800e+00, 1.0910e+01, 1.0050e+01, 1.0260e+01,\n",
      "        7.9300e+00, 4.8800e+00, 8.9900e+00, 1.1111e+02, 8.1000e+00, 1.0440e+01,\n",
      "        9.2300e+00, 7.5700e+00, 8.4500e+00, 1.2210e+01, 1.2980e+01, 1.2360e+01,\n",
      "        6.9800e+00, 1.1830e+01, 1.0940e+01, 7.7720e+01, 2.3000e-01, 1.4600e+00,\n",
      "        7.9100e+00, 7.7600e+00, 5.3000e+00, 7.5300e+00, 7.3700e+00, 2.0000e+00,\n",
      "        7.9900e+00, 2.0700e+00, 4.7600e+00, 4.9900e+00, 7.6000e+00, 1.1500e+00,\n",
      "        2.3800e+00, 3.0000e+00, 4.2200e+00, 4.3050e+01, 3.4900e+00, 1.9800e+00,\n",
      "        2.4600e+00, 2.8000e+00, 3.1000e+00, 4.7200e+00, 4.9200e+00, 4.3000e+00,\n",
      "        3.4900e+00, 4.5800e+00, 2.8300e+00, 4.3800e+00, 7.9870e+01, 2.6900e+00,\n",
      "        6.4500e+00, 4.0700e+00, 5.6100e+00, 6.3700e+00, 6.9000e-01, 5.6100e+00,\n",
      "        1.6100e+00, 6.5300e+00, 5.3800e+00, 4.5300e+00, 3.8400e+00, 6.9900e+00,\n",
      "        5.9100e+00, 6.3700e+00, 1.6100e+00, 5.6100e+00, 5.7140e+01, 7.4500e+00,\n",
      "        6.9000e-01, 7.0700e+00, 5.5300e+00, 8.0000e-02, 7.8300e+00, 8.0000e-02,\n",
      "        3.3800e+00, 7.1400e+00, 2.0000e+00, 0.0000e+00, 4.0700e+00, 1.5000e-01,\n",
      "        3.3800e+00, 7.8300e+00, 0.0000e+00, 4.6000e-01, 3.7310e+01, 8.9100e+00,\n",
      "        8.0000e-02, 8.0000e-02, 8.4000e-01, 7.4500e+00, 7.3000e+00, 1.4500e+00,\n",
      "        2.6000e+00, 1.5000e-01, 5.4000e-01, 3.8000e-01, 7.5300e+00, 4.8080e+01,\n",
      "        2.9400e+00, 1.5700e+00, 5.6800e+00, 3.7600e+00, 4.0400e+00, 3.6300e+00,\n",
      "        2.5300e+00, 4.7200e+00, 3.6600e+00, 4.5300e+00, 1.9200e+00, 5.1500e+00,\n",
      "        3.9600e+00, 9.0930e+01, 9.3700e+00, 8.5200e+00, 9.4500e+00, 1.2520e+01,\n",
      "        5.3800e+00, 3.5300e+00, 1.4600e+00, 9.0600e+00, 7.8300e+00, 8.4000e-01,\n",
      "        1.1830e+01, 1.0060e+01, 1.0800e+00, 2.1430e+01, 3.5300e+00, 1.2300e+00,\n",
      "        1.6100e+00, 2.3000e-01, 0.0000e+00, 2.3000e-01, 3.0700e+00, 5.4000e-01,\n",
      "        6.1000e-01, 3.3000e+00, 2.5300e+00, 3.1000e-01, 8.4000e-01, 3.3800e+00,\n",
      "        1.8509e+02, 1.7890e+01, 1.6900e+01, 1.6670e+01, 1.9280e+01, 1.5440e+01,\n",
      "        1.6820e+01, 1.5670e+01, 1.6820e+01, 4.0700e+00, 1.3290e+01, 1.5670e+01,\n",
      "        1.6590e+01, 1.5509e+02, 1.4750e+01, 1.2130e+01, 1.0880e+01, 1.3060e+01,\n",
      "        1.0980e+01, 1.0530e+01, 1.3980e+01, 1.1900e+01, 1.1670e+01, 1.2240e+01,\n",
      "        1.1830e+01, 8.4600e+00, 1.2670e+01])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "print(create_graph(0, 0 , 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geometric_graph(graph):\n",
    "    data = Data(\n",
    "        # Input graph.\n",
    "        x=graph[\"node_features\"],\n",
    "        #pos=pos,\n",
    "        edge_index=graph[\"edge_index\"],\n",
    "        edge_attr=graph[\"edge_features\"],\n",
    "        # Output node targets.\n",
    "        y=graph[\"node_targets\"],\n",
    "        num_nodes = len(graph[\"node_features\"])\n",
    "        #y_mask=y_mask,\n",
    "        # Utilities.\n",
    "        #node_ap=node_ap,\n",
    "        # Referencing\n",
    "        #scenario=graph['scenario'],\n",
    "        #deployment=graph['deployment'],\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "\n",
    "class EdgeModel(torch.nn.Module):\n",
    "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
    "        super().__init__()\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hiddens, n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
    "        #print(\"In edge model\")\n",
    "        #print(src, src.shape)\n",
    "        #print(dest, dest.shape)\n",
    "        #print(edge_attr, edge_attr.shape)\n",
    "        out = torch.cat([src, dest, edge_attr], 1)\n",
    "        out = self.edge_mlp(out)\n",
    "        #print(\"Exit edge model\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class NodeModel(torch.nn.Module):\n",
    "    def __init__(self, n_node_features, hiddens, n_targets):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.node_mlp_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hiddens, hiddens),\n",
    "        )\n",
    "        self.node_mlp_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hiddens, n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        #print(\"In node model\")\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[col], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out)\n",
    "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        out = self.node_mlp_2(out)\n",
    "        #print(\"Exit node model\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class MetaNet(torch.nn.Module):\n",
    "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
    "        super(MetaNet, self).__init__()\n",
    "\n",
    "        # Input Layer\n",
    "        self.input = MetaLayer(\n",
    "            edge_model=EdgeModel(\n",
    "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
    "                hiddens=num_hidden, n_targets=num_hidden),\n",
    "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
    "            )\n",
    "\n",
    "        # Output Layer\n",
    "        self.output = MetaLayer(\n",
    "            edge_model=EdgeModel(\n",
    "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
    "                hiddens=num_hidden, n_targets=num_hidden),\n",
    "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
    "        #print(\"In meta model\")\n",
    "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        #print(\"Exit meta model\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features = 11\n",
    "num_edge_features = 1\n",
    "num_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MetaNet(num_node_features, num_edge_features, num_hidden).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset):\n",
    "    # Monitor training.\n",
    "    losses = []\n",
    "\n",
    "    # Put model in training mode!\n",
    "    model.train()\n",
    "    i=0\n",
    "    for i, batch in enumerate(dataset):\n",
    "        #print(\"misaa\")\n",
    "        # Training step.\n",
    "    \n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
    "        #print(f\"Training oss for {i}: {loss}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Monitoring\n",
    "        losses.append(loss.item())\n",
    "        if(i == 559): break\n",
    "    # Return training metrics.\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate(dataset):\n",
    "    # Monitor evaluation.\n",
    "    losses = []\n",
    "    rmse = []\n",
    "\n",
    "    # Validation (1)\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    for i, batch in enumerate(dataset):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Calculate validation losses.\n",
    "        out = model(batch)\n",
    "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
    "\n",
    "        # Metric logging.\n",
    "        losses.append(loss.item())\n",
    "        #rmse.append(rmse_batch.item())\n",
    "        if(i == 159): break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With train, validation and test data.\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
    "# divide into training and testing points\n",
    "class CustomDataset(InMemoryDataset):\n",
    "    def __init__(self, root, split=\"train\", transform=None):\n",
    "        self.data = pd.read_csv(\"sta-int_map.csv\")\n",
    "        self.split = split\n",
    "        super(CustomDataset, self).__init__(root, split, transform)\n",
    "        #self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        #self.data = pd.read_csv(\"deployment_with_int_map.csv\")\n",
    "        #self.data, self.slices = pd.read_csv(\"deployment_with_int_map.csv\")\n",
    "        \n",
    "        print(\"In init\")\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        print(\"In raw_file_names\")\n",
    "        return [\"sta-int_map.csv\"]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        print(\"In processed_file_names\")\n",
    "        li = ['data_train_' + str(i) + '.pt' for i in range(560)]+ ['data_valid_' + str(j) + '.pt' for j in range(560, 720)] + ['data_test_' + str(k) + '.pt' for k in range(720, 800)]\n",
    "        #print(li)\n",
    "        return ['data_train_' + str(i) + '.pt' for i in range(560)]+ ['data_valid_' + str(j) + '.pt' for j in range(160)] + ['data_test_' + str(k) + '.pt' for k in range(80)]\n",
    "        \n",
    "    def _download(self):\n",
    "        '''\n",
    "        print(\"In download\")\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        # The zip file is removed\n",
    "        os.unlink(path)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        print(\"In process\")\n",
    "        #df = pd.read_csv(self.raw_paths[0])\n",
    "        X = self.data[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)',\n",
    "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'node_type',\n",
    "            'SINR', 'average_airtime', 'deployment']]\n",
    "        y = self.data.loc[:, [\"throughput\", \"deployment\"]]\n",
    "        X_train = X.iloc[:75327, :]\n",
    "        X_valid = X.iloc[75327:82674, :]\n",
    "        X_test = X.iloc[82674:,:]\n",
    "        print(X_test.columns)\n",
    "        y_train = y.iloc[:75327]\n",
    "        y_valid = y.iloc[75327:82674]\n",
    "        y_test = y.iloc[82674:]\n",
    "        graphs = []\n",
    "        print(\"Here\")\n",
    "        l = [i for i in range(800)]\n",
    "        self.l_train = random.sample(l, 560)\n",
    "        l = [x for x in l if x not in self.l_train]\n",
    "        self.l_valid = random.sample(l, 160)\n",
    "        l = [x for x in l if x not in self.l_valid]\n",
    "        self.l_test = l\n",
    "        count = 0\n",
    "        if(self.split == \"train\"):\n",
    "            \n",
    "            for i in self.l_train:\n",
    "                \n",
    "                #X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float)\n",
    "                #y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float)\n",
    "                graph = create_graph(X_train, y_train, i)\n",
    "                \n",
    "                graph = create_geometric_graph(graph)\n",
    "                graphs.append(graph)\n",
    "\n",
    "                torch.save(graph, os.path.join(self.processed_dir, f'data_train_{count}.pt'))\n",
    "                count += 1\n",
    "        elif(self.split == \"valid\"):\n",
    "            for i in self.l_valid:\n",
    "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
    "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
    "                graph = create_graph(X_valid, y_valid, i)\n",
    "                graph = create_geometric_graph(graph)\n",
    "                graphs.append(graph)\n",
    "            \n",
    "                torch.save(graph, os.path.join(self.processed_dir, f'data_valid_{count}.pt'))  \n",
    "                count += 1\n",
    "        else:\n",
    "            for i in self.l_test:\n",
    "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
    "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
    "                graph = create_graph(X_test, y_test, i)\n",
    "                graph = create_geometric_graph(graph)\n",
    "                graphs.append(graph)\n",
    "            \n",
    "                torch.save(graph, os.path.join(self.processed_dir, f'data_test_{count}.pt'))\n",
    "                count += 1  \n",
    "        #return graphs[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if(self.split == \"train\"):\n",
    "            #return len(self.processed_file_names[0])\n",
    "            return 560\n",
    "        elif self.split == \"valid\":\n",
    "            return 160\n",
    "        else:\n",
    "            return 80\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #print(\"Part: \", self.processed_file_names[1])\n",
    "        \n",
    "        if(self.split == \"train\"):\n",
    "            data = torch.load(os.path.join(self.processed_dir, f'data_train_{idx}.pt'))\n",
    "        elif(self.split == \"valid\"):\n",
    "            data = torch.load(os.path.join(self.processed_dir, f'data_valid_{idx}.pt'))\n",
    "        elif (self.split==\"test\"):\n",
    "            data = torch.load(os.path.join(self.processed_dir, f'data_test_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\PIL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In processed_file_names\n",
      "In process\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11',\n",
      "       'wlan_code_index', 'x(m)', 'y(m)', 'primary_channel',\n",
      "       'min_channel_allowed', 'max_channel_allowed', 'node_type', 'SINR',\n",
      "       'average_airtime', 'deployment'],\n",
      "      dtype='object')\n",
      "Here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 11])\n",
      "torch.Size([45, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 11])\n",
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([176, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([190, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([211, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([190, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([190, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([210, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([165, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([210, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([207, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([205, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 11])\n",
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n",
      "torch.Size([36, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([176, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([169, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([207, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n",
      "torch.Size([42, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([166, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([165, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([174, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([206, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 11])\n",
      "torch.Size([34, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([83, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([173, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([206, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([82, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([216, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([179, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n",
      "torch.Size([37, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([174, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([183, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([209, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([217, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([215, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([206, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([168, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([206, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([183, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([169, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([214, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([211, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([207, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([167, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([216, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([205, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([207, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([190, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 11])\n",
      "torch.Size([35, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([217, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([179, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([209, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([210, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([179, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([213, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([178, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([183, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([83, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 11])\n",
      "In init\n",
      "In processed_file_names\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "Done!\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In process\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11',\n",
      "       'wlan_code_index', 'x(m)', 'y(m)', 'primary_channel',\n",
      "       'min_channel_allowed', 'max_channel_allowed', 'node_type', 'SINR',\n",
      "       'average_airtime', 'deployment'],\n",
      "      dtype='object')\n",
      "Here\n",
      "torch.Size([40, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([174, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([183, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([209, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([169, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([78, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([207, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([206, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([190, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([211, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([190, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 11])\n",
      "torch.Size([19, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([215, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([207, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([208, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([179, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([206, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([202, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n",
      "In init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In processed_file_names\n",
      "In process\n",
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11',\n",
      "       'wlan_code_index', 'x(m)', 'y(m)', 'primary_channel',\n",
      "       'min_channel_allowed', 'max_channel_allowed', 'node_type', 'SINR',\n",
      "       'average_airtime', 'deployment'],\n",
      "      dtype='object')\n",
      "Here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([183, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([203, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([216, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([204, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([214, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([169, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([191, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([73, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([71, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([66, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([74, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 11])\n",
      "torch.Size([22, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 11])\n",
      "torch.Size([30, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 11])\n",
      "torch.Size([32, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 11])\n",
      "In init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Local\\Temp\\ipykernel_7752\\1426162578.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edges, dtype=torch.long)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CustomDataset(root=\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\PIL/\", split='train')\n",
    "dataset_valid = CustomDataset(root=\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\PIL/\", split='valid')\n",
    "dataset_test = CustomDataset(root=\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\PIL/\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch_geometric.data import DataLoader\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=3, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Len of Training loss: 18, Average loss: 27.949796358744305\n",
      "Len of Validation loss: 54, Average loss: 26.843507766723633\n",
      "Epoch: 1, Len of Training loss: 18, Average loss: 26.0985008875529\n",
      "Len of Validation loss: 54, Average loss: 24.0454852492721\n",
      "Epoch: 2, Len of Training loss: 18, Average loss: 23.106643358866375\n",
      "Len of Validation loss: 54, Average loss: 25.867083584820783\n",
      "Epoch: 3, Len of Training loss: 18, Average loss: 21.456239806281197\n",
      "Len of Validation loss: 54, Average loss: 20.20924788934213\n",
      "Epoch: 4, Len of Training loss: 18, Average loss: 19.408234066433376\n",
      "Len of Validation loss: 54, Average loss: 18.384470339174623\n",
      "Epoch: 5, Len of Training loss: 18, Average loss: 20.433975484636093\n",
      "Len of Validation loss: 54, Average loss: 22.56856695810954\n",
      "Epoch: 6, Len of Training loss: 18, Average loss: 20.815102312299942\n",
      "Len of Validation loss: 54, Average loss: 18.67083553031639\n",
      "Epoch: 7, Len of Training loss: 18, Average loss: 17.617582321166992\n",
      "Len of Validation loss: 54, Average loss: 16.56152813522904\n",
      "Epoch: 8, Len of Training loss: 18, Average loss: 16.037423504723442\n",
      "Len of Validation loss: 54, Average loss: 14.688568530259309\n",
      "Epoch: 9, Len of Training loss: 18, Average loss: 14.663738621605766\n",
      "Len of Validation loss: 54, Average loss: 14.163347756421125\n",
      "Epoch: 10, Len of Training loss: 18, Average loss: 15.024690469106039\n",
      "Len of Validation loss: 54, Average loss: 14.684256783238164\n",
      "Epoch: 11, Len of Training loss: 18, Average loss: 15.282209873199463\n",
      "Len of Validation loss: 54, Average loss: 14.444052325354683\n",
      "Epoch: 12, Len of Training loss: 18, Average loss: 14.508610354529488\n",
      "Len of Validation loss: 54, Average loss: 13.931312437410709\n",
      "Epoch: 13, Len of Training loss: 18, Average loss: 13.653928862677681\n",
      "Len of Validation loss: 54, Average loss: 13.083168939307884\n",
      "Epoch: 14, Len of Training loss: 18, Average loss: 13.237262196011013\n",
      "Len of Validation loss: 54, Average loss: 13.215091731813219\n",
      "Epoch: 15, Len of Training loss: 18, Average loss: 13.1339496506585\n",
      "Len of Validation loss: 54, Average loss: 13.006646951039633\n",
      "Epoch: 16, Len of Training loss: 18, Average loss: 13.651069376203749\n",
      "Len of Validation loss: 54, Average loss: 14.793351261704057\n",
      "Epoch: 17, Len of Training loss: 18, Average loss: 13.879843340979683\n",
      "Len of Validation loss: 54, Average loss: 13.986134741041395\n",
      "Epoch: 18, Len of Training loss: 18, Average loss: 13.341267532772488\n",
      "Len of Validation loss: 54, Average loss: 12.63459442279957\n",
      "Epoch: 19, Len of Training loss: 18, Average loss: 12.616258197360569\n",
      "Len of Validation loss: 54, Average loss: 11.632778777016533\n",
      "Epoch: 20, Len of Training loss: 18, Average loss: 12.361697249942356\n",
      "Len of Validation loss: 54, Average loss: 13.971965471903482\n",
      "Epoch: 21, Len of Training loss: 18, Average loss: 12.747033966912163\n",
      "Len of Validation loss: 54, Average loss: 11.768664474840518\n",
      "Epoch: 22, Len of Training loss: 18, Average loss: 12.739719019995796\n",
      "Len of Validation loss: 54, Average loss: 12.212770453205815\n",
      "Epoch: 23, Len of Training loss: 18, Average loss: 12.480385409461128\n",
      "Len of Validation loss: 54, Average loss: 11.459389209747314\n",
      "Epoch: 24, Len of Training loss: 18, Average loss: 12.079421838124594\n",
      "Len of Validation loss: 54, Average loss: 11.490081557521114\n",
      "Epoch: 25, Len of Training loss: 18, Average loss: 11.59586567348904\n",
      "Len of Validation loss: 54, Average loss: 11.384417675159595\n",
      "Epoch: 26, Len of Training loss: 18, Average loss: 11.926570256551107\n",
      "Len of Validation loss: 54, Average loss: 13.327269218586109\n",
      "Epoch: 27, Len of Training loss: 18, Average loss: 12.757540437910292\n",
      "Len of Validation loss: 54, Average loss: 11.628768806104306\n",
      "Epoch: 28, Len of Training loss: 18, Average loss: 12.166634612613255\n",
      "Len of Validation loss: 54, Average loss: 12.087790692294085\n",
      "Epoch: 29, Len of Training loss: 18, Average loss: 11.390188111199272\n",
      "Len of Validation loss: 54, Average loss: 10.81862341033088\n",
      "Epoch: 30, Len of Training loss: 18, Average loss: 11.149613115522596\n",
      "Len of Validation loss: 54, Average loss: 10.704665696179426\n",
      "Epoch: 31, Len of Training loss: 18, Average loss: 12.004911210801867\n",
      "Len of Validation loss: 54, Average loss: 11.93426443029333\n",
      "Epoch: 32, Len of Training loss: 18, Average loss: 12.679667154947916\n",
      "Len of Validation loss: 54, Average loss: 13.238780887038619\n",
      "Epoch: 33, Len of Training loss: 18, Average loss: 11.668641567230225\n",
      "Len of Validation loss: 54, Average loss: 11.258580958401716\n",
      "Epoch: 34, Len of Training loss: 18, Average loss: 11.497801462809244\n",
      "Len of Validation loss: 54, Average loss: 10.873290847848963\n",
      "Epoch: 35, Len of Training loss: 18, Average loss: 10.8745788998074\n",
      "Len of Validation loss: 54, Average loss: 10.26688739988539\n",
      "Epoch: 36, Len of Training loss: 18, Average loss: 10.710481034384834\n",
      "Len of Validation loss: 54, Average loss: 10.247782698384038\n",
      "Epoch: 37, Len of Training loss: 18, Average loss: 10.960541248321533\n",
      "Len of Validation loss: 54, Average loss: 11.705515499468204\n",
      "Epoch: 38, Len of Training loss: 18, Average loss: 10.908685207366943\n",
      "Len of Validation loss: 54, Average loss: 10.497688072699088\n",
      "Epoch: 39, Len of Training loss: 18, Average loss: 10.968748834398058\n",
      "Len of Validation loss: 54, Average loss: 10.148533723972461\n",
      "Epoch: 40, Len of Training loss: 18, Average loss: 10.73183822631836\n",
      "Len of Validation loss: 54, Average loss: 9.710363193794533\n",
      "Epoch: 41, Len of Training loss: 18, Average loss: 10.347042083740234\n",
      "Len of Validation loss: 54, Average loss: 9.299092089688337\n",
      "Epoch: 42, Len of Training loss: 18, Average loss: 9.883800824483236\n",
      "Len of Validation loss: 54, Average loss: 10.003091467751396\n",
      "Epoch: 43, Len of Training loss: 18, Average loss: 10.159018013212416\n",
      "Len of Validation loss: 54, Average loss: 10.467855577115659\n",
      "Epoch: 44, Len of Training loss: 18, Average loss: 10.434195518493652\n",
      "Len of Validation loss: 54, Average loss: 9.644811303527266\n",
      "Epoch: 45, Len of Training loss: 18, Average loss: 9.986213313208687\n",
      "Len of Validation loss: 54, Average loss: 10.342470645904541\n",
      "Epoch: 46, Len of Training loss: 18, Average loss: 10.019145912594265\n",
      "Len of Validation loss: 54, Average loss: 9.79562136862013\n",
      "Epoch: 47, Len of Training loss: 18, Average loss: 10.377449909845987\n",
      "Len of Validation loss: 54, Average loss: 10.52931844746625\n",
      "Epoch: 48, Len of Training loss: 18, Average loss: 9.63337747255961\n",
      "Len of Validation loss: 54, Average loss: 9.497372521294487\n",
      "Epoch: 49, Len of Training loss: 18, Average loss: 9.746520227856106\n",
      "Len of Validation loss: 54, Average loss: 9.579054991404215\n",
      "Epoch: 50, Len of Training loss: 18, Average loss: 9.726264582739937\n",
      "Len of Validation loss: 54, Average loss: 9.020328066967151\n",
      "Epoch: 51, Len of Training loss: 18, Average loss: 9.299071709314982\n",
      "Len of Validation loss: 54, Average loss: 10.828695721096462\n",
      "Epoch: 52, Len of Training loss: 18, Average loss: 9.839071989059448\n",
      "Len of Validation loss: 54, Average loss: 9.288000177454066\n",
      "Epoch: 53, Len of Training loss: 18, Average loss: 9.26900471581353\n",
      "Len of Validation loss: 54, Average loss: 8.6852144047066\n",
      "Epoch: 54, Len of Training loss: 18, Average loss: 9.493191109763252\n",
      "Len of Validation loss: 54, Average loss: 9.29707474178738\n",
      "Epoch: 55, Len of Training loss: 18, Average loss: 9.522261381149292\n",
      "Len of Validation loss: 54, Average loss: 10.380553616417778\n",
      "Epoch: 56, Len of Training loss: 18, Average loss: 9.802580383088854\n",
      "Len of Validation loss: 54, Average loss: 9.10710699469955\n",
      "Epoch: 57, Len of Training loss: 18, Average loss: 9.334691630469429\n",
      "Len of Validation loss: 54, Average loss: 8.656450624819156\n",
      "Epoch: 58, Len of Training loss: 18, Average loss: 9.190453026029799\n",
      "Len of Validation loss: 54, Average loss: 8.7552005538234\n",
      "Epoch: 59, Len of Training loss: 18, Average loss: 9.134817414813572\n",
      "Len of Validation loss: 54, Average loss: 8.69469459410067\n",
      "Epoch: 60, Len of Training loss: 18, Average loss: 8.826745324664646\n",
      "Len of Validation loss: 54, Average loss: 8.291486519354361\n",
      "Epoch: 61, Len of Training loss: 18, Average loss: 8.719012790256077\n",
      "Len of Validation loss: 54, Average loss: 8.20673077194779\n",
      "Epoch: 62, Len of Training loss: 18, Average loss: 8.782305055194431\n",
      "Len of Validation loss: 54, Average loss: 8.347793693895694\n",
      "Epoch: 63, Len of Training loss: 18, Average loss: 8.943197303348118\n",
      "Len of Validation loss: 54, Average loss: 8.85529473092821\n",
      "Epoch: 64, Len of Training loss: 18, Average loss: 8.692272398206923\n",
      "Len of Validation loss: 54, Average loss: 8.300277144820601\n",
      "Epoch: 65, Len of Training loss: 18, Average loss: 8.900472773445976\n",
      "Len of Validation loss: 54, Average loss: 8.115239496584293\n",
      "Epoch: 66, Len of Training loss: 18, Average loss: 8.733774503072103\n",
      "Len of Validation loss: 54, Average loss: 8.258594022856819\n",
      "Epoch: 67, Len of Training loss: 18, Average loss: 8.98411782582601\n",
      "Len of Validation loss: 54, Average loss: 8.393149552521882\n",
      "Epoch: 68, Len of Training loss: 18, Average loss: 8.465547667609322\n",
      "Len of Validation loss: 54, Average loss: 8.625796008993078\n",
      "Epoch: 69, Len of Training loss: 18, Average loss: 9.067610873116386\n",
      "Len of Validation loss: 54, Average loss: 8.116950322080541\n",
      "Epoch: 70, Len of Training loss: 18, Average loss: 8.557657877604166\n",
      "Len of Validation loss: 54, Average loss: 8.401107355400368\n",
      "Epoch: 71, Len of Training loss: 18, Average loss: 8.370178593529594\n",
      "Len of Validation loss: 54, Average loss: 8.07642854143072\n",
      "Epoch: 72, Len of Training loss: 18, Average loss: 8.3005584081014\n",
      "Len of Validation loss: 54, Average loss: 8.1995304690467\n",
      "Epoch: 73, Len of Training loss: 18, Average loss: 8.164573934343126\n",
      "Len of Validation loss: 54, Average loss: 8.997273568753842\n",
      "Epoch: 74, Len of Training loss: 18, Average loss: 9.098137908511692\n",
      "Len of Validation loss: 54, Average loss: 8.520293721446285\n",
      "Epoch: 75, Len of Training loss: 18, Average loss: 8.619149499469334\n",
      "Len of Validation loss: 54, Average loss: 8.093737134227046\n",
      "Epoch: 76, Len of Training loss: 18, Average loss: 8.394193808237711\n",
      "Len of Validation loss: 54, Average loss: 7.815493839758414\n",
      "Epoch: 77, Len of Training loss: 18, Average loss: 8.399571471744114\n",
      "Len of Validation loss: 54, Average loss: 7.662230774208352\n",
      "Epoch: 78, Len of Training loss: 18, Average loss: 8.483216868506538\n",
      "Len of Validation loss: 54, Average loss: 8.16503075758616\n",
      "Epoch: 79, Len of Training loss: 18, Average loss: 8.94448815451728\n",
      "Len of Validation loss: 54, Average loss: 8.977260642581516\n",
      "Epoch: 80, Len of Training loss: 18, Average loss: 8.664437346988255\n",
      "Len of Validation loss: 54, Average loss: 8.097150188905221\n",
      "Epoch: 81, Len of Training loss: 18, Average loss: 8.642988125483194\n",
      "Len of Validation loss: 54, Average loss: 8.071271604961819\n",
      "Epoch: 82, Len of Training loss: 18, Average loss: 8.150110059314304\n",
      "Len of Validation loss: 54, Average loss: 7.914899698010197\n",
      "Epoch: 83, Len of Training loss: 18, Average loss: 8.306067652172512\n",
      "Len of Validation loss: 54, Average loss: 8.224351556212813\n",
      "Epoch: 84, Len of Training loss: 18, Average loss: 8.389955441157023\n",
      "Len of Validation loss: 54, Average loss: 10.839051749971178\n",
      "Epoch: 85, Len of Training loss: 18, Average loss: 20.75947250260247\n",
      "Len of Validation loss: 54, Average loss: 14.614636898040771\n",
      "Epoch: 86, Len of Training loss: 18, Average loss: 14.496623463100857\n",
      "Len of Validation loss: 54, Average loss: 11.450082456624067\n",
      "Epoch: 87, Len of Training loss: 18, Average loss: 11.254214922587076\n",
      "Len of Validation loss: 54, Average loss: 10.036009832664773\n",
      "Epoch: 88, Len of Training loss: 18, Average loss: 10.910711447397867\n",
      "Len of Validation loss: 54, Average loss: 9.38907986217075\n",
      "Epoch: 89, Len of Training loss: 18, Average loss: 10.04318552547031\n",
      "Len of Validation loss: 54, Average loss: 8.892534741648921\n",
      "Epoch: 90, Len of Training loss: 18, Average loss: 9.204688469568888\n",
      "Len of Validation loss: 54, Average loss: 8.807644658618504\n",
      "Epoch: 91, Len of Training loss: 18, Average loss: 8.94049006038242\n",
      "Len of Validation loss: 54, Average loss: 8.42638420617139\n",
      "Epoch: 92, Len of Training loss: 18, Average loss: 8.674704631169638\n",
      "Len of Validation loss: 54, Average loss: 8.006504279595834\n",
      "Epoch: 93, Len of Training loss: 18, Average loss: 8.566526836819119\n",
      "Len of Validation loss: 54, Average loss: 8.94810296429528\n",
      "Epoch: 94, Len of Training loss: 18, Average loss: 9.437295224931505\n",
      "Len of Validation loss: 54, Average loss: 8.796971254878574\n",
      "Epoch: 95, Len of Training loss: 18, Average loss: 8.94824677043491\n",
      "Len of Validation loss: 54, Average loss: 9.09345774297361\n",
      "Epoch: 96, Len of Training loss: 18, Average loss: 8.545962651570639\n",
      "Len of Validation loss: 54, Average loss: 8.31424159473843\n",
      "Epoch: 97, Len of Training loss: 18, Average loss: 8.15037645234002\n",
      "Len of Validation loss: 54, Average loss: 7.776472365414655\n",
      "Epoch: 98, Len of Training loss: 18, Average loss: 8.199876361423069\n",
      "Len of Validation loss: 54, Average loss: 7.781488921907213\n",
      "Epoch: 99, Len of Training loss: 18, Average loss: 8.203114032745361\n",
      "Len of Validation loss: 54, Average loss: 8.079138605682939\n",
      "Epoch: 100, Len of Training loss: 18, Average loss: 8.40636420249939\n",
      "Len of Validation loss: 54, Average loss: 8.08890195246096\n",
      "Epoch: 101, Len of Training loss: 18, Average loss: 8.21616702609592\n",
      "Len of Validation loss: 54, Average loss: 7.531871217268485\n",
      "Epoch: 102, Len of Training loss: 18, Average loss: 7.986846685409546\n",
      "Len of Validation loss: 54, Average loss: 8.132421065259862\n",
      "Epoch: 103, Len of Training loss: 18, Average loss: 8.149431520038181\n",
      "Len of Validation loss: 54, Average loss: 8.187778146178633\n",
      "Epoch: 104, Len of Training loss: 18, Average loss: 8.582330995135838\n",
      "Len of Validation loss: 54, Average loss: 8.905506451924643\n",
      "Epoch: 105, Len of Training loss: 18, Average loss: 9.018586874008179\n",
      "Len of Validation loss: 54, Average loss: 8.917812718285454\n",
      "Epoch: 106, Len of Training loss: 18, Average loss: 8.51576656765408\n",
      "Len of Validation loss: 54, Average loss: 7.945795686156662\n",
      "Epoch: 107, Len of Training loss: 18, Average loss: 8.321029557122124\n",
      "Len of Validation loss: 54, Average loss: 7.9838197231292725\n",
      "Epoch: 108, Len of Training loss: 18, Average loss: 8.76500482029385\n",
      "Len of Validation loss: 54, Average loss: 7.855759938557942\n",
      "Epoch: 109, Len of Training loss: 18, Average loss: 8.760034746593899\n",
      "Len of Validation loss: 54, Average loss: 7.801933531407957\n",
      "Epoch: 110, Len of Training loss: 18, Average loss: 7.931499931547377\n",
      "Len of Validation loss: 54, Average loss: 7.86056783022704\n",
      "Epoch: 111, Len of Training loss: 18, Average loss: 7.990904039806789\n",
      "Len of Validation loss: 54, Average loss: 8.315438301474959\n",
      "Epoch: 112, Len of Training loss: 18, Average loss: 8.293099641799927\n",
      "Len of Validation loss: 54, Average loss: 7.975403953481604\n",
      "Epoch: 113, Len of Training loss: 18, Average loss: 8.068405946095785\n",
      "Len of Validation loss: 54, Average loss: 7.595913299807796\n",
      "Epoch: 114, Len of Training loss: 18, Average loss: 8.025083170996773\n",
      "Len of Validation loss: 54, Average loss: 7.971186690860325\n",
      "Epoch: 115, Len of Training loss: 18, Average loss: 7.963719712363349\n",
      "Len of Validation loss: 54, Average loss: 9.790919370121426\n",
      "Epoch: 116, Len of Training loss: 18, Average loss: 8.238106833563911\n",
      "Len of Validation loss: 54, Average loss: 8.46849149244803\n",
      "Epoch: 117, Len of Training loss: 18, Average loss: 8.343187544080946\n",
      "Len of Validation loss: 54, Average loss: 8.757869835253116\n",
      "Epoch: 118, Len of Training loss: 18, Average loss: 8.728656238979763\n",
      "Len of Validation loss: 54, Average loss: 8.753971960809496\n",
      "Epoch: 119, Len of Training loss: 18, Average loss: 8.585499419106377\n",
      "Len of Validation loss: 54, Average loss: 7.657266321005644\n",
      "Epoch: 120, Len of Training loss: 18, Average loss: 8.07524479760064\n",
      "Len of Validation loss: 54, Average loss: 7.927665026099594\n",
      "Epoch: 121, Len of Training loss: 18, Average loss: 8.056203365325928\n",
      "Len of Validation loss: 54, Average loss: 8.443711157198306\n",
      "Epoch: 122, Len of Training loss: 18, Average loss: 7.990662919150458\n",
      "Len of Validation loss: 54, Average loss: 7.916384502693459\n",
      "Epoch: 123, Len of Training loss: 18, Average loss: 8.050628556145561\n",
      "Len of Validation loss: 54, Average loss: 7.770806286070082\n",
      "Epoch: 124, Len of Training loss: 18, Average loss: 7.810266070895725\n",
      "Len of Validation loss: 54, Average loss: 8.048653602600098\n",
      "Epoch: 125, Len of Training loss: 18, Average loss: 7.960029045740764\n",
      "Len of Validation loss: 54, Average loss: 8.024778851756343\n",
      "Epoch: 126, Len of Training loss: 18, Average loss: 7.848281224568685\n",
      "Len of Validation loss: 54, Average loss: 7.394496608663489\n",
      "Epoch: 127, Len of Training loss: 18, Average loss: 7.889297962188721\n",
      "Len of Validation loss: 54, Average loss: 7.589833829138014\n",
      "Epoch: 128, Len of Training loss: 18, Average loss: 8.20634757147895\n",
      "Len of Validation loss: 54, Average loss: 8.235402853400618\n",
      "Epoch: 129, Len of Training loss: 18, Average loss: 8.38062318166097\n",
      "Len of Validation loss: 54, Average loss: 7.95830014899925\n",
      "Epoch: 130, Len of Training loss: 18, Average loss: 8.15752821498447\n",
      "Len of Validation loss: 54, Average loss: 7.639170836519312\n",
      "Epoch: 131, Len of Training loss: 18, Average loss: 7.863179816140069\n",
      "Len of Validation loss: 54, Average loss: 7.568921777937147\n",
      "Epoch: 132, Len of Training loss: 18, Average loss: 7.855230410893758\n",
      "Len of Validation loss: 54, Average loss: 7.666441992477134\n",
      "Epoch: 133, Len of Training loss: 18, Average loss: 7.805739455752903\n",
      "Len of Validation loss: 54, Average loss: 8.19893530563072\n",
      "Epoch: 134, Len of Training loss: 18, Average loss: 7.804212172826131\n",
      "Len of Validation loss: 54, Average loss: 7.737333686263473\n",
      "Epoch: 135, Len of Training loss: 18, Average loss: 7.519299083285862\n",
      "Len of Validation loss: 54, Average loss: 7.212363238687868\n",
      "Epoch: 136, Len of Training loss: 18, Average loss: 7.616275469462077\n",
      "Len of Validation loss: 54, Average loss: 7.525850097338359\n",
      "Epoch: 137, Len of Training loss: 18, Average loss: 7.655422502093845\n",
      "Len of Validation loss: 54, Average loss: 9.004360402071917\n",
      "Epoch: 138, Len of Training loss: 18, Average loss: 8.265251874923706\n",
      "Len of Validation loss: 54, Average loss: 7.518290921493813\n",
      "Epoch: 139, Len of Training loss: 18, Average loss: 8.103704108132256\n",
      "Len of Validation loss: 54, Average loss: 7.885397279704058\n",
      "Epoch: 140, Len of Training loss: 18, Average loss: 7.723919603559706\n",
      "Len of Validation loss: 54, Average loss: 7.717880341741774\n",
      "Epoch: 141, Len of Training loss: 18, Average loss: 7.6433009306589765\n",
      "Len of Validation loss: 54, Average loss: 7.077310270733303\n",
      "Epoch: 142, Len of Training loss: 18, Average loss: 7.9568675888909235\n",
      "Len of Validation loss: 54, Average loss: 8.377511470406144\n",
      "Epoch: 143, Len of Training loss: 18, Average loss: 7.732196675406562\n",
      "Len of Validation loss: 54, Average loss: 7.2982844070152\n",
      "Epoch: 144, Len of Training loss: 18, Average loss: 7.722818348142836\n",
      "Len of Validation loss: 54, Average loss: 7.3674633017292726\n",
      "Epoch: 145, Len of Training loss: 18, Average loss: 7.614353471332127\n",
      "Len of Validation loss: 54, Average loss: 7.511534805651064\n",
      "Epoch: 146, Len of Training loss: 18, Average loss: 7.731520175933838\n",
      "Len of Validation loss: 54, Average loss: 7.4065898082874435\n",
      "Epoch: 147, Len of Training loss: 18, Average loss: 7.760365910000271\n",
      "Len of Validation loss: 54, Average loss: 7.357246769799127\n",
      "Epoch: 148, Len of Training loss: 18, Average loss: 7.501877228418986\n",
      "Len of Validation loss: 54, Average loss: 7.497113249920033\n",
      "Epoch: 149, Len of Training loss: 18, Average loss: 7.5904966990153\n",
      "Len of Validation loss: 54, Average loss: 7.574143197801378\n",
      "Epoch: 150, Len of Training loss: 18, Average loss: 7.86788272857666\n",
      "Len of Validation loss: 54, Average loss: 7.912110597999008\n",
      "Epoch: 151, Len of Training loss: 18, Average loss: 7.87226046456231\n",
      "Len of Validation loss: 54, Average loss: 8.285161923479151\n",
      "Epoch: 152, Len of Training loss: 18, Average loss: 7.837584601508246\n",
      "Len of Validation loss: 54, Average loss: 7.929365789448774\n",
      "Epoch: 153, Len of Training loss: 18, Average loss: 7.651183896594578\n",
      "Len of Validation loss: 54, Average loss: 7.229408511409053\n",
      "Epoch: 154, Len of Training loss: 18, Average loss: 7.575989166895549\n",
      "Len of Validation loss: 54, Average loss: 7.78031011863991\n",
      "Epoch: 155, Len of Training loss: 18, Average loss: 7.958257834116618\n",
      "Len of Validation loss: 54, Average loss: 7.466185512366118\n",
      "Epoch: 156, Len of Training loss: 18, Average loss: 7.615421586566502\n",
      "Len of Validation loss: 54, Average loss: 7.580997612741259\n",
      "Epoch: 157, Len of Training loss: 18, Average loss: 7.814214600457086\n",
      "Len of Validation loss: 54, Average loss: 7.556516890172605\n",
      "Epoch: 158, Len of Training loss: 18, Average loss: 7.592383146286011\n",
      "Len of Validation loss: 54, Average loss: 7.5356715167010275\n",
      "Epoch: 159, Len of Training loss: 18, Average loss: 7.709369818369548\n",
      "Len of Validation loss: 54, Average loss: 7.711782570238467\n",
      "Epoch: 160, Len of Training loss: 18, Average loss: 8.02842934926351\n",
      "Len of Validation loss: 54, Average loss: 7.599947898476212\n",
      "Epoch: 161, Len of Training loss: 18, Average loss: 7.732730309168498\n",
      "Len of Validation loss: 54, Average loss: 7.714013117331046\n",
      "Epoch: 162, Len of Training loss: 18, Average loss: 7.829903470145331\n",
      "Len of Validation loss: 54, Average loss: 8.221790198926572\n",
      "Epoch: 163, Len of Training loss: 18, Average loss: 7.645313925213284\n",
      "Len of Validation loss: 54, Average loss: 7.328656558637266\n",
      "Epoch: 164, Len of Training loss: 18, Average loss: 7.448044882880317\n",
      "Len of Validation loss: 54, Average loss: 7.525804400444031\n",
      "Epoch: 165, Len of Training loss: 18, Average loss: 7.447006040149265\n",
      "Len of Validation loss: 54, Average loss: 7.912314666642083\n",
      "Epoch: 166, Len of Training loss: 18, Average loss: 7.738464885287815\n",
      "Len of Validation loss: 54, Average loss: 8.406887654904965\n",
      "Epoch: 167, Len of Training loss: 18, Average loss: 8.374460856119791\n",
      "Len of Validation loss: 54, Average loss: 7.870793748784949\n",
      "Epoch: 168, Len of Training loss: 18, Average loss: 7.693381627400716\n",
      "Len of Validation loss: 54, Average loss: 7.456621598314356\n",
      "Epoch: 169, Len of Training loss: 18, Average loss: 7.566748989952935\n",
      "Len of Validation loss: 54, Average loss: 7.363496396276686\n",
      "Epoch: 170, Len of Training loss: 18, Average loss: 7.581948280334473\n",
      "Len of Validation loss: 54, Average loss: 7.383934828970167\n",
      "Epoch: 171, Len of Training loss: 18, Average loss: 7.816937314139472\n",
      "Len of Validation loss: 54, Average loss: 7.270608058682194\n",
      "Epoch: 172, Len of Training loss: 18, Average loss: 7.589430967966716\n",
      "Len of Validation loss: 54, Average loss: 7.525061276223925\n",
      "Epoch: 173, Len of Training loss: 18, Average loss: 7.859195099936591\n",
      "Len of Validation loss: 54, Average loss: 8.154238179877952\n",
      "Epoch: 174, Len of Training loss: 18, Average loss: 7.720131821102566\n",
      "Len of Validation loss: 54, Average loss: 7.4251212279001875\n",
      "Epoch: 175, Len of Training loss: 18, Average loss: 7.670542558034261\n",
      "Len of Validation loss: 54, Average loss: 7.7531899611155195\n",
      "Epoch: 176, Len of Training loss: 18, Average loss: 7.537698692745632\n",
      "Len of Validation loss: 54, Average loss: 7.487526372626975\n",
      "Epoch: 177, Len of Training loss: 18, Average loss: 7.4719761742485895\n",
      "Len of Validation loss: 54, Average loss: 7.64477977487776\n",
      "Epoch: 178, Len of Training loss: 18, Average loss: 7.530802541308933\n",
      "Len of Validation loss: 54, Average loss: 8.30722067532716\n",
      "Epoch: 179, Len of Training loss: 18, Average loss: 8.022747675577799\n",
      "Len of Validation loss: 54, Average loss: 8.845240685674879\n",
      "Epoch: 180, Len of Training loss: 18, Average loss: 7.998216840955946\n",
      "Len of Validation loss: 54, Average loss: 7.468168188024451\n",
      "Epoch: 181, Len of Training loss: 18, Average loss: 7.722559637493557\n",
      "Len of Validation loss: 54, Average loss: 7.545711817564787\n",
      "Epoch: 182, Len of Training loss: 18, Average loss: 7.487352344724867\n",
      "Len of Validation loss: 54, Average loss: 7.266191800435384\n",
      "Epoch: 183, Len of Training loss: 18, Average loss: 7.739634010526869\n",
      "Len of Validation loss: 54, Average loss: 8.53437168509872\n",
      "Epoch: 184, Len of Training loss: 18, Average loss: 7.802023251851399\n",
      "Len of Validation loss: 54, Average loss: 7.358875733834726\n",
      "Epoch: 185, Len of Training loss: 18, Average loss: 7.422649568981594\n",
      "Len of Validation loss: 54, Average loss: 7.621422763224001\n",
      "Epoch: 186, Len of Training loss: 18, Average loss: 7.472071462207371\n",
      "Len of Validation loss: 54, Average loss: 8.906031096423114\n",
      "Epoch: 187, Len of Training loss: 18, Average loss: 7.745985666910808\n",
      "Len of Validation loss: 54, Average loss: 7.5089752585799605\n",
      "Epoch: 188, Len of Training loss: 18, Average loss: 7.300803502400716\n",
      "Len of Validation loss: 54, Average loss: 7.448760063559921\n",
      "Epoch: 189, Len of Training loss: 18, Average loss: 7.202223698298137\n",
      "Len of Validation loss: 54, Average loss: 7.488002755023815\n",
      "Epoch: 190, Len of Training loss: 18, Average loss: 7.3111147615644665\n",
      "Len of Validation loss: 54, Average loss: 7.1373014803285955\n",
      "Epoch: 191, Len of Training loss: 18, Average loss: 7.270645221074422\n",
      "Len of Validation loss: 54, Average loss: 7.69076931476593\n",
      "Epoch: 192, Len of Training loss: 18, Average loss: 7.684479104148017\n",
      "Len of Validation loss: 54, Average loss: 7.6836444404390125\n",
      "Epoch: 193, Len of Training loss: 18, Average loss: 7.619760380850898\n",
      "Len of Validation loss: 54, Average loss: 8.751893092084813\n",
      "Epoch: 194, Len of Training loss: 18, Average loss: 8.014076153437296\n",
      "Len of Validation loss: 54, Average loss: 8.121543380949232\n",
      "Epoch: 195, Len of Training loss: 18, Average loss: 8.14844404326545\n",
      "Len of Validation loss: 54, Average loss: 7.879340114416899\n",
      "Epoch: 196, Len of Training loss: 18, Average loss: 7.745469782087538\n",
      "Len of Validation loss: 54, Average loss: 8.778234865930346\n",
      "Epoch: 197, Len of Training loss: 18, Average loss: 8.039816353056166\n",
      "Len of Validation loss: 54, Average loss: 8.2861317175406\n",
      "Epoch: 198, Len of Training loss: 18, Average loss: 7.7999803225199384\n",
      "Len of Validation loss: 54, Average loss: 8.169321320675037\n",
      "Epoch: 199, Len of Training loss: 18, Average loss: 7.518009450700548\n",
      "Len of Validation loss: 54, Average loss: 7.48197842085803\n",
      "Epoch: 200, Len of Training loss: 18, Average loss: 7.431943602032131\n",
      "Len of Validation loss: 54, Average loss: 7.220683084593879\n",
      "Epoch: 201, Len of Training loss: 18, Average loss: 7.163319216834174\n",
      "Len of Validation loss: 54, Average loss: 7.280726415139657\n",
      "Epoch: 202, Len of Training loss: 18, Average loss: 7.201018147998386\n",
      "Len of Validation loss: 54, Average loss: 7.927851871207908\n",
      "Epoch: 203, Len of Training loss: 18, Average loss: 7.671231216854519\n",
      "Len of Validation loss: 54, Average loss: 7.507125501279478\n",
      "Epoch: 204, Len of Training loss: 18, Average loss: 7.232258637746175\n",
      "Len of Validation loss: 54, Average loss: 7.410644809405009\n",
      "Epoch: 205, Len of Training loss: 18, Average loss: 7.16881537437439\n",
      "Len of Validation loss: 54, Average loss: 7.051820785910995\n",
      "Epoch: 206, Len of Training loss: 18, Average loss: 7.203136523564656\n",
      "Len of Validation loss: 54, Average loss: 7.25151601544133\n",
      "Epoch: 207, Len of Training loss: 18, Average loss: 7.224570698208279\n",
      "Len of Validation loss: 54, Average loss: 7.557483324298152\n",
      "Epoch: 208, Len of Training loss: 18, Average loss: 7.280013216866387\n",
      "Len of Validation loss: 54, Average loss: 7.652243658348366\n",
      "Epoch: 209, Len of Training loss: 18, Average loss: 7.327295753690931\n",
      "Len of Validation loss: 54, Average loss: 7.199072630317123\n",
      "Epoch: 210, Len of Training loss: 18, Average loss: 7.228222211201985\n",
      "Len of Validation loss: 54, Average loss: 7.347129269882485\n",
      "Epoch: 211, Len of Training loss: 18, Average loss: 7.580366214116414\n",
      "Len of Validation loss: 54, Average loss: 7.714995035418758\n",
      "Epoch: 212, Len of Training loss: 18, Average loss: 7.86922640270657\n",
      "Len of Validation loss: 54, Average loss: 7.504423556504427\n",
      "Epoch: 213, Len of Training loss: 18, Average loss: 7.939288642671373\n",
      "Len of Validation loss: 54, Average loss: 7.928151916574548\n",
      "Epoch: 214, Len of Training loss: 18, Average loss: 7.769710275861952\n",
      "Len of Validation loss: 54, Average loss: 7.606361684975801\n",
      "Epoch: 215, Len of Training loss: 18, Average loss: 7.545049826304118\n",
      "Len of Validation loss: 54, Average loss: 7.269735265661169\n",
      "Epoch: 216, Len of Training loss: 18, Average loss: 7.23598591486613\n",
      "Len of Validation loss: 54, Average loss: 7.593828444127683\n",
      "Epoch: 217, Len of Training loss: 18, Average loss: 7.1958938174777565\n",
      "Len of Validation loss: 54, Average loss: 7.75990049927323\n",
      "Epoch: 218, Len of Training loss: 18, Average loss: 7.311994234720866\n",
      "Len of Validation loss: 54, Average loss: 7.327877155056706\n",
      "Epoch: 219, Len of Training loss: 18, Average loss: 8.435603936513266\n",
      "Len of Validation loss: 54, Average loss: 13.34416561656528\n",
      "Epoch: 220, Len of Training loss: 18, Average loss: 9.192787965138754\n",
      "Len of Validation loss: 54, Average loss: 8.858976664366546\n",
      "Epoch: 221, Len of Training loss: 18, Average loss: 8.99122659365336\n",
      "Len of Validation loss: 54, Average loss: 9.137339901041102\n",
      "Epoch: 222, Len of Training loss: 18, Average loss: 10.831919882032606\n",
      "Len of Validation loss: 54, Average loss: 9.005990235893815\n",
      "Epoch: 223, Len of Training loss: 18, Average loss: 8.543307410346138\n",
      "Len of Validation loss: 54, Average loss: 7.841214568526657\n",
      "Epoch: 224, Len of Training loss: 18, Average loss: 7.726502895355225\n",
      "Len of Validation loss: 54, Average loss: 7.344300283326043\n",
      "Epoch: 225, Len of Training loss: 18, Average loss: 7.664338482750787\n",
      "Len of Validation loss: 54, Average loss: 8.116173302685773\n",
      "Epoch: 226, Len of Training loss: 18, Average loss: 7.795104927486843\n",
      "Len of Validation loss: 54, Average loss: 7.228032730243824\n",
      "Epoch: 227, Len of Training loss: 18, Average loss: 7.49723203976949\n",
      "Len of Validation loss: 54, Average loss: 7.303459096837927\n",
      "Epoch: 228, Len of Training loss: 18, Average loss: 7.410663074917263\n",
      "Len of Validation loss: 54, Average loss: 7.456191508858292\n",
      "Epoch: 229, Len of Training loss: 18, Average loss: 7.7782359388139515\n",
      "Len of Validation loss: 54, Average loss: 7.917504359174658\n",
      "Epoch: 230, Len of Training loss: 18, Average loss: 7.51695900493198\n",
      "Len of Validation loss: 54, Average loss: 9.158855901824104\n",
      "Epoch: 231, Len of Training loss: 18, Average loss: 7.675907135009766\n",
      "Len of Validation loss: 54, Average loss: 7.3961166275872126\n",
      "Epoch: 232, Len of Training loss: 18, Average loss: 7.617223315768772\n",
      "Len of Validation loss: 54, Average loss: 7.898540448259424\n",
      "Epoch: 233, Len of Training loss: 18, Average loss: 7.53858741124471\n",
      "Len of Validation loss: 54, Average loss: 7.466460466384888\n",
      "Epoch: 234, Len of Training loss: 18, Average loss: 7.827341079711914\n",
      "Len of Validation loss: 54, Average loss: 9.698140091366238\n",
      "Epoch: 235, Len of Training loss: 18, Average loss: 7.885329378975762\n",
      "Len of Validation loss: 54, Average loss: 7.519660689212658\n",
      "Epoch: 236, Len of Training loss: 18, Average loss: 7.292147795359294\n",
      "Len of Validation loss: 54, Average loss: 7.292855677781282\n",
      "Epoch: 237, Len of Training loss: 18, Average loss: 7.359256029129028\n",
      "Len of Validation loss: 54, Average loss: 7.348971993834884\n",
      "Epoch: 238, Len of Training loss: 18, Average loss: 7.530759811401367\n",
      "Len of Validation loss: 54, Average loss: 8.120118732805606\n",
      "Epoch: 239, Len of Training loss: 18, Average loss: 7.310667620764838\n",
      "Len of Validation loss: 54, Average loss: 7.179576719248736\n",
      "Epoch: 240, Len of Training loss: 18, Average loss: 7.102424409654406\n",
      "Len of Validation loss: 54, Average loss: 7.302814845685606\n",
      "Epoch: 241, Len of Training loss: 18, Average loss: 7.313569704691569\n",
      "Len of Validation loss: 54, Average loss: 7.363576946435152\n",
      "Epoch: 242, Len of Training loss: 18, Average loss: 7.4151062435574\n",
      "Len of Validation loss: 54, Average loss: 7.960731872805843\n",
      "Epoch: 243, Len of Training loss: 18, Average loss: 7.31884225209554\n",
      "Len of Validation loss: 54, Average loss: 7.402522895071241\n",
      "Epoch: 244, Len of Training loss: 18, Average loss: 7.52620267868042\n",
      "Len of Validation loss: 54, Average loss: 7.676480430143851\n",
      "Epoch: 245, Len of Training loss: 18, Average loss: 7.468456718656752\n",
      "Len of Validation loss: 54, Average loss: 7.15732404479274\n",
      "Epoch: 246, Len of Training loss: 18, Average loss: 7.163892189661662\n",
      "Len of Validation loss: 54, Average loss: 7.655460649066502\n",
      "Epoch: 247, Len of Training loss: 18, Average loss: 7.372071610556708\n",
      "Len of Validation loss: 54, Average loss: 7.364442233686094\n",
      "Epoch: 248, Len of Training loss: 18, Average loss: 7.310883601506551\n",
      "Len of Validation loss: 54, Average loss: 8.749541746245491\n",
      "Epoch: 249, Len of Training loss: 18, Average loss: 7.1218761867947045\n",
      "Len of Validation loss: 54, Average loss: 7.336753474341498\n",
      "Epoch: 250, Len of Training loss: 18, Average loss: 7.262959718704224\n",
      "Len of Validation loss: 54, Average loss: 7.513953924179077\n",
      "Epoch: 251, Len of Training loss: 18, Average loss: 7.449314090940687\n",
      "Len of Validation loss: 54, Average loss: 7.129934990847552\n",
      "Epoch: 252, Len of Training loss: 18, Average loss: 7.192618555492825\n",
      "Len of Validation loss: 54, Average loss: 7.5196233104776455\n",
      "Epoch: 253, Len of Training loss: 18, Average loss: 7.4368409050835504\n",
      "Len of Validation loss: 54, Average loss: 7.190899716483222\n",
      "Epoch: 254, Len of Training loss: 18, Average loss: 7.138447390662299\n",
      "Len of Validation loss: 54, Average loss: 7.319188638969704\n",
      "Epoch: 255, Len of Training loss: 18, Average loss: 7.160757250256008\n",
      "Len of Validation loss: 54, Average loss: 7.602535172745034\n",
      "Epoch: 256, Len of Training loss: 18, Average loss: 7.242463986078898\n",
      "Len of Validation loss: 54, Average loss: 6.912239701659591\n",
      "Epoch: 257, Len of Training loss: 18, Average loss: 6.9940888616773815\n",
      "Len of Validation loss: 54, Average loss: 8.1491298366476\n",
      "Epoch: 258, Len of Training loss: 18, Average loss: 7.260319179958767\n",
      "Len of Validation loss: 54, Average loss: 7.69828521322321\n",
      "Epoch: 259, Len of Training loss: 18, Average loss: 7.326196485095554\n",
      "Len of Validation loss: 54, Average loss: 7.316234186843589\n",
      "Epoch: 260, Len of Training loss: 18, Average loss: 7.210917764239841\n",
      "Len of Validation loss: 54, Average loss: 7.312756538391113\n",
      "Epoch: 261, Len of Training loss: 18, Average loss: 7.00387069914076\n",
      "Len of Validation loss: 54, Average loss: 8.002943696799102\n",
      "Epoch: 262, Len of Training loss: 18, Average loss: 7.359934541914198\n",
      "Len of Validation loss: 54, Average loss: 7.037857938695837\n",
      "Epoch: 263, Len of Training loss: 18, Average loss: 7.176908095677693\n",
      "Len of Validation loss: 54, Average loss: 7.337842552750199\n",
      "Epoch: 264, Len of Training loss: 18, Average loss: 7.201460573408339\n",
      "Len of Validation loss: 54, Average loss: 7.915271551520736\n",
      "Epoch: 265, Len of Training loss: 18, Average loss: 7.314563910166423\n",
      "Len of Validation loss: 54, Average loss: 7.25271941114355\n",
      "Epoch: 266, Len of Training loss: 18, Average loss: 7.141104963090685\n",
      "Len of Validation loss: 54, Average loss: 7.158714201715258\n",
      "Epoch: 267, Len of Training loss: 18, Average loss: 7.070109844207764\n",
      "Len of Validation loss: 54, Average loss: 7.154731163272151\n",
      "Epoch: 268, Len of Training loss: 18, Average loss: 7.327152623070611\n",
      "Len of Validation loss: 54, Average loss: 7.366830816975346\n",
      "Epoch: 269, Len of Training loss: 18, Average loss: 7.197716289096409\n",
      "Len of Validation loss: 54, Average loss: 7.750564482476976\n",
      "Epoch: 270, Len of Training loss: 18, Average loss: 7.090114302105373\n",
      "Len of Validation loss: 54, Average loss: 7.9224365066598965\n",
      "Epoch: 271, Len of Training loss: 18, Average loss: 7.27774092886183\n",
      "Len of Validation loss: 54, Average loss: 8.396048510516131\n",
      "Epoch: 272, Len of Training loss: 18, Average loss: 7.3892900413937035\n",
      "Len of Validation loss: 54, Average loss: 7.638197232175757\n",
      "Epoch: 273, Len of Training loss: 18, Average loss: 7.1709902551439075\n",
      "Len of Validation loss: 54, Average loss: 7.325741423500909\n",
      "Epoch: 274, Len of Training loss: 18, Average loss: 7.220630990134345\n",
      "Len of Validation loss: 54, Average loss: 7.554633749855889\n",
      "Epoch: 275, Len of Training loss: 18, Average loss: 7.042435990439521\n",
      "Len of Validation loss: 54, Average loss: 7.248129438470911\n",
      "Epoch: 276, Len of Training loss: 18, Average loss: 6.98862870534261\n",
      "Len of Validation loss: 54, Average loss: 7.0630600496574685\n",
      "Epoch: 277, Len of Training loss: 18, Average loss: 6.872470484839545\n",
      "Len of Validation loss: 54, Average loss: 7.355672041575114\n",
      "Epoch: 278, Len of Training loss: 18, Average loss: 7.223786724938287\n",
      "Len of Validation loss: 54, Average loss: 8.37135225755197\n",
      "Epoch: 279, Len of Training loss: 18, Average loss: 7.3408457438151045\n",
      "Len of Validation loss: 54, Average loss: 7.489925781885783\n",
      "Epoch: 280, Len of Training loss: 18, Average loss: 6.992190387513903\n",
      "Len of Validation loss: 54, Average loss: 7.0423681073718605\n",
      "Epoch: 281, Len of Training loss: 18, Average loss: 7.001021915011936\n",
      "Len of Validation loss: 54, Average loss: 7.3240139616860285\n",
      "Epoch: 282, Len of Training loss: 18, Average loss: 6.913680765363905\n",
      "Len of Validation loss: 54, Average loss: 7.851406852404277\n",
      "Epoch: 283, Len of Training loss: 18, Average loss: 7.190495279100206\n",
      "Len of Validation loss: 54, Average loss: 7.685751341007374\n",
      "Epoch: 284, Len of Training loss: 18, Average loss: 7.396622180938721\n",
      "Len of Validation loss: 54, Average loss: 7.823639269228335\n",
      "Epoch: 285, Len of Training loss: 18, Average loss: 7.054772588941786\n",
      "Len of Validation loss: 54, Average loss: 7.332809421751234\n",
      "Epoch: 286, Len of Training loss: 18, Average loss: 7.140157646603054\n",
      "Len of Validation loss: 54, Average loss: 7.524540181513186\n",
      "Epoch: 287, Len of Training loss: 18, Average loss: 7.10339715745714\n",
      "Len of Validation loss: 54, Average loss: 7.291709997035839\n",
      "Epoch: 288, Len of Training loss: 18, Average loss: 6.946793264812893\n",
      "Len of Validation loss: 54, Average loss: 7.486005756590101\n",
      "Epoch: 289, Len of Training loss: 18, Average loss: 7.2983837657504615\n",
      "Len of Validation loss: 54, Average loss: 7.056565465750517\n",
      "Epoch: 290, Len of Training loss: 18, Average loss: 7.1555496321784124\n",
      "Len of Validation loss: 54, Average loss: 7.141849694428621\n",
      "Epoch: 291, Len of Training loss: 18, Average loss: 7.270995643403795\n",
      "Len of Validation loss: 54, Average loss: 7.974645137786865\n",
      "Epoch: 292, Len of Training loss: 18, Average loss: 7.368124352561103\n",
      "Len of Validation loss: 54, Average loss: 7.395062053645098\n",
      "Epoch: 293, Len of Training loss: 18, Average loss: 7.003663380940755\n",
      "Len of Validation loss: 54, Average loss: 7.563619110319349\n",
      "Epoch: 294, Len of Training loss: 18, Average loss: 7.115438090430366\n",
      "Len of Validation loss: 54, Average loss: 7.040274103482564\n",
      "Epoch: 295, Len of Training loss: 18, Average loss: 6.956797228919135\n",
      "Len of Validation loss: 54, Average loss: 7.41947615146637\n",
      "Epoch: 296, Len of Training loss: 18, Average loss: 6.9631987942589655\n",
      "Len of Validation loss: 54, Average loss: 6.885101084355955\n",
      "Epoch: 297, Len of Training loss: 18, Average loss: 6.96474814414978\n",
      "Len of Validation loss: 54, Average loss: 7.0992641184065075\n",
      "Epoch: 298, Len of Training loss: 18, Average loss: 7.069117678536309\n",
      "Len of Validation loss: 54, Average loss: 7.249038687458745\n",
      "Epoch: 299, Len of Training loss: 18, Average loss: 7.079824950959948\n",
      "Len of Validation loss: 54, Average loss: 7.569199133802344\n",
      "Epoch: 300, Len of Training loss: 18, Average loss: 7.393749025132921\n",
      "Len of Validation loss: 54, Average loss: 7.4736801474182695\n",
      "Epoch: 301, Len of Training loss: 18, Average loss: 6.9219438499874535\n",
      "Len of Validation loss: 54, Average loss: 7.059604353374905\n",
      "Epoch: 302, Len of Training loss: 18, Average loss: 6.875057644314236\n",
      "Len of Validation loss: 54, Average loss: 7.211463548518993\n",
      "Epoch: 303, Len of Training loss: 18, Average loss: 7.004689375559489\n",
      "Len of Validation loss: 54, Average loss: 7.049441085921393\n",
      "Epoch: 304, Len of Training loss: 18, Average loss: 6.992429653803508\n",
      "Len of Validation loss: 54, Average loss: 7.29559611832654\n",
      "Epoch: 305, Len of Training loss: 18, Average loss: 7.199679374694824\n",
      "Len of Validation loss: 54, Average loss: 7.374847023575394\n",
      "Epoch: 306, Len of Training loss: 18, Average loss: 7.127805100546943\n",
      "Len of Validation loss: 54, Average loss: 7.507679674360487\n",
      "Epoch: 307, Len of Training loss: 18, Average loss: 7.30850346883138\n",
      "Len of Validation loss: 54, Average loss: 7.6787959955356735\n",
      "Epoch: 308, Len of Training loss: 18, Average loss: 7.020340124766032\n",
      "Len of Validation loss: 54, Average loss: 7.106425876970644\n",
      "Epoch: 309, Len of Training loss: 18, Average loss: 6.8988512886895075\n",
      "Len of Validation loss: 54, Average loss: 7.7511274726302535\n",
      "Epoch: 310, Len of Training loss: 18, Average loss: 6.905411296420628\n",
      "Len of Validation loss: 54, Average loss: 7.102276144204317\n",
      "Epoch: 311, Len of Training loss: 18, Average loss: 6.794293244679769\n",
      "Len of Validation loss: 54, Average loss: 7.238282212504634\n",
      "Epoch: 312, Len of Training loss: 18, Average loss: 6.874806774987115\n",
      "Len of Validation loss: 54, Average loss: 7.14162023420687\n",
      "Epoch: 313, Len of Training loss: 18, Average loss: 6.961312823825413\n",
      "Len of Validation loss: 54, Average loss: 7.233659315992285\n",
      "Epoch: 314, Len of Training loss: 18, Average loss: 7.034157196680705\n",
      "Len of Validation loss: 54, Average loss: 7.207275355303729\n",
      "Epoch: 315, Len of Training loss: 18, Average loss: 6.83206221792433\n",
      "Len of Validation loss: 54, Average loss: 7.080712296344616\n",
      "Epoch: 316, Len of Training loss: 18, Average loss: 6.922529511981541\n",
      "Len of Validation loss: 54, Average loss: 7.084955727612531\n",
      "Epoch: 317, Len of Training loss: 18, Average loss: 6.884274853600396\n",
      "Len of Validation loss: 54, Average loss: 7.107475598653157\n",
      "Epoch: 318, Len of Training loss: 18, Average loss: 6.885620488060845\n",
      "Len of Validation loss: 54, Average loss: 7.260107102217497\n",
      "Epoch: 319, Len of Training loss: 18, Average loss: 6.947596364551121\n",
      "Len of Validation loss: 54, Average loss: 7.633425103293525\n",
      "Epoch: 320, Len of Training loss: 18, Average loss: 6.953144073486328\n",
      "Len of Validation loss: 54, Average loss: 7.209948932683027\n",
      "Epoch: 321, Len of Training loss: 18, Average loss: 6.807032161288792\n",
      "Len of Validation loss: 54, Average loss: 7.108474669633089\n",
      "Epoch: 322, Len of Training loss: 18, Average loss: 7.086789449055989\n",
      "Len of Validation loss: 54, Average loss: 7.039853714130543\n",
      "Epoch: 323, Len of Training loss: 18, Average loss: 7.024708403481378\n",
      "Len of Validation loss: 54, Average loss: 7.6189610030916\n",
      "Epoch: 324, Len of Training loss: 18, Average loss: 7.1233365005917015\n",
      "Len of Validation loss: 54, Average loss: 7.274187847420022\n",
      "Epoch: 325, Len of Training loss: 18, Average loss: 6.78756931093004\n",
      "Len of Validation loss: 54, Average loss: 7.140692459212409\n",
      "Epoch: 326, Len of Training loss: 18, Average loss: 6.915614869859484\n",
      "Len of Validation loss: 54, Average loss: 7.382950261787132\n",
      "Epoch: 327, Len of Training loss: 18, Average loss: 7.120773447884454\n",
      "Len of Validation loss: 54, Average loss: 7.319601010393213\n",
      "Epoch: 328, Len of Training loss: 18, Average loss: 6.878290600246853\n",
      "Len of Validation loss: 54, Average loss: 7.10764193976367\n",
      "Epoch: 329, Len of Training loss: 18, Average loss: 6.913675996992323\n",
      "Len of Validation loss: 54, Average loss: 7.211438925177963\n",
      "Epoch: 330, Len of Training loss: 18, Average loss: 6.974753141403198\n",
      "Len of Validation loss: 54, Average loss: 7.507246966715212\n",
      "Epoch: 331, Len of Training loss: 18, Average loss: 6.886510769526164\n",
      "Len of Validation loss: 54, Average loss: 7.273256103197734\n",
      "Epoch: 332, Len of Training loss: 18, Average loss: 6.990676853391859\n",
      "Len of Validation loss: 54, Average loss: 7.810152804410016\n",
      "Epoch: 333, Len of Training loss: 18, Average loss: 6.92158121532864\n",
      "Len of Validation loss: 54, Average loss: 7.274036416301021\n",
      "Epoch: 334, Len of Training loss: 18, Average loss: 6.693522320853339\n",
      "Len of Validation loss: 54, Average loss: 7.564611973585905\n",
      "Epoch: 335, Len of Training loss: 18, Average loss: 6.741799592971802\n",
      "Len of Validation loss: 54, Average loss: 6.988885146600229\n",
      "Epoch: 336, Len of Training loss: 18, Average loss: 6.7201412253909645\n",
      "Len of Validation loss: 54, Average loss: 7.679518244884632\n",
      "Epoch: 337, Len of Training loss: 18, Average loss: 7.109879202312893\n",
      "Len of Validation loss: 54, Average loss: 7.465142303042942\n",
      "Epoch: 338, Len of Training loss: 18, Average loss: 7.21804404258728\n",
      "Len of Validation loss: 54, Average loss: 7.071107745170593\n",
      "Epoch: 339, Len of Training loss: 18, Average loss: 7.069659948348999\n",
      "Len of Validation loss: 54, Average loss: 7.884758432706197\n",
      "Epoch: 340, Len of Training loss: 18, Average loss: 6.943467246161567\n",
      "Len of Validation loss: 54, Average loss: 7.654640246320654\n",
      "Epoch: 341, Len of Training loss: 18, Average loss: 7.359189086490208\n",
      "Len of Validation loss: 54, Average loss: 6.990446302625868\n",
      "Epoch: 342, Len of Training loss: 18, Average loss: 6.913448863559299\n",
      "Len of Validation loss: 54, Average loss: 7.189709230705544\n",
      "Epoch: 343, Len of Training loss: 18, Average loss: 6.772423558764988\n",
      "Len of Validation loss: 54, Average loss: 6.992508164158574\n",
      "Epoch: 344, Len of Training loss: 18, Average loss: 6.852916161219279\n",
      "Len of Validation loss: 54, Average loss: 7.399330002290231\n",
      "Epoch: 345, Len of Training loss: 18, Average loss: 6.8131085766686335\n",
      "Len of Validation loss: 54, Average loss: 7.057368830398277\n",
      "Epoch: 346, Len of Training loss: 18, Average loss: 6.765342712402344\n",
      "Len of Validation loss: 54, Average loss: 6.836466166708204\n",
      "Epoch: 347, Len of Training loss: 18, Average loss: 6.652690304650201\n",
      "Len of Validation loss: 54, Average loss: 7.592944476339552\n",
      "Epoch: 348, Len of Training loss: 18, Average loss: 6.5963446034325495\n",
      "Len of Validation loss: 54, Average loss: 7.03629309159738\n",
      "Epoch: 349, Len of Training loss: 18, Average loss: 6.67123982641432\n",
      "Len of Validation loss: 54, Average loss: 6.844507142349526\n",
      "Epoch: 350, Len of Training loss: 18, Average loss: 6.684902191162109\n",
      "Len of Validation loss: 54, Average loss: 7.597612248526679\n",
      "Epoch: 351, Len of Training loss: 18, Average loss: 6.707307603624132\n",
      "Len of Validation loss: 54, Average loss: 7.102272170561331\n",
      "Epoch: 352, Len of Training loss: 18, Average loss: 6.857805914349026\n",
      "Len of Validation loss: 54, Average loss: 7.747891359859043\n",
      "Epoch: 353, Len of Training loss: 18, Average loss: 6.997449265586005\n",
      "Len of Validation loss: 54, Average loss: 7.453933004979734\n",
      "Epoch: 354, Len of Training loss: 18, Average loss: 7.0192628966437445\n",
      "Len of Validation loss: 54, Average loss: 7.415443120179353\n",
      "Epoch: 355, Len of Training loss: 18, Average loss: 6.957232713699341\n",
      "Len of Validation loss: 54, Average loss: 7.118161642992938\n",
      "Epoch: 356, Len of Training loss: 18, Average loss: 6.635119729571873\n",
      "Len of Validation loss: 54, Average loss: 7.412406435719243\n",
      "Epoch: 357, Len of Training loss: 18, Average loss: 6.7973001797993975\n",
      "Len of Validation loss: 54, Average loss: 7.337553752793206\n",
      "Epoch: 358, Len of Training loss: 18, Average loss: 7.2722371154361305\n",
      "Len of Validation loss: 54, Average loss: 7.612981425391303\n",
      "Epoch: 359, Len of Training loss: 18, Average loss: 6.876566118664211\n",
      "Len of Validation loss: 54, Average loss: 6.844537695248921\n",
      "Epoch: 360, Len of Training loss: 18, Average loss: 6.797431892818874\n",
      "Len of Validation loss: 54, Average loss: 7.5773841319260775\n",
      "Epoch: 361, Len of Training loss: 18, Average loss: 7.055654578738743\n",
      "Len of Validation loss: 54, Average loss: 7.279461595747206\n",
      "Epoch: 362, Len of Training loss: 18, Average loss: 6.915096865759955\n",
      "Len of Validation loss: 54, Average loss: 8.106209185388353\n",
      "Epoch: 363, Len of Training loss: 18, Average loss: 7.617890066570705\n",
      "Len of Validation loss: 54, Average loss: 8.866910771087364\n",
      "Epoch: 364, Len of Training loss: 18, Average loss: 7.430948416392009\n",
      "Len of Validation loss: 54, Average loss: 7.506911608907911\n",
      "Epoch: 365, Len of Training loss: 18, Average loss: 6.84438771671719\n",
      "Len of Validation loss: 54, Average loss: 7.123689099594399\n",
      "Epoch: 366, Len of Training loss: 18, Average loss: 6.919734080632527\n",
      "Len of Validation loss: 54, Average loss: 7.227347400453356\n",
      "Epoch: 367, Len of Training loss: 18, Average loss: 7.015899711185032\n",
      "Len of Validation loss: 54, Average loss: 7.224368329401369\n",
      "Epoch: 368, Len of Training loss: 18, Average loss: 6.943608283996582\n",
      "Len of Validation loss: 54, Average loss: 7.2308795672875865\n",
      "Epoch: 369, Len of Training loss: 18, Average loss: 6.96209094259474\n",
      "Len of Validation loss: 54, Average loss: 6.986044371569598\n",
      "Epoch: 370, Len of Training loss: 18, Average loss: 6.833611435360378\n",
      "Len of Validation loss: 54, Average loss: 7.159341798888312\n",
      "Epoch: 371, Len of Training loss: 18, Average loss: 6.717362801233928\n",
      "Len of Validation loss: 54, Average loss: 7.244542311739038\n",
      "Epoch: 372, Len of Training loss: 18, Average loss: 6.814501947826809\n",
      "Len of Validation loss: 54, Average loss: 7.542989633701466\n",
      "Epoch: 373, Len of Training loss: 18, Average loss: 6.711049927605523\n",
      "Len of Validation loss: 54, Average loss: 6.971907920307583\n",
      "Epoch: 374, Len of Training loss: 18, Average loss: 6.549830781088935\n",
      "Len of Validation loss: 54, Average loss: 7.0177809706440675\n",
      "Epoch: 375, Len of Training loss: 18, Average loss: 6.553329971101549\n",
      "Len of Validation loss: 54, Average loss: 7.046670626710962\n",
      "Epoch: 376, Len of Training loss: 18, Average loss: 6.645950582292345\n",
      "Len of Validation loss: 54, Average loss: 6.824925917166251\n",
      "Epoch: 377, Len of Training loss: 18, Average loss: 6.564691172705756\n",
      "Len of Validation loss: 54, Average loss: 6.816306763225132\n",
      "Epoch: 378, Len of Training loss: 18, Average loss: 6.522180000940959\n",
      "Len of Validation loss: 54, Average loss: 6.889356701462357\n",
      "Epoch: 379, Len of Training loss: 18, Average loss: 6.792954524358113\n",
      "Len of Validation loss: 54, Average loss: 7.089309228791131\n",
      "Epoch: 380, Len of Training loss: 18, Average loss: 8.428155342737833\n",
      "Len of Validation loss: 54, Average loss: 8.678877640653539\n",
      "Epoch: 381, Len of Training loss: 18, Average loss: 18.81451967027452\n",
      "Len of Validation loss: 54, Average loss: 21.18639850616455\n",
      "Epoch: 382, Len of Training loss: 18, Average loss: 13.741075091891819\n",
      "Len of Validation loss: 54, Average loss: 10.148301963452939\n",
      "Epoch: 383, Len of Training loss: 18, Average loss: 9.05730708440145\n",
      "Len of Validation loss: 54, Average loss: 7.990227266594216\n",
      "Epoch: 384, Len of Training loss: 18, Average loss: 8.14442851808336\n",
      "Len of Validation loss: 54, Average loss: 7.577906074347319\n",
      "Epoch: 385, Len of Training loss: 18, Average loss: 8.082777553134495\n",
      "Len of Validation loss: 54, Average loss: 7.362148836806968\n",
      "Epoch: 386, Len of Training loss: 18, Average loss: 7.612107621298896\n",
      "Len of Validation loss: 54, Average loss: 7.23312313026852\n",
      "Epoch: 387, Len of Training loss: 18, Average loss: 7.286092440287272\n",
      "Len of Validation loss: 54, Average loss: 7.16240421489433\n",
      "Epoch: 388, Len of Training loss: 18, Average loss: 7.447274764378865\n",
      "Len of Validation loss: 54, Average loss: 7.523871510117142\n",
      "Epoch: 389, Len of Training loss: 18, Average loss: 7.60240986612108\n",
      "Len of Validation loss: 54, Average loss: 7.697093195385403\n",
      "Epoch: 390, Len of Training loss: 18, Average loss: 7.441982958051893\n",
      "Len of Validation loss: 54, Average loss: 7.198192931987621\n",
      "Epoch: 391, Len of Training loss: 18, Average loss: 7.6542962657080755\n",
      "Len of Validation loss: 54, Average loss: 9.598764287100899\n",
      "Epoch: 392, Len of Training loss: 18, Average loss: 7.842538356781006\n",
      "Len of Validation loss: 54, Average loss: 7.343290426112987\n",
      "Epoch: 393, Len of Training loss: 18, Average loss: 7.488690747155084\n",
      "Len of Validation loss: 54, Average loss: 7.018105533387926\n",
      "Epoch: 394, Len of Training loss: 18, Average loss: 7.051255332099067\n",
      "Len of Validation loss: 54, Average loss: 7.404085380059701\n",
      "Epoch: 395, Len of Training loss: 18, Average loss: 7.456522782643636\n",
      "Len of Validation loss: 54, Average loss: 7.243328200446235\n",
      "Epoch: 396, Len of Training loss: 18, Average loss: 7.194536050160726\n",
      "Len of Validation loss: 54, Average loss: 6.98331864674886\n",
      "Epoch: 397, Len of Training loss: 18, Average loss: 7.406484206517537\n",
      "Len of Validation loss: 54, Average loss: 7.222416392079106\n",
      "Epoch: 398, Len of Training loss: 18, Average loss: 7.222164577907986\n",
      "Len of Validation loss: 54, Average loss: 7.15443449108689\n",
      "Epoch: 399, Len of Training loss: 18, Average loss: 7.1175821092393665\n",
      "Len of Validation loss: 54, Average loss: 7.3553052875730724\n",
      "Epoch: 400, Len of Training loss: 18, Average loss: 7.628037505679661\n",
      "Len of Validation loss: 54, Average loss: 7.40056222014957\n",
      "Epoch: 401, Len of Training loss: 18, Average loss: 7.358238114251031\n",
      "Len of Validation loss: 54, Average loss: 7.192591817290695\n",
      "Epoch: 402, Len of Training loss: 18, Average loss: 7.270188675986396\n",
      "Len of Validation loss: 54, Average loss: 6.9767394993040295\n",
      "Epoch: 403, Len of Training loss: 18, Average loss: 7.0690693855285645\n",
      "Len of Validation loss: 54, Average loss: 7.0505293475257025\n",
      "Epoch: 404, Len of Training loss: 18, Average loss: 6.811069488525391\n",
      "Len of Validation loss: 54, Average loss: 7.006366155765675\n",
      "Epoch: 405, Len of Training loss: 18, Average loss: 7.162955045700073\n",
      "Len of Validation loss: 54, Average loss: 7.198076950179206\n",
      "Epoch: 406, Len of Training loss: 18, Average loss: 7.0322089195251465\n",
      "Len of Validation loss: 54, Average loss: 7.32339753486492\n",
      "Epoch: 407, Len of Training loss: 18, Average loss: 7.199440399805705\n",
      "Len of Validation loss: 54, Average loss: 7.821447213490804\n",
      "Epoch: 408, Len of Training loss: 18, Average loss: 7.1210207144419355\n",
      "Len of Validation loss: 54, Average loss: 7.491072950539766\n",
      "Epoch: 409, Len of Training loss: 18, Average loss: 6.986864699257745\n",
      "Len of Validation loss: 54, Average loss: 6.961355924606323\n",
      "Epoch: 410, Len of Training loss: 18, Average loss: 6.977254576153225\n",
      "Len of Validation loss: 54, Average loss: 7.022741238276164\n",
      "Epoch: 411, Len of Training loss: 18, Average loss: 6.881713999642266\n",
      "Len of Validation loss: 54, Average loss: 7.359622226821052\n",
      "Epoch: 412, Len of Training loss: 18, Average loss: 6.984062406751844\n",
      "Len of Validation loss: 54, Average loss: 7.18576858220277\n",
      "Epoch: 413, Len of Training loss: 18, Average loss: 6.803837405310737\n",
      "Len of Validation loss: 54, Average loss: 7.030860212114122\n",
      "Epoch: 414, Len of Training loss: 18, Average loss: 6.844118939505683\n",
      "Len of Validation loss: 54, Average loss: 7.068975082150212\n",
      "Epoch: 415, Len of Training loss: 18, Average loss: 7.022239605585734\n",
      "Len of Validation loss: 54, Average loss: 7.257079221584179\n",
      "Epoch: 416, Len of Training loss: 18, Average loss: 7.157607687844171\n",
      "Len of Validation loss: 54, Average loss: 7.527061325532419\n",
      "Epoch: 417, Len of Training loss: 18, Average loss: 7.1625111897786455\n",
      "Len of Validation loss: 54, Average loss: 6.966339217291938\n",
      "Epoch: 418, Len of Training loss: 18, Average loss: 6.931403027640449\n",
      "Len of Validation loss: 54, Average loss: 6.684152391221788\n",
      "Epoch: 419, Len of Training loss: 18, Average loss: 6.95186612341139\n",
      "Len of Validation loss: 54, Average loss: 7.955090390311347\n",
      "Epoch: 420, Len of Training loss: 18, Average loss: 6.928465551800198\n",
      "Len of Validation loss: 54, Average loss: 6.977880711908694\n",
      "Epoch: 421, Len of Training loss: 18, Average loss: 7.015924692153931\n",
      "Len of Validation loss: 54, Average loss: 7.772097057766384\n",
      "Epoch: 422, Len of Training loss: 18, Average loss: 6.872634410858154\n",
      "Len of Validation loss: 54, Average loss: 7.108934292086849\n",
      "Epoch: 423, Len of Training loss: 18, Average loss: 6.778537829717\n",
      "Len of Validation loss: 54, Average loss: 6.840863921024181\n",
      "Epoch: 424, Len of Training loss: 18, Average loss: 6.853390137354533\n",
      "Len of Validation loss: 54, Average loss: 6.86562626891666\n",
      "Epoch: 425, Len of Training loss: 18, Average loss: 6.729885366227892\n",
      "Len of Validation loss: 54, Average loss: 7.803351190355089\n",
      "Epoch: 426, Len of Training loss: 18, Average loss: 7.055562098821004\n",
      "Len of Validation loss: 54, Average loss: 7.471340561354602\n",
      "Epoch: 427, Len of Training loss: 18, Average loss: 7.116377062267727\n",
      "Len of Validation loss: 54, Average loss: 7.065359040542885\n",
      "Epoch: 428, Len of Training loss: 18, Average loss: 6.88206418355306\n",
      "Len of Validation loss: 54, Average loss: 6.805162239957739\n",
      "Epoch: 429, Len of Training loss: 18, Average loss: 6.719069745805529\n",
      "Len of Validation loss: 54, Average loss: 7.450749609205458\n",
      "Epoch: 430, Len of Training loss: 18, Average loss: 7.198915163675944\n",
      "Len of Validation loss: 54, Average loss: 6.909409169797544\n",
      "Epoch: 431, Len of Training loss: 18, Average loss: 6.887552393807305\n",
      "Len of Validation loss: 54, Average loss: 6.728243690949899\n",
      "Epoch: 432, Len of Training loss: 18, Average loss: 6.9619329240587025\n",
      "Len of Validation loss: 54, Average loss: 7.062748816278246\n",
      "Epoch: 433, Len of Training loss: 18, Average loss: 7.040806611378987\n",
      "Len of Validation loss: 54, Average loss: 6.841601601353398\n",
      "Epoch: 434, Len of Training loss: 18, Average loss: 6.701275693045722\n",
      "Len of Validation loss: 54, Average loss: 6.734904602721885\n",
      "Epoch: 435, Len of Training loss: 18, Average loss: 6.6465152104695635\n",
      "Len of Validation loss: 54, Average loss: 7.120426690136945\n",
      "Epoch: 436, Len of Training loss: 18, Average loss: 6.6769229835934105\n",
      "Len of Validation loss: 54, Average loss: 6.9415422633842185\n",
      "Epoch: 437, Len of Training loss: 18, Average loss: 6.639602608150906\n",
      "Len of Validation loss: 54, Average loss: 8.478053176844561\n",
      "Epoch: 438, Len of Training loss: 18, Average loss: 7.241156631045872\n",
      "Len of Validation loss: 54, Average loss: 7.3271695861109984\n",
      "Epoch: 439, Len of Training loss: 18, Average loss: 6.80972072813246\n",
      "Len of Validation loss: 54, Average loss: 6.846237416620608\n",
      "Epoch: 440, Len of Training loss: 18, Average loss: 7.282028701570299\n",
      "Len of Validation loss: 54, Average loss: 6.7442358025798095\n",
      "Epoch: 441, Len of Training loss: 18, Average loss: 6.985086149639553\n",
      "Len of Validation loss: 54, Average loss: 6.828162281601517\n",
      "Epoch: 442, Len of Training loss: 18, Average loss: 6.740162584516737\n",
      "Len of Validation loss: 54, Average loss: 7.146417039412039\n",
      "Epoch: 443, Len of Training loss: 18, Average loss: 6.739458878835042\n",
      "Len of Validation loss: 54, Average loss: 7.191717368585092\n",
      "Epoch: 444, Len of Training loss: 18, Average loss: 6.725361585617065\n",
      "Len of Validation loss: 54, Average loss: 6.875619031764843\n",
      "Epoch: 445, Len of Training loss: 18, Average loss: 6.726720333099365\n",
      "Len of Validation loss: 54, Average loss: 7.067808054111622\n",
      "Epoch: 446, Len of Training loss: 18, Average loss: 6.693903393215603\n",
      "Len of Validation loss: 54, Average loss: 6.865386243219729\n",
      "Epoch: 447, Len of Training loss: 18, Average loss: 6.81201958656311\n",
      "Len of Validation loss: 54, Average loss: 7.236454517753036\n",
      "Epoch: 448, Len of Training loss: 18, Average loss: 6.733096970452203\n",
      "Len of Validation loss: 54, Average loss: 7.077362758141977\n",
      "Epoch: 449, Len of Training loss: 18, Average loss: 6.760353194342719\n",
      "Len of Validation loss: 54, Average loss: 7.127048152464408\n",
      "Epoch: 450, Len of Training loss: 18, Average loss: 7.043078581492106\n",
      "Len of Validation loss: 54, Average loss: 7.218808752519113\n",
      "Epoch: 451, Len of Training loss: 18, Average loss: 6.690569056404962\n",
      "Len of Validation loss: 54, Average loss: 6.79882781152372\n",
      "Epoch: 452, Len of Training loss: 18, Average loss: 6.659412913852268\n",
      "Len of Validation loss: 54, Average loss: 7.009127881791857\n",
      "Epoch: 453, Len of Training loss: 18, Average loss: 7.106187264124553\n",
      "Len of Validation loss: 54, Average loss: 8.82234690365968\n",
      "Epoch: 454, Len of Training loss: 18, Average loss: 7.680464426676433\n",
      "Len of Validation loss: 54, Average loss: 7.52800197954531\n",
      "Epoch: 455, Len of Training loss: 18, Average loss: 7.270372046364678\n",
      "Len of Validation loss: 54, Average loss: 8.332087013456556\n",
      "Epoch: 456, Len of Training loss: 18, Average loss: 7.346467521455553\n",
      "Len of Validation loss: 54, Average loss: 7.394949670191164\n",
      "Epoch: 457, Len of Training loss: 18, Average loss: 6.862890587912665\n",
      "Len of Validation loss: 54, Average loss: 7.174507900520608\n",
      "Epoch: 458, Len of Training loss: 18, Average loss: 6.666611856884426\n",
      "Len of Validation loss: 54, Average loss: 6.847243141244959\n",
      "Epoch: 459, Len of Training loss: 18, Average loss: 6.678773429658678\n",
      "Len of Validation loss: 54, Average loss: 7.19220918196219\n",
      "Epoch: 460, Len of Training loss: 18, Average loss: 6.592759582731459\n",
      "Len of Validation loss: 54, Average loss: 7.074496816705774\n",
      "Epoch: 461, Len of Training loss: 18, Average loss: 6.901912212371826\n",
      "Len of Validation loss: 54, Average loss: 7.177904358616582\n",
      "Epoch: 462, Len of Training loss: 18, Average loss: 6.737591743469238\n",
      "Len of Validation loss: 54, Average loss: 7.102088605916059\n",
      "Epoch: 463, Len of Training loss: 18, Average loss: 6.682557582855225\n",
      "Len of Validation loss: 54, Average loss: 7.749187323782179\n",
      "Epoch: 464, Len of Training loss: 18, Average loss: 6.987412452697754\n",
      "Len of Validation loss: 54, Average loss: 7.919084275210345\n",
      "Epoch: 465, Len of Training loss: 18, Average loss: 7.240960703955756\n",
      "Len of Validation loss: 54, Average loss: 7.3828100937384145\n",
      "Epoch: 466, Len of Training loss: 18, Average loss: 6.811850176917182\n",
      "Len of Validation loss: 54, Average loss: 7.065452906820509\n",
      "Epoch: 467, Len of Training loss: 18, Average loss: 6.920572784211901\n",
      "Len of Validation loss: 54, Average loss: 7.06069658420704\n",
      "Epoch: 468, Len of Training loss: 18, Average loss: 6.684464984469944\n",
      "Len of Validation loss: 54, Average loss: 6.977070600898178\n",
      "Epoch: 469, Len of Training loss: 18, Average loss: 6.65798478656345\n",
      "Len of Validation loss: 54, Average loss: 7.371753493944804\n",
      "Epoch: 470, Len of Training loss: 18, Average loss: 6.502869447072347\n",
      "Len of Validation loss: 54, Average loss: 6.891219660087868\n",
      "Epoch: 471, Len of Training loss: 18, Average loss: 6.604987568325466\n",
      "Len of Validation loss: 54, Average loss: 7.16881823098218\n",
      "Epoch: 472, Len of Training loss: 18, Average loss: 6.968009127510919\n",
      "Len of Validation loss: 54, Average loss: 6.866391332061203\n",
      "Epoch: 473, Len of Training loss: 18, Average loss: 6.520740773942736\n",
      "Len of Validation loss: 54, Average loss: 7.118490099906921\n",
      "Epoch: 474, Len of Training loss: 18, Average loss: 6.747365156809489\n",
      "Len of Validation loss: 54, Average loss: 7.64766471474259\n",
      "Epoch: 475, Len of Training loss: 18, Average loss: 6.7139398786756725\n",
      "Len of Validation loss: 54, Average loss: 6.855966991848415\n",
      "Epoch: 476, Len of Training loss: 18, Average loss: 6.624974436230129\n",
      "Len of Validation loss: 54, Average loss: 7.509149083384761\n",
      "Epoch: 477, Len of Training loss: 18, Average loss: 6.680801921420628\n",
      "Len of Validation loss: 54, Average loss: 7.897015536272967\n",
      "Epoch: 478, Len of Training loss: 18, Average loss: 7.068351719114515\n",
      "Len of Validation loss: 54, Average loss: 7.670020854031598\n",
      "Epoch: 479, Len of Training loss: 18, Average loss: 6.893973641925388\n",
      "Len of Validation loss: 54, Average loss: 7.093257621482566\n",
      "Epoch: 480, Len of Training loss: 18, Average loss: 6.6982526779174805\n",
      "Len of Validation loss: 54, Average loss: 6.924917366769579\n",
      "Epoch: 481, Len of Training loss: 18, Average loss: 6.5805792808532715\n",
      "Len of Validation loss: 54, Average loss: 6.774507451940466\n",
      "Epoch: 482, Len of Training loss: 18, Average loss: 6.448739290237427\n",
      "Len of Validation loss: 54, Average loss: 6.826239025151288\n",
      "Epoch: 483, Len of Training loss: 18, Average loss: 6.542825592888726\n",
      "Len of Validation loss: 54, Average loss: 6.97219525443183\n",
      "Epoch: 484, Len of Training loss: 18, Average loss: 6.612340609232585\n",
      "Len of Validation loss: 54, Average loss: 7.329340872941194\n",
      "Epoch: 485, Len of Training loss: 18, Average loss: 6.568039655685425\n",
      "Len of Validation loss: 54, Average loss: 6.9300703295954955\n",
      "Epoch: 486, Len of Training loss: 18, Average loss: 6.632180982165867\n",
      "Len of Validation loss: 54, Average loss: 6.820486386617024\n",
      "Epoch: 487, Len of Training loss: 18, Average loss: 6.8214048279656305\n",
      "Len of Validation loss: 54, Average loss: 7.504851204377633\n",
      "Epoch: 488, Len of Training loss: 18, Average loss: 6.46418931749132\n",
      "Len of Validation loss: 54, Average loss: 6.970859836649011\n",
      "Epoch: 489, Len of Training loss: 18, Average loss: 6.47290121184455\n",
      "Len of Validation loss: 54, Average loss: 7.838334754661277\n",
      "Epoch: 490, Len of Training loss: 18, Average loss: 6.7875870068868\n",
      "Len of Validation loss: 54, Average loss: 6.964734779463874\n",
      "Epoch: 491, Len of Training loss: 18, Average loss: 6.972208314471775\n",
      "Len of Validation loss: 54, Average loss: 7.167695354532312\n",
      "Epoch: 492, Len of Training loss: 18, Average loss: 6.7274800936381025\n",
      "Len of Validation loss: 54, Average loss: 7.551524237350181\n",
      "Epoch: 493, Len of Training loss: 18, Average loss: 6.753280719121297\n",
      "Len of Validation loss: 54, Average loss: 7.334753482430069\n",
      "Epoch: 494, Len of Training loss: 18, Average loss: 6.852693054411146\n",
      "Len of Validation loss: 54, Average loss: 7.613270534409417\n",
      "Epoch: 495, Len of Training loss: 18, Average loss: 6.607194238238865\n",
      "Len of Validation loss: 54, Average loss: 7.226639959547255\n",
      "Epoch: 496, Len of Training loss: 18, Average loss: 6.416340933905707\n",
      "Len of Validation loss: 54, Average loss: 6.9339024093416\n",
      "Epoch: 497, Len of Training loss: 18, Average loss: 6.425009250640869\n",
      "Len of Validation loss: 54, Average loss: 6.7390628081780894\n",
      "Epoch: 498, Len of Training loss: 18, Average loss: 6.376000801722209\n",
      "Len of Validation loss: 54, Average loss: 6.840287341011895\n",
      "Epoch: 499, Len of Training loss: 18, Average loss: 6.709566566679213\n",
      "Len of Validation loss: 54, Average loss: 7.532159584539908\n",
      "Epoch: 500, Len of Training loss: 18, Average loss: 6.7506803671518965\n",
      "Len of Validation loss: 54, Average loss: 7.223962112709328\n",
      "Epoch: 501, Len of Training loss: 18, Average loss: 6.514651113086277\n",
      "Len of Validation loss: 54, Average loss: 7.603786698094121\n",
      "Epoch: 502, Len of Training loss: 18, Average loss: 6.565022521548801\n",
      "Len of Validation loss: 54, Average loss: 6.896262279263249\n",
      "Epoch: 503, Len of Training loss: 18, Average loss: 6.855468644036187\n",
      "Len of Validation loss: 54, Average loss: 7.253629825733326\n",
      "Epoch: 504, Len of Training loss: 18, Average loss: 6.9296839237213135\n",
      "Len of Validation loss: 54, Average loss: 6.921985109647115\n",
      "Epoch: 505, Len of Training loss: 18, Average loss: 6.739287694295247\n",
      "Len of Validation loss: 54, Average loss: 7.095956952483566\n",
      "Epoch: 506, Len of Training loss: 18, Average loss: 6.516621404223972\n",
      "Len of Validation loss: 54, Average loss: 6.921370718214247\n",
      "Epoch: 507, Len of Training loss: 18, Average loss: 6.4324718316396075\n",
      "Len of Validation loss: 54, Average loss: 7.314759815180743\n",
      "Epoch: 508, Len of Training loss: 18, Average loss: 6.6257809268103705\n",
      "Len of Validation loss: 54, Average loss: 7.4904145885396884\n",
      "Epoch: 509, Len of Training loss: 18, Average loss: 6.733090877532959\n",
      "Len of Validation loss: 54, Average loss: 8.231849829355875\n",
      "Epoch: 510, Len of Training loss: 18, Average loss: 6.929645776748657\n",
      "Len of Validation loss: 54, Average loss: 7.042771586665401\n",
      "Epoch: 511, Len of Training loss: 18, Average loss: 6.433704985512628\n",
      "Len of Validation loss: 54, Average loss: 7.007425555476436\n",
      "Epoch: 512, Len of Training loss: 18, Average loss: 6.4598228931427\n",
      "Len of Validation loss: 54, Average loss: 6.9147967011840255\n",
      "Epoch: 513, Len of Training loss: 18, Average loss: 6.450743065940009\n",
      "Len of Validation loss: 54, Average loss: 6.883905053138733\n",
      "Epoch: 514, Len of Training loss: 18, Average loss: 6.36182000901964\n",
      "Len of Validation loss: 54, Average loss: 6.956925718872635\n",
      "Epoch: 515, Len of Training loss: 18, Average loss: 6.336086008283827\n",
      "Len of Validation loss: 54, Average loss: 6.81901576783922\n",
      "Epoch: 516, Len of Training loss: 18, Average loss: 6.388141738043891\n",
      "Len of Validation loss: 54, Average loss: 6.9294266391683506\n",
      "Epoch: 517, Len of Training loss: 18, Average loss: 6.7324538230896\n",
      "Len of Validation loss: 54, Average loss: 6.909258228761178\n",
      "Epoch: 518, Len of Training loss: 18, Average loss: 6.482051107618544\n",
      "Len of Validation loss: 54, Average loss: 6.907969739702013\n",
      "Epoch: 519, Len of Training loss: 18, Average loss: 6.441528956095378\n",
      "Len of Validation loss: 54, Average loss: 7.019870201746623\n",
      "Epoch: 520, Len of Training loss: 18, Average loss: 6.5001400576697455\n",
      "Len of Validation loss: 54, Average loss: 7.190653792134038\n",
      "Epoch: 521, Len of Training loss: 18, Average loss: 6.821235471301609\n",
      "Len of Validation loss: 54, Average loss: 7.292346786569666\n",
      "Epoch: 522, Len of Training loss: 18, Average loss: 6.640368673536512\n",
      "Len of Validation loss: 54, Average loss: 6.640704415462635\n",
      "Epoch: 523, Len of Training loss: 18, Average loss: 6.518864207797581\n",
      "Len of Validation loss: 54, Average loss: 6.75369252981963\n",
      "Epoch: 524, Len of Training loss: 18, Average loss: 6.549765215979682\n",
      "Len of Validation loss: 54, Average loss: 6.606043581609373\n",
      "Epoch: 525, Len of Training loss: 18, Average loss: 6.279969135920207\n",
      "Len of Validation loss: 54, Average loss: 6.929505273147866\n",
      "Epoch: 526, Len of Training loss: 18, Average loss: 6.362435499827067\n",
      "Len of Validation loss: 54, Average loss: 7.02341522552349\n",
      "Epoch: 527, Len of Training loss: 18, Average loss: 6.386967950397068\n",
      "Len of Validation loss: 54, Average loss: 7.011644568708208\n",
      "Epoch: 528, Len of Training loss: 18, Average loss: 6.568030383851793\n",
      "Len of Validation loss: 54, Average loss: 6.7555315891901655\n",
      "Epoch: 529, Len of Training loss: 18, Average loss: 6.347917318344116\n",
      "Len of Validation loss: 54, Average loss: 6.8738876183827715\n",
      "Epoch: 530, Len of Training loss: 18, Average loss: 6.548132154676649\n",
      "Len of Validation loss: 54, Average loss: 7.299395614200169\n",
      "Epoch: 531, Len of Training loss: 18, Average loss: 6.420199049843682\n",
      "Len of Validation loss: 54, Average loss: 6.669371198724817\n",
      "Epoch: 532, Len of Training loss: 18, Average loss: 6.260701153013441\n",
      "Len of Validation loss: 54, Average loss: 6.841212259398566\n",
      "Epoch: 533, Len of Training loss: 18, Average loss: 6.426984866460164\n",
      "Len of Validation loss: 54, Average loss: 6.892363278954117\n",
      "Epoch: 534, Len of Training loss: 18, Average loss: 6.722575664520264\n",
      "Len of Validation loss: 54, Average loss: 7.103909236413461\n",
      "Epoch: 535, Len of Training loss: 18, Average loss: 6.37309455871582\n",
      "Len of Validation loss: 54, Average loss: 6.667836176024543\n",
      "Epoch: 536, Len of Training loss: 18, Average loss: 6.379767073525323\n",
      "Len of Validation loss: 54, Average loss: 6.8298463203288895\n",
      "Epoch: 537, Len of Training loss: 18, Average loss: 6.577696243921916\n",
      "Len of Validation loss: 54, Average loss: 6.69310321631255\n",
      "Epoch: 538, Len of Training loss: 18, Average loss: 6.519082811143663\n",
      "Len of Validation loss: 54, Average loss: 6.888867192798191\n",
      "Epoch: 539, Len of Training loss: 18, Average loss: 6.341867950227526\n",
      "Len of Validation loss: 54, Average loss: 6.925349831581116\n",
      "Epoch: 540, Len of Training loss: 18, Average loss: 6.509876463148329\n",
      "Len of Validation loss: 54, Average loss: 6.9740224635159525\n",
      "Epoch: 541, Len of Training loss: 18, Average loss: 6.485016928778754\n",
      "Len of Validation loss: 54, Average loss: 6.7907729193016335\n",
      "Epoch: 542, Len of Training loss: 18, Average loss: 6.609293831719293\n",
      "Len of Validation loss: 54, Average loss: 6.77792325284746\n",
      "Epoch: 543, Len of Training loss: 18, Average loss: 6.432595464918348\n",
      "Len of Validation loss: 54, Average loss: 6.922166736037643\n",
      "Epoch: 544, Len of Training loss: 18, Average loss: 6.276762035157946\n",
      "Len of Validation loss: 54, Average loss: 7.328147044888249\n",
      "Epoch: 545, Len of Training loss: 18, Average loss: 6.216381311416626\n",
      "Len of Validation loss: 54, Average loss: 6.683043011912593\n",
      "Epoch: 546, Len of Training loss: 18, Average loss: 6.1801515685187445\n",
      "Len of Validation loss: 54, Average loss: 6.705769181251526\n",
      "Epoch: 547, Len of Training loss: 18, Average loss: 6.29752614763048\n",
      "Len of Validation loss: 54, Average loss: 6.872664853378579\n",
      "Epoch: 548, Len of Training loss: 18, Average loss: 6.278360737694634\n",
      "Len of Validation loss: 54, Average loss: 7.800531789108559\n",
      "Epoch: 549, Len of Training loss: 18, Average loss: 6.822087367375691\n",
      "Len of Validation loss: 54, Average loss: 8.439553185745522\n",
      "Epoch: 550, Len of Training loss: 18, Average loss: 6.607706440819634\n",
      "Len of Validation loss: 54, Average loss: 7.346246781172575\n",
      "Epoch: 551, Len of Training loss: 18, Average loss: 7.024865998162164\n",
      "Len of Validation loss: 54, Average loss: 7.50908840585638\n",
      "Epoch: 552, Len of Training loss: 18, Average loss: 6.744721015294393\n",
      "Len of Validation loss: 54, Average loss: 7.084404172720732\n",
      "Epoch: 553, Len of Training loss: 18, Average loss: 6.664679023954603\n",
      "Len of Validation loss: 54, Average loss: 6.707233914622554\n",
      "Epoch: 554, Len of Training loss: 18, Average loss: 6.407617780897352\n",
      "Len of Validation loss: 54, Average loss: 6.728728091275251\n",
      "Epoch: 555, Len of Training loss: 18, Average loss: 6.734318733215332\n",
      "Len of Validation loss: 54, Average loss: 7.722638841028567\n",
      "Epoch: 556, Len of Training loss: 18, Average loss: 7.593270460764567\n",
      "Len of Validation loss: 54, Average loss: 7.417055130004883\n",
      "Epoch: 557, Len of Training loss: 18, Average loss: 7.079678535461426\n",
      "Len of Validation loss: 54, Average loss: 7.12709081614459\n",
      "Epoch: 558, Len of Training loss: 18, Average loss: 6.617280933592054\n",
      "Len of Validation loss: 54, Average loss: 7.651641077465481\n",
      "Epoch: 559, Len of Training loss: 18, Average loss: 6.966658565733168\n",
      "Len of Validation loss: 54, Average loss: 6.942688407721342\n",
      "Epoch: 560, Len of Training loss: 18, Average loss: 6.897605313195123\n",
      "Len of Validation loss: 54, Average loss: 6.902478959825304\n",
      "Epoch: 561, Len of Training loss: 18, Average loss: 6.4201438426971436\n",
      "Len of Validation loss: 54, Average loss: 6.9352481453507036\n",
      "Epoch: 562, Len of Training loss: 18, Average loss: 6.392158932156033\n",
      "Len of Validation loss: 54, Average loss: 6.884905894597371\n",
      "Epoch: 563, Len of Training loss: 18, Average loss: 6.287006325191921\n",
      "Len of Validation loss: 54, Average loss: 6.533675648547985\n",
      "Epoch: 564, Len of Training loss: 18, Average loss: 6.244255012936062\n",
      "Len of Validation loss: 54, Average loss: 6.608869636500323\n",
      "Epoch: 565, Len of Training loss: 18, Average loss: 6.246547116173638\n",
      "Len of Validation loss: 54, Average loss: 6.81626417460265\n",
      "Epoch: 566, Len of Training loss: 18, Average loss: 6.373421483569675\n",
      "Len of Validation loss: 54, Average loss: 6.925838920805189\n",
      "Epoch: 567, Len of Training loss: 18, Average loss: 6.408559163411458\n",
      "Len of Validation loss: 54, Average loss: 6.530903171609949\n",
      "Epoch: 568, Len of Training loss: 18, Average loss: 6.184676594204372\n",
      "Len of Validation loss: 54, Average loss: 6.9096527099609375\n",
      "Epoch: 569, Len of Training loss: 18, Average loss: 6.197979715135363\n",
      "Len of Validation loss: 54, Average loss: 6.5998907265839755\n",
      "Epoch: 570, Len of Training loss: 18, Average loss: 6.109545363320245\n",
      "Len of Validation loss: 54, Average loss: 6.6329473433671176\n",
      "Epoch: 571, Len of Training loss: 18, Average loss: 6.437123696009318\n",
      "Len of Validation loss: 54, Average loss: 6.58293522728814\n",
      "Epoch: 572, Len of Training loss: 18, Average loss: 6.337281889385647\n",
      "Len of Validation loss: 54, Average loss: 6.6419319753293635\n",
      "Epoch: 573, Len of Training loss: 18, Average loss: 6.197800344891018\n",
      "Len of Validation loss: 54, Average loss: 6.783412483003405\n",
      "Epoch: 574, Len of Training loss: 18, Average loss: 6.174863179524739\n",
      "Len of Validation loss: 54, Average loss: 6.925657722685072\n",
      "Epoch: 575, Len of Training loss: 18, Average loss: 6.271054029464722\n",
      "Len of Validation loss: 54, Average loss: 8.21670060246079\n",
      "Epoch: 576, Len of Training loss: 18, Average loss: 6.462025907304552\n",
      "Len of Validation loss: 54, Average loss: 6.630067564823009\n",
      "Epoch: 577, Len of Training loss: 18, Average loss: 6.174189435111152\n",
      "Len of Validation loss: 54, Average loss: 6.736839859573929\n",
      "Epoch: 578, Len of Training loss: 18, Average loss: 6.109572516547309\n",
      "Len of Validation loss: 54, Average loss: 6.660784920056661\n",
      "Epoch: 579, Len of Training loss: 18, Average loss: 6.114932881461249\n",
      "Len of Validation loss: 54, Average loss: 6.841735040699994\n",
      "Epoch: 580, Len of Training loss: 18, Average loss: 6.317328717973497\n",
      "Len of Validation loss: 54, Average loss: 6.99815798247302\n",
      "Epoch: 581, Len of Training loss: 18, Average loss: 6.229569382137722\n",
      "Len of Validation loss: 54, Average loss: 6.594868854240135\n",
      "Epoch: 582, Len of Training loss: 18, Average loss: 6.061222341325548\n",
      "Len of Validation loss: 54, Average loss: 6.542586432562934\n",
      "Epoch: 583, Len of Training loss: 18, Average loss: 6.168824937608507\n",
      "Len of Validation loss: 54, Average loss: 6.776993950208028\n",
      "Epoch: 584, Len of Training loss: 18, Average loss: 6.321697420544094\n",
      "Len of Validation loss: 54, Average loss: 7.872391744896218\n",
      "Epoch: 585, Len of Training loss: 18, Average loss: 6.185031970342\n",
      "Len of Validation loss: 54, Average loss: 6.432546240312082\n",
      "Epoch: 586, Len of Training loss: 18, Average loss: 6.209263987011379\n",
      "Len of Validation loss: 54, Average loss: 6.73787126717744\n",
      "Epoch: 587, Len of Training loss: 18, Average loss: 6.259670284059313\n",
      "Len of Validation loss: 54, Average loss: 7.497849376113327\n",
      "Epoch: 588, Len of Training loss: 18, Average loss: 6.276202413770887\n",
      "Len of Validation loss: 54, Average loss: 6.67680350056401\n",
      "Epoch: 589, Len of Training loss: 18, Average loss: 6.264478074179755\n",
      "Len of Validation loss: 54, Average loss: 6.915057195557488\n",
      "Epoch: 590, Len of Training loss: 18, Average loss: 6.131014744440715\n",
      "Len of Validation loss: 54, Average loss: 6.755005792335227\n",
      "Epoch: 591, Len of Training loss: 18, Average loss: 6.011234442392985\n",
      "Len of Validation loss: 54, Average loss: 6.576196794156675\n",
      "Epoch: 592, Len of Training loss: 18, Average loss: 6.043308999803331\n",
      "Len of Validation loss: 54, Average loss: 6.618405863090798\n",
      "Epoch: 593, Len of Training loss: 18, Average loss: 6.055970907211304\n",
      "Len of Validation loss: 54, Average loss: 6.491556295642146\n",
      "Epoch: 594, Len of Training loss: 18, Average loss: 6.164767954084608\n",
      "Len of Validation loss: 54, Average loss: 6.611496095304136\n",
      "Epoch: 595, Len of Training loss: 18, Average loss: 6.376744402779473\n",
      "Len of Validation loss: 54, Average loss: 6.8467302808055175\n",
      "Epoch: 596, Len of Training loss: 18, Average loss: 6.579362048043145\n",
      "Len of Validation loss: 54, Average loss: 6.931806785088998\n",
      "Epoch: 597, Len of Training loss: 18, Average loss: 6.163920349544949\n",
      "Len of Validation loss: 54, Average loss: 6.826883280718768\n",
      "Epoch: 598, Len of Training loss: 18, Average loss: 5.992869748009576\n",
      "Len of Validation loss: 54, Average loss: 6.774485212785226\n",
      "Epoch: 599, Len of Training loss: 18, Average loss: 5.960808727476332\n",
      "Len of Validation loss: 54, Average loss: 6.907979559015344\n",
      "Epoch: 600, Len of Training loss: 18, Average loss: 6.127582523557875\n",
      "Len of Validation loss: 54, Average loss: 6.575726712191546\n",
      "Epoch: 601, Len of Training loss: 18, Average loss: 6.146907276577419\n",
      "Len of Validation loss: 54, Average loss: 6.487973592899464\n",
      "Epoch: 602, Len of Training loss: 18, Average loss: 6.156684954961141\n",
      "Len of Validation loss: 54, Average loss: 7.415312850916827\n",
      "Epoch: 603, Len of Training loss: 18, Average loss: 6.33740316496955\n",
      "Len of Validation loss: 54, Average loss: 7.165067160571063\n",
      "Epoch: 604, Len of Training loss: 18, Average loss: 6.411185423533122\n",
      "Len of Validation loss: 54, Average loss: 6.922344168027242\n",
      "Epoch: 605, Len of Training loss: 18, Average loss: 6.354999674691094\n",
      "Len of Validation loss: 54, Average loss: 7.396986912797998\n",
      "Epoch: 606, Len of Training loss: 18, Average loss: 6.414258533053928\n",
      "Len of Validation loss: 54, Average loss: 6.831926553337662\n",
      "Epoch: 607, Len of Training loss: 18, Average loss: 6.238708204693264\n",
      "Len of Validation loss: 54, Average loss: 6.698769988837065\n",
      "Epoch: 608, Len of Training loss: 18, Average loss: 6.108675638834636\n",
      "Len of Validation loss: 54, Average loss: 6.503987082728633\n",
      "Epoch: 609, Len of Training loss: 18, Average loss: 6.027531597349379\n",
      "Len of Validation loss: 54, Average loss: 7.115899196377507\n",
      "Epoch: 610, Len of Training loss: 18, Average loss: 6.178278658125135\n",
      "Len of Validation loss: 54, Average loss: 6.70369236557572\n",
      "Epoch: 611, Len of Training loss: 18, Average loss: 6.051024383968777\n",
      "Len of Validation loss: 54, Average loss: 6.600713049923932\n",
      "Epoch: 612, Len of Training loss: 18, Average loss: 6.0534670617845325\n",
      "Len of Validation loss: 54, Average loss: 6.491957121425205\n",
      "Epoch: 613, Len of Training loss: 18, Average loss: 5.994893895255195\n",
      "Len of Validation loss: 54, Average loss: 6.8530005481508045\n",
      "Epoch: 614, Len of Training loss: 18, Average loss: 6.034940878550212\n",
      "Len of Validation loss: 54, Average loss: 6.845148572215328\n",
      "Epoch: 615, Len of Training loss: 18, Average loss: 6.139588647418552\n",
      "Len of Validation loss: 54, Average loss: 6.492506645343922\n",
      "Epoch: 616, Len of Training loss: 18, Average loss: 5.974795553419325\n",
      "Len of Validation loss: 54, Average loss: 6.6315792136722145\n",
      "Epoch: 617, Len of Training loss: 18, Average loss: 6.015629159079658\n",
      "Len of Validation loss: 54, Average loss: 6.78386276739615\n",
      "Epoch: 618, Len of Training loss: 18, Average loss: 6.395984835094875\n",
      "Len of Validation loss: 54, Average loss: 6.933324063265765\n",
      "Epoch: 619, Len of Training loss: 18, Average loss: 6.093045128716363\n",
      "Len of Validation loss: 54, Average loss: 6.546261133971037\n",
      "Epoch: 620, Len of Training loss: 18, Average loss: 6.059140523274739\n",
      "Len of Validation loss: 54, Average loss: 6.399967582137497\n",
      "Epoch: 621, Len of Training loss: 18, Average loss: 6.005291912290785\n",
      "Len of Validation loss: 54, Average loss: 6.419006206371166\n",
      "Epoch: 622, Len of Training loss: 18, Average loss: 6.029302226172553\n",
      "Len of Validation loss: 54, Average loss: 7.321696488945572\n",
      "Epoch: 623, Len of Training loss: 18, Average loss: 6.4121449788411455\n",
      "Len of Validation loss: 54, Average loss: 7.063781477786876\n",
      "Epoch: 624, Len of Training loss: 18, Average loss: 6.687205102708605\n",
      "Len of Validation loss: 54, Average loss: 6.971587348867346\n",
      "Epoch: 625, Len of Training loss: 18, Average loss: 6.162844737370809\n",
      "Len of Validation loss: 54, Average loss: 6.791468871964349\n",
      "Epoch: 626, Len of Training loss: 18, Average loss: 6.105472379260593\n",
      "Len of Validation loss: 54, Average loss: 6.590948078367445\n",
      "Epoch: 627, Len of Training loss: 18, Average loss: 6.02573135164049\n",
      "Len of Validation loss: 54, Average loss: 6.794162432352702\n",
      "Epoch: 628, Len of Training loss: 18, Average loss: 6.051128917270237\n",
      "Len of Validation loss: 54, Average loss: 6.9030152559280396\n",
      "Epoch: 629, Len of Training loss: 18, Average loss: 6.182431009080675\n",
      "Len of Validation loss: 54, Average loss: 6.597617979402895\n",
      "Epoch: 630, Len of Training loss: 18, Average loss: 5.9867233700222435\n",
      "Len of Validation loss: 54, Average loss: 6.695144401656257\n",
      "Epoch: 631, Len of Training loss: 18, Average loss: 5.796329074435764\n",
      "Len of Validation loss: 54, Average loss: 6.360717852910359\n",
      "Epoch: 632, Len of Training loss: 18, Average loss: 5.843385987811619\n",
      "Len of Validation loss: 54, Average loss: 6.771351562605964\n",
      "Epoch: 633, Len of Training loss: 18, Average loss: 5.991069555282593\n",
      "Len of Validation loss: 54, Average loss: 6.552192992634243\n",
      "Epoch: 634, Len of Training loss: 18, Average loss: 5.80569322903951\n",
      "Len of Validation loss: 54, Average loss: 6.794544679147226\n",
      "Epoch: 635, Len of Training loss: 18, Average loss: 5.883934842215644\n",
      "Len of Validation loss: 54, Average loss: 6.458363616908038\n",
      "Epoch: 636, Len of Training loss: 18, Average loss: 5.814574162165324\n",
      "Len of Validation loss: 54, Average loss: 6.519828496155916\n",
      "Epoch: 637, Len of Training loss: 18, Average loss: 5.886974334716797\n",
      "Len of Validation loss: 54, Average loss: 6.774720858644556\n",
      "Epoch: 638, Len of Training loss: 18, Average loss: 6.047800223032634\n",
      "Len of Validation loss: 54, Average loss: 7.219789010507089\n",
      "Epoch: 639, Len of Training loss: 18, Average loss: 6.2933008670806885\n",
      "Len of Validation loss: 54, Average loss: 6.511045787069532\n",
      "Epoch: 640, Len of Training loss: 18, Average loss: 6.088763289981419\n",
      "Len of Validation loss: 54, Average loss: 6.581398619545831\n",
      "Epoch: 641, Len of Training loss: 18, Average loss: 6.073604398303562\n",
      "Len of Validation loss: 54, Average loss: 7.308375954627991\n",
      "Epoch: 642, Len of Training loss: 18, Average loss: 6.261685954199897\n",
      "Len of Validation loss: 54, Average loss: 6.8912138100023625\n",
      "Epoch: 643, Len of Training loss: 18, Average loss: 6.007453997929891\n",
      "Len of Validation loss: 54, Average loss: 6.850199085694772\n",
      "Epoch: 644, Len of Training loss: 18, Average loss: 5.744174692365858\n",
      "Len of Validation loss: 54, Average loss: 6.421178076002333\n",
      "Epoch: 645, Len of Training loss: 18, Average loss: 5.670759518941243\n",
      "Len of Validation loss: 54, Average loss: 6.720287159637168\n",
      "Epoch: 646, Len of Training loss: 18, Average loss: 5.835307359695435\n",
      "Len of Validation loss: 54, Average loss: 6.795843150880602\n",
      "Epoch: 647, Len of Training loss: 18, Average loss: 6.023853222529094\n",
      "Len of Validation loss: 54, Average loss: 6.475874035446732\n",
      "Epoch: 648, Len of Training loss: 18, Average loss: 5.947000079684788\n",
      "Len of Validation loss: 54, Average loss: 6.357287305372733\n",
      "Epoch: 649, Len of Training loss: 18, Average loss: 5.682857195536296\n",
      "Len of Validation loss: 54, Average loss: 6.6669164719405\n",
      "Epoch: 650, Len of Training loss: 18, Average loss: 5.593139860365126\n",
      "Len of Validation loss: 54, Average loss: 6.416499937022174\n",
      "Epoch: 651, Len of Training loss: 18, Average loss: 5.643670426474677\n",
      "Len of Validation loss: 54, Average loss: 6.496007053940384\n",
      "Epoch: 652, Len of Training loss: 18, Average loss: 5.951774650149876\n",
      "Len of Validation loss: 54, Average loss: 6.285778469509548\n",
      "Epoch: 653, Len of Training loss: 18, Average loss: 5.915195359124078\n",
      "Len of Validation loss: 54, Average loss: 6.635055166703683\n",
      "Epoch: 654, Len of Training loss: 18, Average loss: 6.047999938329061\n",
      "Len of Validation loss: 54, Average loss: 6.539233322496767\n",
      "Epoch: 655, Len of Training loss: 18, Average loss: 6.042206539048089\n",
      "Len of Validation loss: 54, Average loss: 6.308076425834939\n",
      "Epoch: 656, Len of Training loss: 18, Average loss: 5.728764639960395\n",
      "Len of Validation loss: 54, Average loss: 7.14874607986874\n",
      "Epoch: 657, Len of Training loss: 18, Average loss: 5.9221861362457275\n",
      "Len of Validation loss: 54, Average loss: 6.496931045143692\n",
      "Epoch: 658, Len of Training loss: 18, Average loss: 5.733283016416761\n",
      "Len of Validation loss: 54, Average loss: 6.893853059521428\n",
      "Epoch: 659, Len of Training loss: 18, Average loss: 5.768499533335368\n",
      "Len of Validation loss: 54, Average loss: 6.504791153801812\n",
      "Epoch: 660, Len of Training loss: 18, Average loss: 5.90278500980801\n",
      "Len of Validation loss: 54, Average loss: 6.260593162642585\n",
      "Epoch: 661, Len of Training loss: 18, Average loss: 5.724214341905382\n",
      "Len of Validation loss: 54, Average loss: 6.301325374179417\n",
      "Epoch: 662, Len of Training loss: 18, Average loss: 5.881864123874241\n",
      "Len of Validation loss: 54, Average loss: 6.444754984643724\n",
      "Epoch: 663, Len of Training loss: 18, Average loss: 5.821337752872044\n",
      "Len of Validation loss: 54, Average loss: 6.298723966987045\n",
      "Epoch: 664, Len of Training loss: 18, Average loss: 5.718630525800917\n",
      "Len of Validation loss: 54, Average loss: 6.118178597203007\n",
      "Epoch: 665, Len of Training loss: 18, Average loss: 5.6090078883700905\n",
      "Len of Validation loss: 54, Average loss: 6.430234630902608\n",
      "Epoch: 666, Len of Training loss: 18, Average loss: 5.752916918860541\n",
      "Len of Validation loss: 54, Average loss: 6.650552802615696\n",
      "Epoch: 667, Len of Training loss: 18, Average loss: 5.787004232406616\n",
      "Len of Validation loss: 54, Average loss: 6.1699952990920455\n",
      "Epoch: 668, Len of Training loss: 18, Average loss: 5.659288909700182\n",
      "Len of Validation loss: 54, Average loss: 6.678739300480595\n",
      "Epoch: 669, Len of Training loss: 18, Average loss: 5.562641978263855\n",
      "Len of Validation loss: 54, Average loss: 6.600946691301134\n",
      "Epoch: 670, Len of Training loss: 18, Average loss: 5.730643404854669\n",
      "Len of Validation loss: 54, Average loss: 6.4212910510875565\n",
      "Epoch: 671, Len of Training loss: 18, Average loss: 5.750207000308567\n",
      "Len of Validation loss: 54, Average loss: 6.558936180891814\n",
      "Epoch: 672, Len of Training loss: 18, Average loss: 5.6517471472422285\n",
      "Len of Validation loss: 54, Average loss: 6.111325018935734\n",
      "Epoch: 673, Len of Training loss: 18, Average loss: 6.073762125439114\n",
      "Len of Validation loss: 54, Average loss: 6.618135787822582\n",
      "Epoch: 674, Len of Training loss: 18, Average loss: 6.0712049802144366\n",
      "Len of Validation loss: 54, Average loss: 6.63021782150975\n",
      "Epoch: 675, Len of Training loss: 18, Average loss: 5.810602294074164\n",
      "Len of Validation loss: 54, Average loss: 6.432975106769138\n",
      "Epoch: 676, Len of Training loss: 18, Average loss: 5.957540565066868\n",
      "Len of Validation loss: 54, Average loss: 6.2355594502555\n",
      "Epoch: 677, Len of Training loss: 18, Average loss: 5.788910494910346\n",
      "Len of Validation loss: 54, Average loss: 6.644284023178948\n",
      "Epoch: 678, Len of Training loss: 18, Average loss: 5.800113942888048\n",
      "Len of Validation loss: 54, Average loss: 6.279184292863916\n",
      "Epoch: 679, Len of Training loss: 18, Average loss: 5.59878675142924\n",
      "Len of Validation loss: 54, Average loss: 6.295032827942459\n",
      "Epoch: 680, Len of Training loss: 18, Average loss: 5.553757826487224\n",
      "Len of Validation loss: 54, Average loss: 6.430496388011509\n",
      "Epoch: 681, Len of Training loss: 18, Average loss: 5.532427204979791\n",
      "Len of Validation loss: 54, Average loss: 6.274081336127387\n",
      "Epoch: 682, Len of Training loss: 18, Average loss: 5.446139004495409\n",
      "Len of Validation loss: 54, Average loss: 6.28833536307017\n",
      "Epoch: 683, Len of Training loss: 18, Average loss: 5.436905410554674\n",
      "Len of Validation loss: 54, Average loss: 6.29576732494213\n",
      "Epoch: 684, Len of Training loss: 18, Average loss: 5.556782378090753\n",
      "Len of Validation loss: 54, Average loss: 6.139637691003305\n",
      "Epoch: 685, Len of Training loss: 18, Average loss: 5.555489354663425\n",
      "Len of Validation loss: 54, Average loss: 6.269692076577081\n",
      "Epoch: 686, Len of Training loss: 18, Average loss: 5.448353793885973\n",
      "Len of Validation loss: 54, Average loss: 6.194122265886377\n",
      "Epoch: 687, Len of Training loss: 18, Average loss: 5.476377460691664\n",
      "Len of Validation loss: 54, Average loss: 6.2389691096765025\n",
      "Epoch: 688, Len of Training loss: 18, Average loss: 5.784868213865492\n",
      "Len of Validation loss: 54, Average loss: 6.389178863278142\n",
      "Epoch: 689, Len of Training loss: 18, Average loss: 6.056969006856282\n",
      "Len of Validation loss: 54, Average loss: 6.52249249264046\n",
      "Epoch: 690, Len of Training loss: 18, Average loss: 5.820503314336141\n",
      "Len of Validation loss: 54, Average loss: 6.606826186180115\n",
      "Epoch: 691, Len of Training loss: 18, Average loss: 6.941015137566461\n",
      "Len of Validation loss: 54, Average loss: 8.032284860257748\n",
      "Epoch: 692, Len of Training loss: 18, Average loss: 8.61290431022644\n",
      "Len of Validation loss: 54, Average loss: 8.144454717636108\n",
      "Epoch: 693, Len of Training loss: 18, Average loss: 7.76262805196974\n",
      "Len of Validation loss: 54, Average loss: 7.487563755777147\n",
      "Epoch: 694, Len of Training loss: 18, Average loss: 7.147355265087551\n",
      "Len of Validation loss: 54, Average loss: 6.8525799689469515\n",
      "Epoch: 695, Len of Training loss: 18, Average loss: 6.817302068074544\n",
      "Len of Validation loss: 54, Average loss: 6.81903401127568\n",
      "Epoch: 696, Len of Training loss: 18, Average loss: 6.37953715854221\n",
      "Len of Validation loss: 54, Average loss: 6.542662302652995\n",
      "Epoch: 697, Len of Training loss: 18, Average loss: 6.186674329969618\n",
      "Len of Validation loss: 54, Average loss: 6.497852232721117\n",
      "Epoch: 698, Len of Training loss: 18, Average loss: 6.011566718419393\n",
      "Len of Validation loss: 54, Average loss: 6.696231616867913\n",
      "Epoch: 699, Len of Training loss: 18, Average loss: 5.8802047040727405\n",
      "Len of Validation loss: 54, Average loss: 6.9497785568237305\n",
      "Epoch: 700, Len of Training loss: 18, Average loss: 5.7748805152045355\n",
      "Len of Validation loss: 54, Average loss: 6.526700492258425\n",
      "Epoch: 701, Len of Training loss: 18, Average loss: 5.666919814215766\n",
      "Len of Validation loss: 54, Average loss: 6.281825202482718\n",
      "Epoch: 702, Len of Training loss: 18, Average loss: 5.736236015955607\n",
      "Len of Validation loss: 54, Average loss: 6.845723134500009\n",
      "Epoch: 703, Len of Training loss: 18, Average loss: 6.612584961785211\n",
      "Len of Validation loss: 54, Average loss: 6.449341694513957\n",
      "Epoch: 704, Len of Training loss: 18, Average loss: 5.864613162146674\n",
      "Len of Validation loss: 54, Average loss: 6.170577393637763\n",
      "Epoch: 705, Len of Training loss: 18, Average loss: 5.613574186960856\n",
      "Len of Validation loss: 54, Average loss: 6.315054191483392\n",
      "Epoch: 706, Len of Training loss: 18, Average loss: 5.594070487552219\n",
      "Len of Validation loss: 54, Average loss: 6.528228909881027\n",
      "Epoch: 707, Len of Training loss: 18, Average loss: 5.573319461610582\n",
      "Len of Validation loss: 54, Average loss: 6.0745301290794655\n",
      "Epoch: 708, Len of Training loss: 18, Average loss: 5.553246392144097\n",
      "Len of Validation loss: 54, Average loss: 6.310955003455833\n",
      "Epoch: 709, Len of Training loss: 18, Average loss: 5.455094416936238\n",
      "Len of Validation loss: 54, Average loss: 6.0895877105218394\n",
      "Epoch: 710, Len of Training loss: 18, Average loss: 5.526730669869317\n",
      "Len of Validation loss: 54, Average loss: 6.252860082520379\n",
      "Epoch: 711, Len of Training loss: 18, Average loss: 5.641709221733941\n",
      "Len of Validation loss: 54, Average loss: 6.190892687550297\n",
      "Epoch: 712, Len of Training loss: 18, Average loss: 5.491930325826009\n",
      "Len of Validation loss: 54, Average loss: 6.350265410211351\n",
      "Epoch: 713, Len of Training loss: 18, Average loss: 5.42657028304206\n",
      "Len of Validation loss: 54, Average loss: 6.012855754958259\n",
      "Epoch: 714, Len of Training loss: 18, Average loss: 5.359797610176934\n",
      "Len of Validation loss: 54, Average loss: 6.097957761199386\n",
      "Epoch: 715, Len of Training loss: 18, Average loss: 5.287026882171631\n",
      "Len of Validation loss: 54, Average loss: 5.971064673529731\n",
      "Epoch: 716, Len of Training loss: 18, Average loss: 5.472238302230835\n",
      "Len of Validation loss: 54, Average loss: 6.040427817238702\n",
      "Epoch: 717, Len of Training loss: 18, Average loss: 5.4893709023793535\n",
      "Len of Validation loss: 54, Average loss: 6.661951480088411\n",
      "Epoch: 718, Len of Training loss: 18, Average loss: 5.811837434768677\n",
      "Len of Validation loss: 54, Average loss: 6.509115974108378\n",
      "Epoch: 719, Len of Training loss: 18, Average loss: 5.917777750227186\n",
      "Len of Validation loss: 54, Average loss: 6.367394849106118\n",
      "Epoch: 720, Len of Training loss: 18, Average loss: 5.850515312618679\n",
      "Len of Validation loss: 54, Average loss: 6.363036853295785\n",
      "Epoch: 721, Len of Training loss: 18, Average loss: 5.678521937794155\n",
      "Len of Validation loss: 54, Average loss: 6.409441581478825\n",
      "Epoch: 722, Len of Training loss: 18, Average loss: 5.395534197489421\n",
      "Len of Validation loss: 54, Average loss: 6.340001830348262\n",
      "Epoch: 723, Len of Training loss: 18, Average loss: 5.398661481009589\n",
      "Len of Validation loss: 54, Average loss: 6.2587615825511795\n",
      "Epoch: 724, Len of Training loss: 18, Average loss: 5.394006358252631\n",
      "Len of Validation loss: 54, Average loss: 6.065442478215253\n",
      "Epoch: 725, Len of Training loss: 18, Average loss: 5.247316307491726\n",
      "Len of Validation loss: 54, Average loss: 6.231762497513382\n",
      "Epoch: 726, Len of Training loss: 18, Average loss: 5.2285688983069525\n",
      "Len of Validation loss: 54, Average loss: 6.353393263287014\n",
      "Epoch: 727, Len of Training loss: 18, Average loss: 5.247535202238295\n",
      "Len of Validation loss: 54, Average loss: 6.046132617526585\n",
      "Epoch: 728, Len of Training loss: 18, Average loss: 5.168750339084202\n",
      "Len of Validation loss: 54, Average loss: 6.0606731529589055\n",
      "Epoch: 729, Len of Training loss: 18, Average loss: 5.283520354164971\n",
      "Len of Validation loss: 54, Average loss: 6.933162552339059\n",
      "Epoch: 730, Len of Training loss: 18, Average loss: 5.892125182681614\n",
      "Len of Validation loss: 54, Average loss: 6.927299287584093\n",
      "Epoch: 731, Len of Training loss: 18, Average loss: 5.8181760046217175\n",
      "Len of Validation loss: 54, Average loss: 6.441318931402983\n",
      "Epoch: 732, Len of Training loss: 18, Average loss: 5.507113138834636\n",
      "Len of Validation loss: 54, Average loss: 6.178177343474494\n",
      "Epoch: 733, Len of Training loss: 18, Average loss: 5.371781759791904\n",
      "Len of Validation loss: 54, Average loss: 6.01437399563966\n",
      "Epoch: 734, Len of Training loss: 18, Average loss: 5.401765320036146\n",
      "Len of Validation loss: 54, Average loss: 6.026005647800587\n",
      "Epoch: 735, Len of Training loss: 18, Average loss: 5.3042136033376055\n",
      "Len of Validation loss: 54, Average loss: 6.33924220667945\n",
      "Epoch: 736, Len of Training loss: 18, Average loss: 5.294827063878377\n",
      "Len of Validation loss: 54, Average loss: 6.362261569058454\n",
      "Epoch: 737, Len of Training loss: 18, Average loss: 5.2760120497809515\n",
      "Len of Validation loss: 54, Average loss: 6.165424302772239\n",
      "Epoch: 738, Len of Training loss: 18, Average loss: 5.471631871329413\n",
      "Len of Validation loss: 54, Average loss: 6.154638051986694\n",
      "Epoch: 739, Len of Training loss: 18, Average loss: 5.3655775388081866\n",
      "Len of Validation loss: 54, Average loss: 6.676009540204649\n",
      "Epoch: 740, Len of Training loss: 18, Average loss: 5.226631058586968\n",
      "Len of Validation loss: 54, Average loss: 5.972523808479309\n",
      "Epoch: 741, Len of Training loss: 18, Average loss: 5.505771530999078\n",
      "Len of Validation loss: 54, Average loss: 6.160051941871643\n",
      "Epoch: 742, Len of Training loss: 18, Average loss: 5.243039316601223\n",
      "Len of Validation loss: 54, Average loss: 5.978455283023693\n",
      "Epoch: 743, Len of Training loss: 18, Average loss: 5.485985146628486\n",
      "Len of Validation loss: 54, Average loss: 5.994338821481775\n",
      "Epoch: 744, Len of Training loss: 18, Average loss: 5.2937995592753095\n",
      "Len of Validation loss: 54, Average loss: 6.286106515813757\n",
      "Epoch: 745, Len of Training loss: 18, Average loss: 5.739472733603583\n",
      "Len of Validation loss: 54, Average loss: 6.4275035460789995\n",
      "Epoch: 746, Len of Training loss: 18, Average loss: 6.436947928534614\n",
      "Len of Validation loss: 54, Average loss: 6.782852481912683\n",
      "Epoch: 747, Len of Training loss: 18, Average loss: 6.499329328536987\n",
      "Len of Validation loss: 54, Average loss: 6.393455898320234\n",
      "Epoch: 748, Len of Training loss: 18, Average loss: 6.080859369701809\n",
      "Len of Validation loss: 54, Average loss: 6.879349655575222\n",
      "Epoch: 749, Len of Training loss: 18, Average loss: 6.422882636388143\n",
      "Len of Validation loss: 54, Average loss: 6.708530178776494\n",
      "Epoch: 750, Len of Training loss: 18, Average loss: 5.848705662621392\n",
      "Len of Validation loss: 54, Average loss: 6.523643542219092\n",
      "Epoch: 751, Len of Training loss: 18, Average loss: 5.722488747702704\n",
      "Len of Validation loss: 54, Average loss: 6.355160518928811\n",
      "Epoch: 752, Len of Training loss: 18, Average loss: 5.613819652133518\n",
      "Len of Validation loss: 54, Average loss: 6.708694065058673\n",
      "Epoch: 753, Len of Training loss: 18, Average loss: 5.69148498111301\n",
      "Len of Validation loss: 54, Average loss: 6.356663783391316\n",
      "Epoch: 754, Len of Training loss: 18, Average loss: 5.299488398763868\n",
      "Len of Validation loss: 54, Average loss: 6.031314717398749\n",
      "Epoch: 755, Len of Training loss: 18, Average loss: 5.119361003239949\n",
      "Len of Validation loss: 54, Average loss: 6.17341544451537\n",
      "Epoch: 756, Len of Training loss: 18, Average loss: 5.123110877143012\n",
      "Len of Validation loss: 54, Average loss: 5.952076320294981\n",
      "Epoch: 757, Len of Training loss: 18, Average loss: 5.08692741394043\n",
      "Len of Validation loss: 54, Average loss: 5.804880438027559\n",
      "Epoch: 758, Len of Training loss: 18, Average loss: 5.020268413755629\n",
      "Len of Validation loss: 54, Average loss: 5.863749499674197\n",
      "Epoch: 759, Len of Training loss: 18, Average loss: 5.055905898412068\n",
      "Len of Validation loss: 54, Average loss: 6.153752035564846\n",
      "Epoch: 760, Len of Training loss: 18, Average loss: 5.138761123021443\n",
      "Len of Validation loss: 54, Average loss: 6.068182137277391\n",
      "Epoch: 761, Len of Training loss: 18, Average loss: 5.261109802458021\n",
      "Len of Validation loss: 54, Average loss: 6.405644107747961\n",
      "Epoch: 762, Len of Training loss: 18, Average loss: 4.982060644361708\n",
      "Len of Validation loss: 54, Average loss: 6.023853778839111\n",
      "Epoch: 763, Len of Training loss: 18, Average loss: 5.039134211010403\n",
      "Len of Validation loss: 54, Average loss: 6.589489817619324\n",
      "Epoch: 764, Len of Training loss: 18, Average loss: 5.1227090623643665\n",
      "Len of Validation loss: 54, Average loss: 6.2171945527747825\n",
      "Epoch: 765, Len of Training loss: 18, Average loss: 5.215834432178074\n",
      "Len of Validation loss: 54, Average loss: 6.69281867256871\n",
      "Epoch: 766, Len of Training loss: 18, Average loss: 5.43562454647488\n",
      "Len of Validation loss: 54, Average loss: 5.992780482327497\n",
      "Epoch: 767, Len of Training loss: 18, Average loss: 5.229302750693427\n",
      "Len of Validation loss: 54, Average loss: 5.923438164922926\n",
      "Epoch: 768, Len of Training loss: 18, Average loss: 5.056027359432644\n",
      "Len of Validation loss: 54, Average loss: 6.139596921426278\n",
      "Epoch: 769, Len of Training loss: 18, Average loss: 5.03386558426751\n",
      "Len of Validation loss: 54, Average loss: 5.969927761289808\n",
      "Epoch: 770, Len of Training loss: 18, Average loss: 5.078033049901326\n",
      "Len of Validation loss: 54, Average loss: 5.832782052181385\n",
      "Epoch: 771, Len of Training loss: 18, Average loss: 4.954122569825914\n",
      "Len of Validation loss: 54, Average loss: 6.0349588129255505\n",
      "Epoch: 772, Len of Training loss: 18, Average loss: 4.9877537621392145\n",
      "Len of Validation loss: 54, Average loss: 6.2962066553257126\n",
      "Epoch: 773, Len of Training loss: 18, Average loss: 5.048995468351576\n",
      "Len of Validation loss: 54, Average loss: 5.870051631221065\n",
      "Epoch: 774, Len of Training loss: 18, Average loss: 4.97874559296502\n",
      "Len of Validation loss: 54, Average loss: 6.218936544877511\n",
      "Epoch: 775, Len of Training loss: 18, Average loss: 5.229896161291334\n",
      "Len of Validation loss: 54, Average loss: 6.188346717092726\n",
      "Epoch: 776, Len of Training loss: 18, Average loss: 5.610768874486287\n",
      "Len of Validation loss: 54, Average loss: 6.234747634993659\n",
      "Epoch: 777, Len of Training loss: 18, Average loss: 5.310781531863743\n",
      "Len of Validation loss: 54, Average loss: 5.986408286624485\n",
      "Epoch: 778, Len of Training loss: 18, Average loss: 5.169103397263421\n",
      "Len of Validation loss: 54, Average loss: 6.108859967302393\n",
      "Epoch: 779, Len of Training loss: 18, Average loss: 5.366323020723131\n",
      "Len of Validation loss: 54, Average loss: 6.0661130922812\n",
      "Epoch: 780, Len of Training loss: 18, Average loss: 5.112046851052178\n",
      "Len of Validation loss: 54, Average loss: 6.01671541178668\n",
      "Epoch: 781, Len of Training loss: 18, Average loss: 4.969760523902045\n",
      "Len of Validation loss: 54, Average loss: 5.713458551300897\n",
      "Epoch: 782, Len of Training loss: 18, Average loss: 4.949118667178684\n",
      "Len of Validation loss: 54, Average loss: 5.805988320597896\n",
      "Epoch: 783, Len of Training loss: 18, Average loss: 4.9743317233191595\n",
      "Len of Validation loss: 54, Average loss: 5.946903983751933\n",
      "Epoch: 784, Len of Training loss: 18, Average loss: 4.895380443996853\n",
      "Len of Validation loss: 54, Average loss: 5.685755235177499\n",
      "Epoch: 785, Len of Training loss: 18, Average loss: 4.813982791370815\n",
      "Len of Validation loss: 54, Average loss: 5.910948837244952\n",
      "Epoch: 786, Len of Training loss: 18, Average loss: 4.795130226347181\n",
      "Len of Validation loss: 54, Average loss: 5.945424132876926\n",
      "Epoch: 787, Len of Training loss: 18, Average loss: 4.908308611975776\n",
      "Len of Validation loss: 54, Average loss: 6.069683635676348\n",
      "Epoch: 788, Len of Training loss: 18, Average loss: 5.3681066036224365\n",
      "Len of Validation loss: 54, Average loss: 6.205208089616564\n",
      "Epoch: 789, Len of Training loss: 18, Average loss: 5.547231409284803\n",
      "Len of Validation loss: 54, Average loss: 8.326235519515144\n",
      "Epoch: 790, Len of Training loss: 18, Average loss: 6.138943725162083\n",
      "Len of Validation loss: 54, Average loss: 6.608748568428887\n",
      "Epoch: 791, Len of Training loss: 18, Average loss: 5.406247324413723\n",
      "Len of Validation loss: 54, Average loss: 5.8758274051878185\n",
      "Epoch: 792, Len of Training loss: 18, Average loss: 5.258671177758111\n",
      "Len of Validation loss: 54, Average loss: 6.018610883642126\n",
      "Epoch: 793, Len of Training loss: 18, Average loss: 5.163360542721218\n",
      "Len of Validation loss: 54, Average loss: 6.250928980332834\n",
      "Epoch: 794, Len of Training loss: 18, Average loss: 5.1051886611514625\n",
      "Len of Validation loss: 54, Average loss: 5.777337065449467\n",
      "Epoch: 795, Len of Training loss: 18, Average loss: 4.919315232170953\n",
      "Len of Validation loss: 54, Average loss: 5.982147464045772\n",
      "Epoch: 796, Len of Training loss: 18, Average loss: 4.833700868818495\n",
      "Len of Validation loss: 54, Average loss: 5.7589225945649325\n",
      "Epoch: 797, Len of Training loss: 18, Average loss: 4.843997346030341\n",
      "Len of Validation loss: 54, Average loss: 5.779965336675997\n",
      "Epoch: 798, Len of Training loss: 18, Average loss: 4.760894775390625\n",
      "Len of Validation loss: 54, Average loss: 6.070142141094914\n",
      "Epoch: 799, Len of Training loss: 18, Average loss: 4.7859167787763806\n",
      "Len of Validation loss: 54, Average loss: 5.693533950381809\n",
      "Epoch: 800, Len of Training loss: 18, Average loss: 4.729404860072666\n",
      "Len of Validation loss: 54, Average loss: 5.870971979918303\n",
      "Epoch: 801, Len of Training loss: 18, Average loss: 4.85259743531545\n",
      "Len of Validation loss: 54, Average loss: 6.203755546499182\n",
      "Epoch: 802, Len of Training loss: 18, Average loss: 5.172330300013225\n",
      "Len of Validation loss: 54, Average loss: 5.835826922346045\n",
      "Epoch: 803, Len of Training loss: 18, Average loss: 5.107737567689684\n",
      "Len of Validation loss: 54, Average loss: 6.3605589027758\n",
      "Epoch: 804, Len of Training loss: 18, Average loss: 4.99116587638855\n",
      "Len of Validation loss: 54, Average loss: 5.877073729479754\n",
      "Epoch: 805, Len of Training loss: 18, Average loss: 4.927462630801731\n",
      "Len of Validation loss: 54, Average loss: 5.665092728756092\n",
      "Epoch: 806, Len of Training loss: 18, Average loss: 4.818656974368626\n",
      "Len of Validation loss: 54, Average loss: 5.4950079432240235\n",
      "Epoch: 807, Len of Training loss: 18, Average loss: 4.639431715011597\n",
      "Len of Validation loss: 54, Average loss: 5.658019491919765\n",
      "Epoch: 808, Len of Training loss: 18, Average loss: 4.758350054423015\n",
      "Len of Validation loss: 54, Average loss: 5.7550040792535855\n",
      "Epoch: 809, Len of Training loss: 18, Average loss: 5.019666433334351\n",
      "Len of Validation loss: 54, Average loss: 6.355597217877706\n",
      "Epoch: 810, Len of Training loss: 18, Average loss: 4.792644725905524\n",
      "Len of Validation loss: 54, Average loss: 5.698372818805553\n",
      "Epoch: 811, Len of Training loss: 18, Average loss: 4.694080564710829\n",
      "Len of Validation loss: 54, Average loss: 5.631818155447642\n",
      "Epoch: 812, Len of Training loss: 18, Average loss: 4.576664553748237\n",
      "Len of Validation loss: 54, Average loss: 5.782014661365086\n",
      "Epoch: 813, Len of Training loss: 18, Average loss: 4.617912411689758\n",
      "Len of Validation loss: 54, Average loss: 5.5731650635048195\n",
      "Epoch: 814, Len of Training loss: 18, Average loss: 4.52289436923133\n",
      "Len of Validation loss: 54, Average loss: 5.707671046257019\n",
      "Epoch: 815, Len of Training loss: 18, Average loss: 4.578117423587376\n",
      "Len of Validation loss: 54, Average loss: 5.670778557106301\n",
      "Epoch: 816, Len of Training loss: 18, Average loss: 4.5145225922266645\n",
      "Len of Validation loss: 54, Average loss: 5.787947632648327\n",
      "Epoch: 817, Len of Training loss: 18, Average loss: 4.699606127209133\n",
      "Len of Validation loss: 54, Average loss: 6.115619889012089\n",
      "Epoch: 818, Len of Training loss: 18, Average loss: 4.7653898398081465\n",
      "Len of Validation loss: 54, Average loss: 5.762885702980889\n",
      "Epoch: 819, Len of Training loss: 18, Average loss: 4.796729220284356\n",
      "Len of Validation loss: 54, Average loss: 5.708167950312297\n",
      "Epoch: 820, Len of Training loss: 18, Average loss: 4.6552251312467785\n",
      "Len of Validation loss: 54, Average loss: 5.888210248064111\n",
      "Epoch: 821, Len of Training loss: 18, Average loss: 4.643423226144579\n",
      "Len of Validation loss: 54, Average loss: 5.5647492717813565\n",
      "Epoch: 822, Len of Training loss: 18, Average loss: 4.542029235098097\n",
      "Len of Validation loss: 54, Average loss: 5.660603947109646\n",
      "Epoch: 823, Len of Training loss: 18, Average loss: 4.594785716798571\n",
      "Len of Validation loss: 54, Average loss: 5.753639216776247\n",
      "Epoch: 824, Len of Training loss: 18, Average loss: 4.89071688387129\n",
      "Len of Validation loss: 54, Average loss: 5.840055395055701\n",
      "Epoch: 825, Len of Training loss: 18, Average loss: 4.716905156771342\n",
      "Len of Validation loss: 54, Average loss: 5.631603417573152\n",
      "Epoch: 826, Len of Training loss: 18, Average loss: 4.782057179345025\n",
      "Len of Validation loss: 54, Average loss: 6.017056743303935\n",
      "Epoch: 827, Len of Training loss: 18, Average loss: 4.859517494837443\n",
      "Len of Validation loss: 54, Average loss: 5.9143599139319525\n",
      "Epoch: 828, Len of Training loss: 18, Average loss: 4.748738792207506\n",
      "Len of Validation loss: 54, Average loss: 5.9839988328792435\n",
      "Epoch: 829, Len of Training loss: 18, Average loss: 4.636071880658467\n",
      "Len of Validation loss: 54, Average loss: 5.762856982372425\n",
      "Epoch: 830, Len of Training loss: 18, Average loss: 4.4315169122484\n",
      "Len of Validation loss: 54, Average loss: 5.448673685391744\n",
      "Epoch: 831, Len of Training loss: 18, Average loss: 4.377480493651496\n",
      "Len of Validation loss: 54, Average loss: 5.527380696049443\n",
      "Epoch: 832, Len of Training loss: 18, Average loss: 4.482892632484436\n",
      "Len of Validation loss: 54, Average loss: 5.514444638181616\n",
      "Epoch: 833, Len of Training loss: 18, Average loss: 4.645372046364678\n",
      "Len of Validation loss: 54, Average loss: 6.093765903402258\n",
      "Epoch: 834, Len of Training loss: 18, Average loss: 4.78663084242079\n",
      "Len of Validation loss: 54, Average loss: 5.901161763403151\n",
      "Epoch: 835, Len of Training loss: 18, Average loss: 4.954586664835612\n",
      "Len of Validation loss: 54, Average loss: 5.853114680007652\n",
      "Epoch: 836, Len of Training loss: 18, Average loss: 4.80235477288564\n",
      "Len of Validation loss: 54, Average loss: 5.916028053672226\n",
      "Epoch: 837, Len of Training loss: 18, Average loss: 4.774669501516554\n",
      "Len of Validation loss: 54, Average loss: 5.994258249247515\n",
      "Epoch: 838, Len of Training loss: 18, Average loss: 4.765274736616346\n",
      "Len of Validation loss: 54, Average loss: 6.024221415872927\n",
      "Epoch: 839, Len of Training loss: 18, Average loss: 4.62885496351454\n",
      "Len of Validation loss: 54, Average loss: 5.517233530680339\n",
      "Epoch: 840, Len of Training loss: 18, Average loss: 4.481950004895528\n",
      "Len of Validation loss: 54, Average loss: 5.80464975922196\n",
      "Epoch: 841, Len of Training loss: 18, Average loss: 4.47075339158376\n",
      "Len of Validation loss: 54, Average loss: 5.966265576857108\n",
      "Epoch: 842, Len of Training loss: 18, Average loss: 4.526466899447971\n",
      "Len of Validation loss: 54, Average loss: 5.510930661801939\n",
      "Epoch: 843, Len of Training loss: 18, Average loss: 4.533872498406304\n",
      "Len of Validation loss: 54, Average loss: 6.1655619541804\n",
      "Epoch: 844, Len of Training loss: 18, Average loss: 5.079585591952006\n",
      "Len of Validation loss: 54, Average loss: 6.195026406535396\n",
      "Epoch: 845, Len of Training loss: 18, Average loss: 5.073538541793823\n",
      "Len of Validation loss: 54, Average loss: 5.849179749135618\n",
      "Epoch: 846, Len of Training loss: 18, Average loss: 4.641459676954481\n",
      "Len of Validation loss: 54, Average loss: 5.700023646707888\n",
      "Epoch: 847, Len of Training loss: 18, Average loss: 4.502942972713047\n",
      "Len of Validation loss: 54, Average loss: 5.689683247495581\n",
      "Epoch: 848, Len of Training loss: 18, Average loss: 4.306602266099718\n",
      "Len of Validation loss: 54, Average loss: 5.47840083528448\n",
      "Epoch: 849, Len of Training loss: 18, Average loss: 4.251016683048672\n",
      "Len of Validation loss: 54, Average loss: 5.694842131049545\n",
      "Epoch: 850, Len of Training loss: 18, Average loss: 4.543010751406352\n",
      "Len of Validation loss: 54, Average loss: 5.68372235916279\n",
      "Epoch: 851, Len of Training loss: 18, Average loss: 4.551162547535366\n",
      "Len of Validation loss: 54, Average loss: 5.629372084582293\n",
      "Epoch: 852, Len of Training loss: 18, Average loss: 4.32579078939226\n",
      "Len of Validation loss: 54, Average loss: 6.025136753364846\n",
      "Epoch: 853, Len of Training loss: 18, Average loss: 4.445453259680006\n",
      "Len of Validation loss: 54, Average loss: 5.496772797019394\n",
      "Epoch: 854, Len of Training loss: 18, Average loss: 4.25720657242669\n",
      "Len of Validation loss: 54, Average loss: 5.46029727768015\n",
      "Epoch: 855, Len of Training loss: 18, Average loss: 4.376963204807705\n",
      "Len of Validation loss: 54, Average loss: 5.620159634837398\n",
      "Epoch: 856, Len of Training loss: 18, Average loss: 4.358640975422329\n",
      "Len of Validation loss: 54, Average loss: 5.581301309444286\n",
      "Epoch: 857, Len of Training loss: 18, Average loss: 4.2547278139326306\n",
      "Len of Validation loss: 54, Average loss: 5.880794507485849\n",
      "Epoch: 858, Len of Training loss: 18, Average loss: 4.3699345456229315\n",
      "Len of Validation loss: 54, Average loss: 5.486973638887759\n",
      "Epoch: 859, Len of Training loss: 18, Average loss: 4.355268332693312\n",
      "Len of Validation loss: 54, Average loss: 5.657010422812568\n",
      "Epoch: 860, Len of Training loss: 18, Average loss: 4.513756420877245\n",
      "Len of Validation loss: 54, Average loss: 5.777483441211559\n",
      "Epoch: 861, Len of Training loss: 18, Average loss: 4.637537744310167\n",
      "Len of Validation loss: 54, Average loss: 5.846800022655064\n",
      "Epoch: 862, Len of Training loss: 18, Average loss: 4.35164561536577\n",
      "Len of Validation loss: 54, Average loss: 5.637146940937749\n",
      "Epoch: 863, Len of Training loss: 18, Average loss: 4.358162509070502\n",
      "Len of Validation loss: 54, Average loss: 5.99479956097073\n",
      "Epoch: 864, Len of Training loss: 18, Average loss: 4.2384920782513085\n",
      "Len of Validation loss: 54, Average loss: 5.498983127099496\n",
      "Epoch: 865, Len of Training loss: 18, Average loss: 4.20719313621521\n",
      "Len of Validation loss: 54, Average loss: 5.553387103257356\n",
      "Epoch: 866, Len of Training loss: 18, Average loss: 4.264006455739339\n",
      "Len of Validation loss: 54, Average loss: 5.315328452322218\n",
      "Epoch: 867, Len of Training loss: 18, Average loss: 4.118059674898784\n",
      "Len of Validation loss: 54, Average loss: 5.592358977706344\n",
      "Epoch: 868, Len of Training loss: 18, Average loss: 4.560456461376614\n",
      "Len of Validation loss: 54, Average loss: 6.413612277419479\n",
      "Epoch: 869, Len of Training loss: 18, Average loss: 4.849528974956936\n",
      "Len of Validation loss: 54, Average loss: 5.775201254420811\n",
      "Epoch: 870, Len of Training loss: 18, Average loss: 4.818910863664415\n",
      "Len of Validation loss: 54, Average loss: 5.483965061329029\n",
      "Epoch: 871, Len of Training loss: 18, Average loss: 4.368608858850267\n",
      "Len of Validation loss: 54, Average loss: 5.303041537602742\n",
      "Epoch: 872, Len of Training loss: 18, Average loss: 4.406843834453159\n",
      "Len of Validation loss: 54, Average loss: 5.632609672016567\n",
      "Epoch: 873, Len of Training loss: 18, Average loss: 4.226347274250454\n",
      "Len of Validation loss: 54, Average loss: 5.500733221018756\n",
      "Epoch: 874, Len of Training loss: 18, Average loss: 4.174228522512648\n",
      "Len of Validation loss: 54, Average loss: 5.418848117192586\n",
      "Epoch: 875, Len of Training loss: 18, Average loss: 4.206625647015041\n",
      "Len of Validation loss: 54, Average loss: 5.560278428925408\n",
      "Epoch: 876, Len of Training loss: 18, Average loss: 4.305596272150676\n",
      "Len of Validation loss: 54, Average loss: 5.510124012275979\n",
      "Epoch: 877, Len of Training loss: 18, Average loss: 4.17140875922309\n",
      "Len of Validation loss: 54, Average loss: 5.902701236583568\n",
      "Epoch: 878, Len of Training loss: 18, Average loss: 4.292514827516344\n",
      "Len of Validation loss: 54, Average loss: 5.624948541323344\n",
      "Epoch: 879, Len of Training loss: 18, Average loss: 4.150166604253981\n",
      "Len of Validation loss: 54, Average loss: 5.31332865467778\n",
      "Epoch: 880, Len of Training loss: 18, Average loss: 4.297153658337063\n",
      "Len of Validation loss: 54, Average loss: 5.4261574612723456\n",
      "Epoch: 881, Len of Training loss: 18, Average loss: 4.254214101367527\n",
      "Len of Validation loss: 54, Average loss: 5.420336211169207\n",
      "Epoch: 882, Len of Training loss: 18, Average loss: 4.1752620405620995\n",
      "Len of Validation loss: 54, Average loss: 5.778518160184224\n",
      "Epoch: 883, Len of Training loss: 18, Average loss: 4.301036530070835\n",
      "Len of Validation loss: 54, Average loss: 5.557056819951093\n",
      "Epoch: 884, Len of Training loss: 18, Average loss: 4.025762293073866\n",
      "Len of Validation loss: 54, Average loss: 5.332916776339213\n",
      "Epoch: 885, Len of Training loss: 18, Average loss: 4.056552330652873\n",
      "Len of Validation loss: 54, Average loss: 5.484237419234382\n",
      "Epoch: 886, Len of Training loss: 18, Average loss: 4.263979276021321\n",
      "Len of Validation loss: 54, Average loss: 5.652360593831098\n",
      "Epoch: 887, Len of Training loss: 18, Average loss: 4.264661418067084\n",
      "Len of Validation loss: 54, Average loss: 5.334809329774645\n",
      "Epoch: 888, Len of Training loss: 18, Average loss: 4.1228970819049415\n",
      "Len of Validation loss: 54, Average loss: 5.192499266730414\n",
      "Epoch: 889, Len of Training loss: 18, Average loss: 4.022680600484212\n",
      "Len of Validation loss: 54, Average loss: 5.294987806567439\n",
      "Epoch: 890, Len of Training loss: 18, Average loss: 4.237574060757955\n",
      "Len of Validation loss: 54, Average loss: 5.687938155951323\n",
      "Epoch: 891, Len of Training loss: 18, Average loss: 4.317177017529805\n",
      "Len of Validation loss: 54, Average loss: 5.319313627702218\n",
      "Epoch: 892, Len of Training loss: 18, Average loss: 4.182595279481676\n",
      "Len of Validation loss: 54, Average loss: 5.73970095316569\n",
      "Epoch: 893, Len of Training loss: 18, Average loss: 4.153032037946913\n",
      "Len of Validation loss: 54, Average loss: 5.347992203853749\n",
      "Epoch: 894, Len of Training loss: 18, Average loss: 4.026255673832363\n",
      "Len of Validation loss: 54, Average loss: 5.323290877872044\n",
      "Epoch: 895, Len of Training loss: 18, Average loss: 3.9293119642469616\n",
      "Len of Validation loss: 54, Average loss: 5.495463932002032\n",
      "Epoch: 896, Len of Training loss: 18, Average loss: 3.9810555378595986\n",
      "Len of Validation loss: 54, Average loss: 5.420094547448335\n",
      "Epoch: 897, Len of Training loss: 18, Average loss: 3.957950856950548\n",
      "Len of Validation loss: 54, Average loss: 5.365360789828831\n",
      "Epoch: 898, Len of Training loss: 18, Average loss: 4.095598220825195\n",
      "Len of Validation loss: 54, Average loss: 5.432882878515455\n",
      "Epoch: 899, Len of Training loss: 18, Average loss: 3.917489488919576\n",
      "Len of Validation loss: 54, Average loss: 5.537987287397738\n",
      "Epoch: 900, Len of Training loss: 18, Average loss: 4.111323157946269\n",
      "Len of Validation loss: 54, Average loss: 5.151332091402124\n",
      "Epoch: 901, Len of Training loss: 18, Average loss: 4.153394089804755\n",
      "Len of Validation loss: 54, Average loss: 5.737837959218909\n",
      "Epoch: 902, Len of Training loss: 18, Average loss: 4.083951897091335\n",
      "Len of Validation loss: 54, Average loss: 5.376204384697808\n",
      "Epoch: 903, Len of Training loss: 18, Average loss: 4.079284138149685\n",
      "Len of Validation loss: 54, Average loss: 5.635373376033924\n",
      "Epoch: 904, Len of Training loss: 18, Average loss: 4.138460861312018\n",
      "Len of Validation loss: 54, Average loss: 5.410098420249091\n",
      "Epoch: 905, Len of Training loss: 18, Average loss: 3.8543245792388916\n",
      "Len of Validation loss: 54, Average loss: 5.451586595288029\n",
      "Epoch: 906, Len of Training loss: 18, Average loss: 3.9975353479385376\n",
      "Len of Validation loss: 54, Average loss: 5.289991502408628\n",
      "Epoch: 907, Len of Training loss: 18, Average loss: 3.9817095862494574\n",
      "Len of Validation loss: 54, Average loss: 5.152410061271103\n",
      "Epoch: 908, Len of Training loss: 18, Average loss: 4.024191406038073\n",
      "Len of Validation loss: 54, Average loss: 5.03508636686537\n",
      "Epoch: 909, Len of Training loss: 18, Average loss: 3.9184017181396484\n",
      "Len of Validation loss: 54, Average loss: 5.332526021533543\n",
      "Epoch: 910, Len of Training loss: 18, Average loss: 3.886189924346076\n",
      "Len of Validation loss: 54, Average loss: 5.212858420831186\n",
      "Epoch: 911, Len of Training loss: 18, Average loss: 4.220139900843303\n",
      "Len of Validation loss: 54, Average loss: 5.852877965679875\n",
      "Epoch: 912, Len of Training loss: 18, Average loss: 4.3272118833329944\n",
      "Len of Validation loss: 54, Average loss: 5.837866447590016\n",
      "Epoch: 913, Len of Training loss: 18, Average loss: 4.175319380230373\n",
      "Len of Validation loss: 54, Average loss: 5.335403186303598\n",
      "Epoch: 914, Len of Training loss: 18, Average loss: 4.238464461432563\n",
      "Len of Validation loss: 54, Average loss: 5.2328075744487625\n",
      "Epoch: 915, Len of Training loss: 18, Average loss: 4.041344417466058\n",
      "Len of Validation loss: 54, Average loss: 5.349578111260025\n",
      "Epoch: 916, Len of Training loss: 18, Average loss: 3.9301786952548556\n",
      "Len of Validation loss: 54, Average loss: 5.2078801216902555\n",
      "Epoch: 917, Len of Training loss: 18, Average loss: 3.935977021853129\n",
      "Len of Validation loss: 54, Average loss: 5.224432362450494\n",
      "Epoch: 918, Len of Training loss: 18, Average loss: 3.928007059627109\n",
      "Len of Validation loss: 54, Average loss: 5.279285033543904\n",
      "Epoch: 919, Len of Training loss: 18, Average loss: 3.9573223855760364\n",
      "Len of Validation loss: 54, Average loss: 5.385999547110663\n",
      "Epoch: 920, Len of Training loss: 18, Average loss: 3.980837239159478\n",
      "Len of Validation loss: 54, Average loss: 5.561145550674862\n",
      "Epoch: 921, Len of Training loss: 18, Average loss: 4.014020946290758\n",
      "Len of Validation loss: 54, Average loss: 5.311193046746431\n",
      "Epoch: 922, Len of Training loss: 18, Average loss: 4.115544358889262\n",
      "Len of Validation loss: 54, Average loss: 5.213674381927207\n",
      "Epoch: 923, Len of Training loss: 18, Average loss: 3.9071346124013266\n",
      "Len of Validation loss: 54, Average loss: 5.551297187805176\n",
      "Epoch: 924, Len of Training loss: 18, Average loss: 3.8509313795301647\n",
      "Len of Validation loss: 54, Average loss: 5.266384968051204\n",
      "Epoch: 925, Len of Training loss: 18, Average loss: 3.8484419849183826\n",
      "Len of Validation loss: 54, Average loss: 5.3926518382849515\n",
      "Epoch: 926, Len of Training loss: 18, Average loss: 3.9257997936672635\n",
      "Len of Validation loss: 54, Average loss: 5.213317107271265\n",
      "Epoch: 927, Len of Training loss: 18, Average loss: 3.941136015786065\n",
      "Len of Validation loss: 54, Average loss: 5.478105540628786\n",
      "Epoch: 928, Len of Training loss: 18, Average loss: 4.396432068612841\n",
      "Len of Validation loss: 54, Average loss: 5.355246274559586\n",
      "Epoch: 929, Len of Training loss: 18, Average loss: 4.189987553490533\n",
      "Len of Validation loss: 54, Average loss: 5.251869572533502\n",
      "Epoch: 930, Len of Training loss: 18, Average loss: 3.946383992830912\n",
      "Len of Validation loss: 54, Average loss: 5.269650536554831\n",
      "Epoch: 931, Len of Training loss: 18, Average loss: 3.8557102017932467\n",
      "Len of Validation loss: 54, Average loss: 5.158714879442145\n",
      "Epoch: 932, Len of Training loss: 18, Average loss: 3.8014436297946506\n",
      "Len of Validation loss: 54, Average loss: 5.028767859494245\n",
      "Epoch: 933, Len of Training loss: 18, Average loss: 3.8387527730729847\n",
      "Len of Validation loss: 54, Average loss: 5.36687386918951\n",
      "Epoch: 934, Len of Training loss: 18, Average loss: 3.9211941957473755\n",
      "Len of Validation loss: 54, Average loss: 5.338748066513626\n",
      "Epoch: 935, Len of Training loss: 18, Average loss: 3.8813037739859686\n",
      "Len of Validation loss: 54, Average loss: 5.512143430886446\n",
      "Epoch: 936, Len of Training loss: 18, Average loss: 3.935676521725125\n",
      "Len of Validation loss: 54, Average loss: 5.212039013703664\n",
      "Epoch: 937, Len of Training loss: 18, Average loss: 3.840053253703647\n",
      "Len of Validation loss: 54, Average loss: 5.330968578656514\n",
      "Epoch: 938, Len of Training loss: 18, Average loss: 3.702332165506151\n",
      "Len of Validation loss: 54, Average loss: 5.076444568457426\n",
      "Epoch: 939, Len of Training loss: 18, Average loss: 3.6905327373080783\n",
      "Len of Validation loss: 54, Average loss: 5.606195502811008\n",
      "Epoch: 940, Len of Training loss: 18, Average loss: 3.7045589155621\n",
      "Len of Validation loss: 54, Average loss: 5.11022671063741\n",
      "Epoch: 941, Len of Training loss: 18, Average loss: 3.7318654325273304\n",
      "Len of Validation loss: 54, Average loss: 5.045903369232461\n",
      "Epoch: 942, Len of Training loss: 18, Average loss: 3.7550555732515125\n",
      "Len of Validation loss: 54, Average loss: 5.24983870100092\n",
      "Epoch: 943, Len of Training loss: 18, Average loss: 3.7263802025053234\n",
      "Len of Validation loss: 54, Average loss: 5.006929247467606\n",
      "Epoch: 944, Len of Training loss: 18, Average loss: 3.766155309147305\n",
      "Len of Validation loss: 54, Average loss: 5.117824280703509\n",
      "Epoch: 945, Len of Training loss: 18, Average loss: 3.955594023068746\n",
      "Len of Validation loss: 54, Average loss: 5.477779613600837\n",
      "Epoch: 946, Len of Training loss: 18, Average loss: 4.109647724363539\n",
      "Len of Validation loss: 54, Average loss: 5.366418922388995\n",
      "Epoch: 947, Len of Training loss: 18, Average loss: 3.8183200889163547\n",
      "Len of Validation loss: 54, Average loss: 5.673799298427723\n",
      "Epoch: 948, Len of Training loss: 18, Average loss: 3.7523917622036405\n",
      "Len of Validation loss: 54, Average loss: 5.422063491962574\n",
      "Epoch: 949, Len of Training loss: 18, Average loss: 3.677864763471815\n",
      "Len of Validation loss: 54, Average loss: 5.233193008987992\n",
      "Epoch: 950, Len of Training loss: 18, Average loss: 3.7392793893814087\n",
      "Len of Validation loss: 54, Average loss: 5.374736821209943\n",
      "Epoch: 951, Len of Training loss: 18, Average loss: 3.776114755206638\n",
      "Len of Validation loss: 54, Average loss: 5.638266872476648\n",
      "Epoch: 952, Len of Training loss: 18, Average loss: 3.676585899458991\n",
      "Len of Validation loss: 54, Average loss: 5.046738174226549\n",
      "Epoch: 953, Len of Training loss: 18, Average loss: 3.5813544193903604\n",
      "Len of Validation loss: 54, Average loss: 5.446347466221562\n",
      "Epoch: 954, Len of Training loss: 18, Average loss: 3.572877605756124\n",
      "Len of Validation loss: 54, Average loss: 5.143706176016066\n",
      "Epoch: 955, Len of Training loss: 18, Average loss: 3.5361027320226035\n",
      "Len of Validation loss: 54, Average loss: 4.840842794488977\n",
      "Epoch: 956, Len of Training loss: 18, Average loss: 3.52495809396108\n",
      "Len of Validation loss: 54, Average loss: 5.258136170881766\n",
      "Epoch: 957, Len of Training loss: 18, Average loss: 3.5596533351474338\n",
      "Len of Validation loss: 54, Average loss: 5.018250524997711\n",
      "Epoch: 958, Len of Training loss: 18, Average loss: 3.465034524599711\n",
      "Len of Validation loss: 54, Average loss: 5.081351611349318\n",
      "Epoch: 959, Len of Training loss: 18, Average loss: 3.526539590623644\n",
      "Len of Validation loss: 54, Average loss: 5.055512830063149\n",
      "Epoch: 960, Len of Training loss: 18, Average loss: 3.644377006424798\n",
      "Len of Validation loss: 54, Average loss: 5.377066016197205\n",
      "Epoch: 961, Len of Training loss: 18, Average loss: 3.6353631019592285\n",
      "Len of Validation loss: 54, Average loss: 4.963346600532532\n",
      "Epoch: 962, Len of Training loss: 18, Average loss: 3.488516410191854\n",
      "Len of Validation loss: 54, Average loss: 5.1079203552669945\n",
      "Epoch: 963, Len of Training loss: 18, Average loss: 3.600336617893643\n",
      "Len of Validation loss: 54, Average loss: 5.288456254535252\n",
      "Epoch: 964, Len of Training loss: 18, Average loss: 3.624806602795919\n",
      "Len of Validation loss: 54, Average loss: 5.147825519243876\n",
      "Epoch: 965, Len of Training loss: 18, Average loss: 3.9038793908225164\n",
      "Len of Validation loss: 54, Average loss: 5.027028664394662\n",
      "Epoch: 966, Len of Training loss: 18, Average loss: 3.6900227202309503\n",
      "Len of Validation loss: 54, Average loss: 5.381501842428137\n",
      "Epoch: 967, Len of Training loss: 18, Average loss: 3.7507424222098456\n",
      "Len of Validation loss: 54, Average loss: 5.054136377793771\n",
      "Epoch: 968, Len of Training loss: 18, Average loss: 3.7022049162122936\n",
      "Len of Validation loss: 54, Average loss: 5.159695148468018\n",
      "Epoch: 969, Len of Training loss: 18, Average loss: 3.599476416905721\n",
      "Len of Validation loss: 54, Average loss: 5.201401834134702\n",
      "Epoch: 970, Len of Training loss: 18, Average loss: 3.534010330835978\n",
      "Len of Validation loss: 54, Average loss: 5.189738622418156\n",
      "Epoch: 971, Len of Training loss: 18, Average loss: 3.672235449155172\n",
      "Len of Validation loss: 54, Average loss: 5.038453654006675\n",
      "Epoch: 972, Len of Training loss: 18, Average loss: 3.715759555498759\n",
      "Len of Validation loss: 54, Average loss: 5.1850568144409745\n",
      "Epoch: 973, Len of Training loss: 18, Average loss: 3.9754926760991416\n",
      "Len of Validation loss: 54, Average loss: 5.796315515482867\n",
      "Epoch: 974, Len of Training loss: 18, Average loss: 3.795442077848646\n",
      "Len of Validation loss: 54, Average loss: 5.053372537648237\n",
      "Epoch: 975, Len of Training loss: 18, Average loss: 3.6739356915156045\n",
      "Len of Validation loss: 54, Average loss: 4.98498210200557\n",
      "Epoch: 976, Len of Training loss: 18, Average loss: 3.611454963684082\n",
      "Len of Validation loss: 54, Average loss: 5.2066642818627535\n",
      "Epoch: 977, Len of Training loss: 18, Average loss: 3.9166069428126016\n",
      "Len of Validation loss: 54, Average loss: 5.6764192713631525\n",
      "Epoch: 978, Len of Training loss: 18, Average loss: 3.861409823099772\n",
      "Len of Validation loss: 54, Average loss: 5.126121931605869\n",
      "Epoch: 979, Len of Training loss: 18, Average loss: 3.6610212988323636\n",
      "Len of Validation loss: 54, Average loss: 4.883346584108141\n",
      "Epoch: 980, Len of Training loss: 18, Average loss: 3.5820641120274863\n",
      "Len of Validation loss: 54, Average loss: 5.066011689327381\n",
      "Epoch: 981, Len of Training loss: 18, Average loss: 3.749460816383362\n",
      "Len of Validation loss: 54, Average loss: 4.90953133282838\n",
      "Epoch: 982, Len of Training loss: 18, Average loss: 3.555225888888041\n",
      "Len of Validation loss: 54, Average loss: 4.970424824290806\n",
      "Epoch: 983, Len of Training loss: 18, Average loss: 3.5580119027031794\n",
      "Len of Validation loss: 54, Average loss: 5.218429466088613\n",
      "Epoch: 984, Len of Training loss: 18, Average loss: 3.385287973615858\n",
      "Len of Validation loss: 54, Average loss: 4.928485486242506\n",
      "Epoch: 985, Len of Training loss: 18, Average loss: 3.44172649913364\n",
      "Len of Validation loss: 54, Average loss: 5.014119742093263\n",
      "Epoch: 986, Len of Training loss: 18, Average loss: 3.527253654268053\n",
      "Len of Validation loss: 54, Average loss: 5.248117252632424\n",
      "Epoch: 987, Len of Training loss: 18, Average loss: 3.8881018824047513\n",
      "Len of Validation loss: 54, Average loss: 5.838130244502315\n",
      "Epoch: 988, Len of Training loss: 18, Average loss: 4.311997069252862\n",
      "Len of Validation loss: 54, Average loss: 5.343714422649807\n",
      "Epoch: 989, Len of Training loss: 18, Average loss: 3.894511434766981\n",
      "Len of Validation loss: 54, Average loss: 5.24166676291713\n",
      "Epoch: 990, Len of Training loss: 18, Average loss: 3.6152343617545233\n",
      "Len of Validation loss: 54, Average loss: 5.063017690623248\n",
      "Epoch: 991, Len of Training loss: 18, Average loss: 3.3836946884791055\n",
      "Len of Validation loss: 54, Average loss: 4.868424543627986\n",
      "Epoch: 992, Len of Training loss: 18, Average loss: 3.25342157151964\n",
      "Len of Validation loss: 54, Average loss: 5.132688694530064\n",
      "Epoch: 993, Len of Training loss: 18, Average loss: 3.2261310948265924\n",
      "Len of Validation loss: 54, Average loss: 4.998457151430625\n",
      "Epoch: 994, Len of Training loss: 18, Average loss: 3.723601738611857\n",
      "Len of Validation loss: 54, Average loss: 5.584067976033246\n",
      "Epoch: 995, Len of Training loss: 18, Average loss: 3.782643093003167\n",
      "Len of Validation loss: 54, Average loss: 5.227437125311957\n",
      "Epoch: 996, Len of Training loss: 18, Average loss: 3.691195991304186\n",
      "Len of Validation loss: 54, Average loss: 5.273968021074931\n",
      "Epoch: 997, Len of Training loss: 18, Average loss: 3.580718570285373\n",
      "Len of Validation loss: 54, Average loss: 4.902520996552926\n",
      "Epoch: 998, Len of Training loss: 18, Average loss: 3.5518245564566717\n",
      "Len of Validation loss: 54, Average loss: 5.234841752935339\n",
      "Epoch: 999, Len of Training loss: 18, Average loss: 3.623090068499247\n",
      "Len of Validation loss: 54, Average loss: 5.388818612805119\n",
      "Epoch: 1000, Len of Training loss: 18, Average loss: 3.8309094376034207\n",
      "Len of Validation loss: 54, Average loss: 5.169569399621752\n",
      "Epoch: 1001, Len of Training loss: 18, Average loss: 3.5730393065346613\n",
      "Len of Validation loss: 54, Average loss: 4.981284680189909\n",
      "Epoch: 1002, Len of Training loss: 18, Average loss: 3.380753066804674\n",
      "Len of Validation loss: 54, Average loss: 4.953348433529889\n",
      "Epoch: 1003, Len of Training loss: 18, Average loss: 3.3369272682401867\n",
      "Len of Validation loss: 54, Average loss: 5.0984566432458385\n",
      "Epoch: 1004, Len of Training loss: 18, Average loss: 3.3330148458480835\n",
      "Len of Validation loss: 54, Average loss: 4.976002233999747\n",
      "Epoch: 1005, Len of Training loss: 18, Average loss: 3.26239373948839\n",
      "Len of Validation loss: 54, Average loss: 4.700766547962472\n",
      "Epoch: 1006, Len of Training loss: 18, Average loss: 3.3706729147169323\n",
      "Len of Validation loss: 54, Average loss: 5.283181073489012\n",
      "Epoch: 1007, Len of Training loss: 18, Average loss: 3.40663038359748\n",
      "Len of Validation loss: 54, Average loss: 5.051015284326342\n",
      "Epoch: 1008, Len of Training loss: 18, Average loss: 3.3924003177218967\n",
      "Len of Validation loss: 54, Average loss: 5.084897864747931\n",
      "Epoch: 1009, Len of Training loss: 18, Average loss: 3.383258832825555\n",
      "Len of Validation loss: 54, Average loss: 4.952229897181193\n",
      "Epoch: 1010, Len of Training loss: 18, Average loss: 3.376332441965739\n",
      "Len of Validation loss: 54, Average loss: 5.189980670257851\n",
      "Epoch: 1011, Len of Training loss: 18, Average loss: 3.3163577715555825\n",
      "Len of Validation loss: 54, Average loss: 4.938041616369177\n",
      "Epoch: 1012, Len of Training loss: 18, Average loss: 3.2108273241255016\n",
      "Len of Validation loss: 54, Average loss: 4.731372175393282\n",
      "Epoch: 1013, Len of Training loss: 18, Average loss: 3.153097073237101\n",
      "Len of Validation loss: 54, Average loss: 4.851858375249086\n",
      "Epoch: 1014, Len of Training loss: 18, Average loss: 3.266676174269782\n",
      "Len of Validation loss: 54, Average loss: 4.9842111931906805\n",
      "Epoch: 1015, Len of Training loss: 18, Average loss: 3.430855313936869\n",
      "Len of Validation loss: 54, Average loss: 5.258353052315889\n",
      "Epoch: 1016, Len of Training loss: 18, Average loss: 3.4630814128451877\n",
      "Len of Validation loss: 54, Average loss: 5.258722901344299\n",
      "Epoch: 1017, Len of Training loss: 18, Average loss: 3.4513397879070706\n",
      "Len of Validation loss: 54, Average loss: 5.028585720945288\n",
      "Epoch: 1018, Len of Training loss: 18, Average loss: 3.414923244052463\n",
      "Len of Validation loss: 54, Average loss: 5.076671335432264\n",
      "Epoch: 1019, Len of Training loss: 18, Average loss: 4.287896169556512\n",
      "Len of Validation loss: 54, Average loss: 5.674298869238959\n",
      "Epoch: 1020, Len of Training loss: 18, Average loss: 4.1537779569625854\n",
      "Len of Validation loss: 54, Average loss: 6.115686646214238\n",
      "Epoch: 1021, Len of Training loss: 18, Average loss: 3.757488965988159\n",
      "Len of Validation loss: 54, Average loss: 5.382549824538054\n",
      "Epoch: 1022, Len of Training loss: 18, Average loss: 3.5923956632614136\n",
      "Len of Validation loss: 54, Average loss: 5.015076531304254\n",
      "Epoch: 1023, Len of Training loss: 18, Average loss: 3.8020854128731623\n",
      "Len of Validation loss: 54, Average loss: 5.022931407999109\n",
      "Epoch: 1024, Len of Training loss: 18, Average loss: 3.561475078264872\n",
      "Len of Validation loss: 54, Average loss: 4.847572200828129\n",
      "Epoch: 1025, Len of Training loss: 18, Average loss: 3.5063624911838107\n",
      "Len of Validation loss: 54, Average loss: 5.11900719889888\n",
      "Epoch: 1026, Len of Training loss: 18, Average loss: 3.4835484557681613\n",
      "Len of Validation loss: 54, Average loss: 4.969569303371288\n",
      "Epoch: 1027, Len of Training loss: 18, Average loss: 3.3482869068781533\n",
      "Len of Validation loss: 54, Average loss: 5.377193832838977\n",
      "Epoch: 1028, Len of Training loss: 18, Average loss: 3.4599477450052896\n",
      "Len of Validation loss: 54, Average loss: 4.930981349062036\n",
      "Epoch: 1029, Len of Training loss: 18, Average loss: 3.1773819393581815\n",
      "Len of Validation loss: 54, Average loss: 4.917116105556488\n",
      "Epoch: 1030, Len of Training loss: 18, Average loss: 3.1442071861690946\n",
      "Len of Validation loss: 54, Average loss: 4.700948218504588\n",
      "Epoch: 1031, Len of Training loss: 18, Average loss: 3.2815011739730835\n",
      "Len of Validation loss: 54, Average loss: 4.842867173530437\n",
      "Epoch: 1032, Len of Training loss: 18, Average loss: 3.2958457602394953\n",
      "Len of Validation loss: 54, Average loss: 4.870322814694157\n",
      "Epoch: 1033, Len of Training loss: 18, Average loss: 3.2422035932540894\n",
      "Len of Validation loss: 54, Average loss: 4.976954020835735\n",
      "Epoch: 1034, Len of Training loss: 18, Average loss: 3.108641359541151\n",
      "Len of Validation loss: 54, Average loss: 4.892069785683243\n",
      "Epoch: 1035, Len of Training loss: 18, Average loss: 3.0859935813479953\n",
      "Len of Validation loss: 54, Average loss: 4.758878019120958\n",
      "Epoch: 1036, Len of Training loss: 18, Average loss: 3.076863924662272\n",
      "Len of Validation loss: 54, Average loss: 4.888092590702905\n",
      "Epoch: 1037, Len of Training loss: 18, Average loss: 3.081351121266683\n",
      "Len of Validation loss: 54, Average loss: 4.777239388889736\n",
      "Epoch: 1038, Len of Training loss: 18, Average loss: 3.037727157274882\n",
      "Len of Validation loss: 54, Average loss: 4.70164821545283\n",
      "Epoch: 1039, Len of Training loss: 18, Average loss: 3.112836241722107\n",
      "Len of Validation loss: 54, Average loss: 4.945969301241416\n",
      "Epoch: 1040, Len of Training loss: 18, Average loss: 3.574057287640042\n",
      "Len of Validation loss: 54, Average loss: 5.72055881994742\n",
      "Epoch: 1041, Len of Training loss: 18, Average loss: 3.5895833836661444\n",
      "Len of Validation loss: 54, Average loss: 4.896989654611658\n",
      "Epoch: 1042, Len of Training loss: 18, Average loss: 3.369480742348565\n",
      "Len of Validation loss: 54, Average loss: 4.89787776823397\n",
      "Epoch: 1043, Len of Training loss: 18, Average loss: 3.2778631183836193\n",
      "Len of Validation loss: 54, Average loss: 4.988865119439584\n",
      "Epoch: 1044, Len of Training loss: 18, Average loss: 3.247475915484958\n",
      "Len of Validation loss: 54, Average loss: 4.706494373303872\n",
      "Epoch: 1045, Len of Training loss: 18, Average loss: 3.120099359088474\n",
      "Len of Validation loss: 54, Average loss: 4.6747796314734\n",
      "Epoch: 1046, Len of Training loss: 18, Average loss: 3.037588636080424\n",
      "Len of Validation loss: 54, Average loss: 5.154958098023026\n",
      "Epoch: 1047, Len of Training loss: 18, Average loss: 3.017147421836853\n",
      "Len of Validation loss: 54, Average loss: 4.749969274909408\n",
      "Epoch: 1048, Len of Training loss: 18, Average loss: 2.9821259842978582\n",
      "Len of Validation loss: 54, Average loss: 4.636846348091408\n",
      "Epoch: 1049, Len of Training loss: 18, Average loss: 2.941215170754327\n",
      "Len of Validation loss: 54, Average loss: 4.700002140469021\n",
      "Epoch: 1050, Len of Training loss: 18, Average loss: 3.078339841630724\n",
      "Len of Validation loss: 54, Average loss: 4.965501096513536\n",
      "Epoch: 1051, Len of Training loss: 18, Average loss: 3.152688079410129\n",
      "Len of Validation loss: 54, Average loss: 4.705832423987212\n",
      "Epoch: 1052, Len of Training loss: 18, Average loss: 3.2628366152445474\n",
      "Len of Validation loss: 54, Average loss: 5.3460621326058\n",
      "Epoch: 1053, Len of Training loss: 18, Average loss: 3.951623386806912\n",
      "Len of Validation loss: 54, Average loss: 5.021024372842577\n",
      "Epoch: 1054, Len of Training loss: 18, Average loss: 3.576693203714159\n",
      "Len of Validation loss: 54, Average loss: 5.168467733595106\n",
      "Epoch: 1055, Len of Training loss: 18, Average loss: 3.449162403742472\n",
      "Len of Validation loss: 54, Average loss: 4.924314070630957\n",
      "Epoch: 1056, Len of Training loss: 18, Average loss: 3.454350471496582\n",
      "Len of Validation loss: 54, Average loss: 5.043723865791604\n",
      "Epoch: 1057, Len of Training loss: 18, Average loss: 3.1799272961086698\n",
      "Len of Validation loss: 54, Average loss: 4.80205183117478\n",
      "Epoch: 1058, Len of Training loss: 18, Average loss: 2.996836874220106\n",
      "Len of Validation loss: 54, Average loss: 4.786007914278242\n",
      "Epoch: 1059, Len of Training loss: 18, Average loss: 3.1758270925945706\n",
      "Len of Validation loss: 54, Average loss: 5.10410620327349\n",
      "Epoch: 1060, Len of Training loss: 18, Average loss: 3.1880486806233725\n",
      "Len of Validation loss: 54, Average loss: 4.617536107699077\n",
      "Epoch: 1061, Len of Training loss: 18, Average loss: 3.080589122242398\n",
      "Len of Validation loss: 54, Average loss: 4.677882276199482\n",
      "Epoch: 1062, Len of Training loss: 18, Average loss: 3.1325738694932728\n",
      "Len of Validation loss: 54, Average loss: 4.992949309172453\n",
      "Epoch: 1063, Len of Training loss: 18, Average loss: 3.312292125489977\n",
      "Len of Validation loss: 54, Average loss: 4.73614239692688\n",
      "Epoch: 1064, Len of Training loss: 18, Average loss: 3.222362518310547\n",
      "Len of Validation loss: 54, Average loss: 4.85281636538329\n",
      "Epoch: 1065, Len of Training loss: 18, Average loss: 3.3059361113442316\n",
      "Len of Validation loss: 54, Average loss: 4.711488796605004\n",
      "Epoch: 1066, Len of Training loss: 18, Average loss: 3.0983219941457114\n",
      "Len of Validation loss: 54, Average loss: 4.557847162087758\n",
      "Epoch: 1067, Len of Training loss: 18, Average loss: 3.074948853916592\n",
      "Len of Validation loss: 54, Average loss: 4.7644726082130715\n",
      "Epoch: 1068, Len of Training loss: 18, Average loss: 3.120445410410563\n",
      "Len of Validation loss: 54, Average loss: 4.6743340492248535\n",
      "Epoch: 1069, Len of Training loss: 18, Average loss: 3.002812875641717\n",
      "Len of Validation loss: 54, Average loss: 4.914321252593288\n",
      "Epoch: 1070, Len of Training loss: 18, Average loss: 2.958064834276835\n",
      "Len of Validation loss: 54, Average loss: 4.63539778523975\n",
      "Epoch: 1071, Len of Training loss: 18, Average loss: 3.1946165694130793\n",
      "Len of Validation loss: 54, Average loss: 4.865163664023082\n",
      "Epoch: 1072, Len of Training loss: 18, Average loss: 3.2824190987481012\n",
      "Len of Validation loss: 54, Average loss: 5.089293005289854\n",
      "Epoch: 1073, Len of Training loss: 18, Average loss: 3.4485339058770075\n",
      "Len of Validation loss: 54, Average loss: 5.0421734518475\n",
      "Epoch: 1074, Len of Training loss: 18, Average loss: 4.087274683846368\n",
      "Len of Validation loss: 54, Average loss: 5.550263947910732\n",
      "Epoch: 1075, Len of Training loss: 18, Average loss: 3.5208079947365656\n",
      "Len of Validation loss: 54, Average loss: 4.642800717442124\n",
      "Epoch: 1076, Len of Training loss: 18, Average loss: 3.167294489012824\n",
      "Len of Validation loss: 54, Average loss: 4.976566592852275\n",
      "Epoch: 1077, Len of Training loss: 18, Average loss: 3.274975366062588\n",
      "Len of Validation loss: 54, Average loss: 4.899184591240353\n",
      "Epoch: 1078, Len of Training loss: 18, Average loss: 3.109153403176202\n",
      "Len of Validation loss: 54, Average loss: 4.923957637062779\n",
      "Epoch: 1079, Len of Training loss: 18, Average loss: 3.0355943044026694\n",
      "Len of Validation loss: 54, Average loss: 4.8490240154442965\n",
      "Epoch: 1080, Len of Training loss: 18, Average loss: 2.9160569111506143\n",
      "Len of Validation loss: 54, Average loss: 4.633185029029846\n",
      "Epoch: 1081, Len of Training loss: 18, Average loss: 2.948916064368354\n",
      "Len of Validation loss: 54, Average loss: 4.582408624666709\n",
      "Epoch: 1082, Len of Training loss: 18, Average loss: 2.86409772766961\n",
      "Len of Validation loss: 54, Average loss: 4.815073134722533\n",
      "Epoch: 1083, Len of Training loss: 18, Average loss: 2.825737026002672\n",
      "Len of Validation loss: 54, Average loss: 4.57231107906059\n",
      "Epoch: 1084, Len of Training loss: 18, Average loss: 3.0550056298573813\n",
      "Len of Validation loss: 54, Average loss: 4.668323183501208\n",
      "Epoch: 1085, Len of Training loss: 18, Average loss: 3.1498056252797446\n",
      "Len of Validation loss: 54, Average loss: 4.906610363059574\n",
      "Epoch: 1086, Len of Training loss: 18, Average loss: 3.1373533142937555\n",
      "Len of Validation loss: 54, Average loss: 5.18759302298228\n",
      "Epoch: 1087, Len of Training loss: 18, Average loss: 3.3929570515950522\n",
      "Len of Validation loss: 54, Average loss: 4.748944403948607\n",
      "Epoch: 1088, Len of Training loss: 18, Average loss: 3.1047192017237344\n",
      "Len of Validation loss: 54, Average loss: 4.765641400107631\n",
      "Epoch: 1089, Len of Training loss: 18, Average loss: 3.112389326095581\n",
      "Len of Validation loss: 54, Average loss: 4.655147444318842\n",
      "Epoch: 1090, Len of Training loss: 18, Average loss: 2.9917035500208535\n",
      "Len of Validation loss: 54, Average loss: 4.829722735616896\n",
      "Epoch: 1091, Len of Training loss: 18, Average loss: 2.965854671266344\n",
      "Len of Validation loss: 54, Average loss: 4.860806200239393\n",
      "Epoch: 1092, Len of Training loss: 18, Average loss: 2.9299161036809287\n",
      "Len of Validation loss: 54, Average loss: 4.654982776553543\n",
      "Epoch: 1093, Len of Training loss: 18, Average loss: 2.8607509401109485\n",
      "Len of Validation loss: 54, Average loss: 4.802205333003291\n",
      "Epoch: 1094, Len of Training loss: 18, Average loss: 2.868495093451606\n",
      "Len of Validation loss: 54, Average loss: 4.526871356699202\n",
      "Epoch: 1095, Len of Training loss: 18, Average loss: 2.8222719960742526\n",
      "Len of Validation loss: 54, Average loss: 4.945578769401267\n",
      "Epoch: 1096, Len of Training loss: 18, Average loss: 2.871329148610433\n",
      "Len of Validation loss: 54, Average loss: 4.54468470370328\n",
      "Epoch: 1097, Len of Training loss: 18, Average loss: 2.7362234195073447\n",
      "Len of Validation loss: 54, Average loss: 4.5765461413948625\n",
      "Epoch: 1098, Len of Training loss: 18, Average loss: 2.7241574128468833\n",
      "Len of Validation loss: 54, Average loss: 4.530364444962254\n",
      "Epoch: 1099, Len of Training loss: 18, Average loss: 2.8340369860331216\n",
      "Len of Validation loss: 54, Average loss: 4.6777830697872025\n",
      "Epoch: 1100, Len of Training loss: 18, Average loss: 2.997722612486945\n",
      "Len of Validation loss: 54, Average loss: 4.888124960440177\n",
      "Epoch: 1101, Len of Training loss: 18, Average loss: 2.9161187012990317\n",
      "Len of Validation loss: 54, Average loss: 4.694364463841474\n",
      "Epoch: 1102, Len of Training loss: 18, Average loss: 2.908501638306512\n",
      "Len of Validation loss: 54, Average loss: 4.500050134128994\n",
      "Epoch: 1103, Len of Training loss: 18, Average loss: 2.840157879723443\n",
      "Len of Validation loss: 54, Average loss: 4.60885649698752\n",
      "Epoch: 1104, Len of Training loss: 18, Average loss: 2.759109616279602\n",
      "Len of Validation loss: 54, Average loss: 4.787239379352993\n",
      "Epoch: 1105, Len of Training loss: 18, Average loss: 2.9329173829820423\n",
      "Len of Validation loss: 54, Average loss: 4.560753113693661\n",
      "Epoch: 1106, Len of Training loss: 18, Average loss: 2.871883061197069\n",
      "Len of Validation loss: 54, Average loss: 4.66903199089898\n",
      "Epoch: 1107, Len of Training loss: 18, Average loss: 2.828803300857544\n",
      "Len of Validation loss: 54, Average loss: 4.577675616299665\n",
      "Epoch: 1108, Len of Training loss: 18, Average loss: 2.766970250341627\n",
      "Len of Validation loss: 54, Average loss: 4.758294703783812\n",
      "Epoch: 1109, Len of Training loss: 18, Average loss: 2.782914810710483\n",
      "Len of Validation loss: 54, Average loss: 6.898216437410425\n",
      "Epoch: 1110, Len of Training loss: 18, Average loss: 3.055552985933092\n",
      "Len of Validation loss: 54, Average loss: 4.614861925443013\n",
      "Epoch: 1111, Len of Training loss: 18, Average loss: 2.8386294576856823\n",
      "Len of Validation loss: 54, Average loss: 4.929029597176446\n",
      "Epoch: 1112, Len of Training loss: 18, Average loss: 2.888298432032267\n",
      "Len of Validation loss: 54, Average loss: 4.750719995410354\n",
      "Epoch: 1113, Len of Training loss: 18, Average loss: 2.87107867664761\n",
      "Len of Validation loss: 54, Average loss: 4.739134249863802\n",
      "Epoch: 1114, Len of Training loss: 18, Average loss: 3.0053074492348566\n",
      "Len of Validation loss: 54, Average loss: 4.681315739949544\n",
      "Epoch: 1115, Len of Training loss: 18, Average loss: 3.1306342548794217\n",
      "Len of Validation loss: 54, Average loss: 5.011609289381239\n",
      "Epoch: 1116, Len of Training loss: 18, Average loss: 3.0745554765065513\n",
      "Len of Validation loss: 54, Average loss: 4.790598295353077\n",
      "Epoch: 1117, Len of Training loss: 18, Average loss: 2.8727012475331626\n",
      "Len of Validation loss: 54, Average loss: 4.584617583839981\n",
      "Epoch: 1118, Len of Training loss: 18, Average loss: 2.84249918990665\n",
      "Len of Validation loss: 54, Average loss: 4.5987278951538935\n",
      "Epoch: 1119, Len of Training loss: 18, Average loss: 2.8584204647276135\n",
      "Len of Validation loss: 54, Average loss: 4.873252707499045\n",
      "Epoch: 1120, Len of Training loss: 18, Average loss: 2.8011893563800387\n",
      "Len of Validation loss: 54, Average loss: 4.4857557128976895\n",
      "Epoch: 1121, Len of Training loss: 18, Average loss: 2.758570909500122\n",
      "Len of Validation loss: 54, Average loss: 4.687630313414115\n",
      "Epoch: 1122, Len of Training loss: 18, Average loss: 2.6358404689364963\n",
      "Len of Validation loss: 54, Average loss: 4.632410769109373\n",
      "Epoch: 1123, Len of Training loss: 18, Average loss: 2.929084645377265\n",
      "Len of Validation loss: 54, Average loss: 4.619709588863231\n",
      "Epoch: 1124, Len of Training loss: 18, Average loss: 3.014410138130188\n",
      "Len of Validation loss: 54, Average loss: 4.978581441773309\n",
      "Epoch: 1125, Len of Training loss: 18, Average loss: 2.885441952281528\n",
      "Len of Validation loss: 54, Average loss: 4.7452239018899425\n",
      "Epoch: 1126, Len of Training loss: 18, Average loss: 2.786248975329929\n",
      "Len of Validation loss: 54, Average loss: 4.373707828698335\n",
      "Epoch: 1127, Len of Training loss: 18, Average loss: 2.768989774915907\n",
      "Len of Validation loss: 54, Average loss: 4.362373257124865\n",
      "Epoch: 1128, Len of Training loss: 18, Average loss: 2.761758327484131\n",
      "Len of Validation loss: 54, Average loss: 4.6297195884916515\n",
      "Epoch: 1129, Len of Training loss: 18, Average loss: 2.6849148670832315\n",
      "Len of Validation loss: 54, Average loss: 4.5132009916835365\n",
      "Epoch: 1130, Len of Training loss: 18, Average loss: 2.6890996827019586\n",
      "Len of Validation loss: 54, Average loss: 4.581448016343294\n",
      "Epoch: 1131, Len of Training loss: 18, Average loss: 2.7698024113972983\n",
      "Len of Validation loss: 54, Average loss: 4.760153947053133\n",
      "Epoch: 1132, Len of Training loss: 18, Average loss: 2.8408878909216986\n",
      "Len of Validation loss: 54, Average loss: 4.902461764989076\n",
      "Epoch: 1133, Len of Training loss: 18, Average loss: 2.8506740861468844\n",
      "Len of Validation loss: 54, Average loss: 4.588103517338082\n",
      "Epoch: 1134, Len of Training loss: 18, Average loss: 2.8192597097820706\n",
      "Len of Validation loss: 54, Average loss: 4.654375356656534\n",
      "Epoch: 1135, Len of Training loss: 18, Average loss: 3.0005477931764393\n",
      "Len of Validation loss: 54, Average loss: 5.0359384439609665\n",
      "Epoch: 1136, Len of Training loss: 18, Average loss: 3.330314517021179\n",
      "Len of Validation loss: 54, Average loss: 4.8514419970689\n",
      "Epoch: 1137, Len of Training loss: 18, Average loss: 3.0746840635935464\n",
      "Len of Validation loss: 54, Average loss: 4.737210821222376\n",
      "Epoch: 1138, Len of Training loss: 18, Average loss: 2.8863150543636746\n",
      "Len of Validation loss: 54, Average loss: 4.44060116564786\n",
      "Epoch: 1139, Len of Training loss: 18, Average loss: 2.736759318245782\n",
      "Len of Validation loss: 54, Average loss: 4.658441839394746\n",
      "Epoch: 1140, Len of Training loss: 18, Average loss: 2.779381513595581\n",
      "Len of Validation loss: 54, Average loss: 4.391008873780568\n",
      "Epoch: 1141, Len of Training loss: 18, Average loss: 2.7452588611178927\n",
      "Len of Validation loss: 54, Average loss: 4.68280538143935\n",
      "Epoch: 1142, Len of Training loss: 18, Average loss: 2.7977909247080484\n",
      "Len of Validation loss: 54, Average loss: 4.5949970130567195\n",
      "Epoch: 1143, Len of Training loss: 18, Average loss: 2.7499514288372464\n",
      "Len of Validation loss: 54, Average loss: 4.361712170971765\n",
      "Epoch: 1144, Len of Training loss: 18, Average loss: 2.62762090894911\n",
      "Len of Validation loss: 54, Average loss: 4.502885518250642\n",
      "Epoch: 1145, Len of Training loss: 18, Average loss: 2.6084678835339017\n",
      "Len of Validation loss: 54, Average loss: 4.553018068825757\n",
      "Epoch: 1146, Len of Training loss: 18, Average loss: 2.7864335775375366\n",
      "Len of Validation loss: 54, Average loss: 4.589593121299037\n",
      "Epoch: 1147, Len of Training loss: 18, Average loss: 2.85291678375668\n",
      "Len of Validation loss: 54, Average loss: 4.554964061136599\n",
      "Epoch: 1148, Len of Training loss: 18, Average loss: 2.928131580352783\n",
      "Len of Validation loss: 54, Average loss: 4.603576108261391\n",
      "Epoch: 1149, Len of Training loss: 18, Average loss: 2.820081737306383\n",
      "Len of Validation loss: 54, Average loss: 4.50515181046945\n",
      "Epoch: 1150, Len of Training loss: 18, Average loss: 2.6452399094899497\n",
      "Len of Validation loss: 54, Average loss: 4.546257374463258\n",
      "Epoch: 1151, Len of Training loss: 18, Average loss: 2.6356624497307672\n",
      "Len of Validation loss: 54, Average loss: 4.46496719121933\n",
      "Epoch: 1152, Len of Training loss: 18, Average loss: 2.655283384852939\n",
      "Len of Validation loss: 54, Average loss: 4.5995945047449185\n",
      "Epoch: 1153, Len of Training loss: 18, Average loss: 2.6150012016296387\n",
      "Len of Validation loss: 54, Average loss: 4.535554338384558\n",
      "Epoch: 1154, Len of Training loss: 18, Average loss: 2.5904742081960044\n",
      "Len of Validation loss: 54, Average loss: 4.654471426098435\n",
      "Epoch: 1155, Len of Training loss: 18, Average loss: 2.670695251888699\n",
      "Len of Validation loss: 54, Average loss: 4.818519742400558\n",
      "Epoch: 1156, Len of Training loss: 18, Average loss: 3.1682794094085693\n",
      "Len of Validation loss: 54, Average loss: 4.503167309142925\n",
      "Epoch: 1157, Len of Training loss: 18, Average loss: 3.4100661940044827\n",
      "Len of Validation loss: 54, Average loss: 5.067896273401049\n",
      "Epoch: 1158, Len of Training loss: 18, Average loss: 3.6796429554621377\n",
      "Len of Validation loss: 54, Average loss: 5.43228386066578\n",
      "Epoch: 1159, Len of Training loss: 18, Average loss: 3.515991634792752\n",
      "Len of Validation loss: 54, Average loss: 4.9309734989095615\n",
      "Epoch: 1160, Len of Training loss: 18, Average loss: 3.4069925281736584\n",
      "Len of Validation loss: 54, Average loss: 4.762592147897791\n",
      "Epoch: 1161, Len of Training loss: 18, Average loss: 3.1229666074117026\n",
      "Len of Validation loss: 54, Average loss: 4.899572866934317\n",
      "Epoch: 1162, Len of Training loss: 18, Average loss: 2.9940983984205456\n",
      "Len of Validation loss: 54, Average loss: 4.558255774003488\n",
      "Epoch: 1163, Len of Training loss: 18, Average loss: 2.725626336203681\n",
      "Len of Validation loss: 54, Average loss: 4.522381707474038\n",
      "Epoch: 1164, Len of Training loss: 18, Average loss: 2.665590657128228\n",
      "Len of Validation loss: 54, Average loss: 4.518838981787364\n",
      "Epoch: 1165, Len of Training loss: 18, Average loss: 2.5834513240390353\n",
      "Len of Validation loss: 54, Average loss: 4.636907420776509\n",
      "Epoch: 1166, Len of Training loss: 18, Average loss: 2.6541346311569214\n",
      "Len of Validation loss: 54, Average loss: 4.476866858976859\n",
      "Epoch: 1167, Len of Training loss: 18, Average loss: 2.5546972486707897\n",
      "Len of Validation loss: 54, Average loss: 4.3402840649640115\n",
      "Epoch: 1168, Len of Training loss: 18, Average loss: 2.5209790070851645\n",
      "Len of Validation loss: 54, Average loss: 4.621089321595651\n",
      "Epoch: 1169, Len of Training loss: 18, Average loss: 2.5944423145718045\n",
      "Len of Validation loss: 54, Average loss: 4.673493957078016\n",
      "Epoch: 1170, Len of Training loss: 18, Average loss: 2.485425670941671\n",
      "Len of Validation loss: 54, Average loss: 4.274736470646328\n",
      "Epoch: 1171, Len of Training loss: 18, Average loss: 2.5453730291790433\n",
      "Len of Validation loss: 54, Average loss: 4.439781222078535\n",
      "Epoch: 1172, Len of Training loss: 18, Average loss: 2.5410829252666898\n",
      "Len of Validation loss: 54, Average loss: 4.358273773281662\n",
      "Epoch: 1173, Len of Training loss: 18, Average loss: 2.4821185535854764\n",
      "Len of Validation loss: 54, Average loss: 4.473638799455431\n",
      "Epoch: 1174, Len of Training loss: 18, Average loss: 2.3916004300117493\n",
      "Len of Validation loss: 54, Average loss: 4.296882452788176\n",
      "Epoch: 1175, Len of Training loss: 18, Average loss: 2.4381230804655285\n",
      "Len of Validation loss: 54, Average loss: 4.352126326825884\n",
      "Epoch: 1176, Len of Training loss: 18, Average loss: 2.5802412033081055\n",
      "Len of Validation loss: 54, Average loss: 5.16837845687513\n",
      "Epoch: 1177, Len of Training loss: 18, Average loss: 2.773387246661716\n",
      "Len of Validation loss: 54, Average loss: 4.591198495140782\n",
      "Epoch: 1178, Len of Training loss: 18, Average loss: 2.634852316644457\n",
      "Len of Validation loss: 54, Average loss: 4.4646208065527455\n",
      "Epoch: 1179, Len of Training loss: 18, Average loss: 2.6372863716549344\n",
      "Len of Validation loss: 54, Average loss: 4.500582421267474\n",
      "Epoch: 1180, Len of Training loss: 18, Average loss: 2.506549768977695\n",
      "Len of Validation loss: 54, Average loss: 4.251243291077791\n",
      "Epoch: 1181, Len of Training loss: 18, Average loss: 2.4404857291115656\n",
      "Len of Validation loss: 54, Average loss: 4.389676818141231\n",
      "Epoch: 1182, Len of Training loss: 18, Average loss: 2.4682703018188477\n",
      "Len of Validation loss: 54, Average loss: 4.314330677191417\n",
      "Epoch: 1183, Len of Training loss: 18, Average loss: 2.4172930320103965\n",
      "Len of Validation loss: 54, Average loss: 4.238369133737352\n",
      "Epoch: 1184, Len of Training loss: 18, Average loss: 2.463297035959032\n",
      "Len of Validation loss: 54, Average loss: 4.334928227795495\n",
      "Epoch: 1185, Len of Training loss: 18, Average loss: 2.5705752505196466\n",
      "Len of Validation loss: 54, Average loss: 4.368701705226192\n",
      "Epoch: 1186, Len of Training loss: 18, Average loss: 2.9014501174290976\n",
      "Len of Validation loss: 54, Average loss: 4.657935107195819\n",
      "Epoch: 1187, Len of Training loss: 18, Average loss: 3.0724861489401922\n",
      "Len of Validation loss: 54, Average loss: 4.7425072060690985\n",
      "Epoch: 1188, Len of Training loss: 18, Average loss: 3.2013285954793296\n",
      "Len of Validation loss: 54, Average loss: 5.356704107037297\n",
      "Epoch: 1189, Len of Training loss: 18, Average loss: 3.157140572865804\n",
      "Len of Validation loss: 54, Average loss: 4.622237304846446\n",
      "Epoch: 1190, Len of Training loss: 18, Average loss: 2.902249574661255\n",
      "Len of Validation loss: 54, Average loss: 4.906905249313072\n",
      "Epoch: 1191, Len of Training loss: 18, Average loss: 2.7325152688556247\n",
      "Len of Validation loss: 54, Average loss: 4.405015287575899\n",
      "Epoch: 1192, Len of Training loss: 18, Average loss: 2.6258663998709784\n",
      "Len of Validation loss: 54, Average loss: 4.648670856599455\n",
      "Epoch: 1193, Len of Training loss: 18, Average loss: 2.795944611231486\n",
      "Len of Validation loss: 54, Average loss: 4.688152419196235\n",
      "Epoch: 1194, Len of Training loss: 18, Average loss: 2.8222252792782254\n",
      "Len of Validation loss: 54, Average loss: 4.812588841826828\n",
      "Epoch: 1195, Len of Training loss: 18, Average loss: 2.933751755290561\n",
      "Len of Validation loss: 54, Average loss: 4.676838161768736\n",
      "Epoch: 1196, Len of Training loss: 18, Average loss: 2.8882806566026478\n",
      "Len of Validation loss: 54, Average loss: 4.552045535158228\n",
      "Epoch: 1197, Len of Training loss: 18, Average loss: 2.820370501942105\n",
      "Len of Validation loss: 54, Average loss: 4.734434957857485\n",
      "Epoch: 1198, Len of Training loss: 18, Average loss: 2.687704775068495\n",
      "Len of Validation loss: 54, Average loss: 4.541571787110081\n",
      "Epoch: 1199, Len of Training loss: 18, Average loss: 2.5140997568766275\n",
      "Len of Validation loss: 54, Average loss: 4.566959643805468\n",
      "Epoch: 1200, Len of Training loss: 18, Average loss: 2.437187155087789\n",
      "Len of Validation loss: 54, Average loss: 4.596927488291705\n",
      "Epoch: 1201, Len of Training loss: 18, Average loss: 2.4886349174711437\n",
      "Len of Validation loss: 54, Average loss: 4.285709464991534\n",
      "Epoch: 1202, Len of Training loss: 18, Average loss: 2.373097128338284\n",
      "Len of Validation loss: 54, Average loss: 4.120471517244975\n",
      "Epoch: 1203, Len of Training loss: 18, Average loss: 2.3584044443236456\n",
      "Len of Validation loss: 54, Average loss: 4.222585029072231\n",
      "Epoch: 1204, Len of Training loss: 18, Average loss: 2.3363365530967712\n",
      "Len of Validation loss: 54, Average loss: 4.2783197981339915\n",
      "Epoch: 1205, Len of Training loss: 18, Average loss: 2.3404901358816357\n",
      "Len of Validation loss: 54, Average loss: 4.215804945539545\n",
      "Epoch: 1206, Len of Training loss: 18, Average loss: 2.304758840137058\n",
      "Len of Validation loss: 54, Average loss: 4.158984069471006\n",
      "Epoch: 1207, Len of Training loss: 18, Average loss: 2.2674916121694775\n",
      "Len of Validation loss: 54, Average loss: 4.168326914310455\n",
      "Epoch: 1208, Len of Training loss: 18, Average loss: 2.2922310564253063\n",
      "Len of Validation loss: 54, Average loss: 4.162521335813734\n",
      "Epoch: 1209, Len of Training loss: 18, Average loss: 2.4694643285539417\n",
      "Len of Validation loss: 54, Average loss: 4.637797119440855\n",
      "Epoch: 1210, Len of Training loss: 18, Average loss: 2.5249337752660117\n",
      "Len of Validation loss: 54, Average loss: 4.371863566063069\n",
      "Epoch: 1211, Len of Training loss: 18, Average loss: 2.522116091516283\n",
      "Len of Validation loss: 54, Average loss: 4.654566769246702\n",
      "Epoch: 1212, Len of Training loss: 18, Average loss: 2.4473531908459134\n",
      "Len of Validation loss: 54, Average loss: 4.400720627219589\n",
      "Epoch: 1213, Len of Training loss: 18, Average loss: 3.0346981287002563\n",
      "Len of Validation loss: 54, Average loss: 4.738289089114578\n",
      "Epoch: 1214, Len of Training loss: 18, Average loss: 3.01351269086202\n",
      "Len of Validation loss: 54, Average loss: 4.466139883906753\n",
      "Epoch: 1215, Len of Training loss: 18, Average loss: 2.9765302340189614\n",
      "Len of Validation loss: 54, Average loss: 4.781362043486701\n",
      "Epoch: 1216, Len of Training loss: 18, Average loss: 3.0174652338027954\n",
      "Len of Validation loss: 54, Average loss: 4.6575295218714965\n",
      "Epoch: 1217, Len of Training loss: 18, Average loss: 2.853430893686083\n",
      "Len of Validation loss: 54, Average loss: 4.420333676868015\n",
      "Epoch: 1218, Len of Training loss: 18, Average loss: 2.6179016563627453\n",
      "Len of Validation loss: 54, Average loss: 4.438403056727515\n",
      "Epoch: 1219, Len of Training loss: 18, Average loss: 2.602692166964213\n",
      "Len of Validation loss: 54, Average loss: 4.485456058272609\n",
      "Epoch: 1220, Len of Training loss: 18, Average loss: 2.5583264695273504\n",
      "Len of Validation loss: 54, Average loss: 4.357286749062715\n",
      "Epoch: 1221, Len of Training loss: 18, Average loss: 2.5677553945117526\n",
      "Len of Validation loss: 54, Average loss: 4.298027645658563\n",
      "Epoch: 1222, Len of Training loss: 18, Average loss: 2.4990053839153714\n",
      "Len of Validation loss: 54, Average loss: 4.5630176575095565\n",
      "Epoch: 1223, Len of Training loss: 18, Average loss: 2.5467516581217446\n",
      "Len of Validation loss: 54, Average loss: 4.3107936691354825\n",
      "Epoch: 1224, Len of Training loss: 18, Average loss: 2.50385844707489\n",
      "Len of Validation loss: 54, Average loss: 4.765454607981223\n",
      "Epoch: 1225, Len of Training loss: 18, Average loss: 2.6684853633244834\n",
      "Len of Validation loss: 54, Average loss: 4.313964973997186\n",
      "Epoch: 1226, Len of Training loss: 18, Average loss: 2.5688038137223987\n",
      "Len of Validation loss: 54, Average loss: 4.398001646554029\n",
      "Epoch: 1227, Len of Training loss: 18, Average loss: 2.584885517756144\n",
      "Len of Validation loss: 54, Average loss: 4.253445971895148\n",
      "Epoch: 1228, Len of Training loss: 18, Average loss: 2.490555511580573\n",
      "Len of Validation loss: 54, Average loss: 4.386137827678963\n",
      "Epoch: 1229, Len of Training loss: 18, Average loss: 2.3462011284298367\n",
      "Len of Validation loss: 54, Average loss: 4.314592348204719\n",
      "Epoch: 1230, Len of Training loss: 18, Average loss: 2.32985563410653\n",
      "Len of Validation loss: 54, Average loss: 4.491983731587728\n",
      "Epoch: 1231, Len of Training loss: 18, Average loss: 2.428532838821411\n",
      "Len of Validation loss: 54, Average loss: 4.444480004134001\n",
      "Epoch: 1232, Len of Training loss: 18, Average loss: 2.408506009313795\n",
      "Len of Validation loss: 54, Average loss: 4.288810363522282\n",
      "Epoch: 1233, Len of Training loss: 18, Average loss: 2.3139539692136974\n",
      "Len of Validation loss: 54, Average loss: 4.088465595686877\n",
      "Epoch: 1234, Len of Training loss: 18, Average loss: 2.3404793010817633\n",
      "Len of Validation loss: 54, Average loss: 4.189294340433897\n",
      "Epoch: 1235, Len of Training loss: 18, Average loss: 2.6254484256108603\n",
      "Len of Validation loss: 54, Average loss: 4.604614193792696\n",
      "Epoch: 1236, Len of Training loss: 18, Average loss: 2.5207998620139227\n",
      "Len of Validation loss: 54, Average loss: 4.439880289413311\n",
      "Epoch: 1237, Len of Training loss: 18, Average loss: 2.5106252166959973\n",
      "Len of Validation loss: 54, Average loss: 4.415256065350992\n",
      "Epoch: 1238, Len of Training loss: 18, Average loss: 2.3741881317562528\n",
      "Len of Validation loss: 54, Average loss: 4.175828666598709\n",
      "Epoch: 1239, Len of Training loss: 18, Average loss: 2.2486649089389377\n",
      "Len of Validation loss: 54, Average loss: 4.296542827729826\n",
      "Epoch: 1240, Len of Training loss: 18, Average loss: 2.228572984536489\n",
      "Len of Validation loss: 54, Average loss: 4.359164279920083\n",
      "Epoch: 1241, Len of Training loss: 18, Average loss: 2.208172698815664\n",
      "Len of Validation loss: 54, Average loss: 4.2455636351196855\n",
      "Epoch: 1242, Len of Training loss: 18, Average loss: 2.1820767654312982\n",
      "Len of Validation loss: 54, Average loss: 4.25684458679623\n",
      "Epoch: 1243, Len of Training loss: 18, Average loss: 2.2090896169344583\n",
      "Len of Validation loss: 54, Average loss: 4.553609971646909\n",
      "Epoch: 1244, Len of Training loss: 18, Average loss: 2.4444813330968223\n",
      "Len of Validation loss: 54, Average loss: 4.33856757040377\n",
      "Epoch: 1245, Len of Training loss: 18, Average loss: 2.4017505910661487\n",
      "Len of Validation loss: 54, Average loss: 4.345598984647681\n",
      "Epoch: 1246, Len of Training loss: 18, Average loss: 2.5210120015674167\n",
      "Len of Validation loss: 54, Average loss: 4.488543773138964\n",
      "Epoch: 1247, Len of Training loss: 18, Average loss: 2.6617423031065197\n",
      "Len of Validation loss: 54, Average loss: 4.222143497731951\n",
      "Epoch: 1248, Len of Training loss: 18, Average loss: 2.455978830655416\n",
      "Len of Validation loss: 54, Average loss: 4.408313786541974\n",
      "Epoch: 1249, Len of Training loss: 18, Average loss: 2.4285470644632974\n",
      "Len of Validation loss: 54, Average loss: 4.308964254679503\n",
      "Epoch: 1250, Len of Training loss: 18, Average loss: 2.373319466908773\n",
      "Len of Validation loss: 54, Average loss: 4.4271492582780345\n",
      "Epoch: 1251, Len of Training loss: 18, Average loss: 2.422490749094221\n",
      "Len of Validation loss: 54, Average loss: 4.3943562485553604\n",
      "Epoch: 1252, Len of Training loss: 18, Average loss: 2.3176771799723306\n",
      "Len of Validation loss: 54, Average loss: 4.365659574667613\n",
      "Epoch: 1253, Len of Training loss: 18, Average loss: 2.325048075781928\n",
      "Len of Validation loss: 54, Average loss: 4.103042017530512\n",
      "Epoch: 1254, Len of Training loss: 18, Average loss: 2.1978142725096808\n",
      "Len of Validation loss: 54, Average loss: 4.239916346691273\n",
      "Epoch: 1255, Len of Training loss: 18, Average loss: 2.2659275465541415\n",
      "Len of Validation loss: 54, Average loss: 4.401821463196366\n",
      "Epoch: 1256, Len of Training loss: 18, Average loss: 2.7392854425642224\n",
      "Len of Validation loss: 54, Average loss: 4.537863841763249\n",
      "Epoch: 1257, Len of Training loss: 18, Average loss: 2.8368831740485296\n",
      "Len of Validation loss: 54, Average loss: 5.128539813889398\n",
      "Epoch: 1258, Len of Training loss: 18, Average loss: 3.0582055515713162\n",
      "Len of Validation loss: 54, Average loss: 4.896164110413304\n",
      "Epoch: 1259, Len of Training loss: 18, Average loss: 2.92115146583981\n",
      "Len of Validation loss: 54, Average loss: 4.694544204959163\n",
      "Epoch: 1260, Len of Training loss: 18, Average loss: 2.539846036169264\n",
      "Len of Validation loss: 54, Average loss: 4.233291701034263\n",
      "Epoch: 1261, Len of Training loss: 18, Average loss: 2.490221632851495\n",
      "Len of Validation loss: 54, Average loss: 4.329045364150295\n",
      "Epoch: 1262, Len of Training loss: 18, Average loss: 2.3185877005259194\n",
      "Len of Validation loss: 54, Average loss: 4.108466777536604\n",
      "Epoch: 1263, Len of Training loss: 18, Average loss: 2.306230260266198\n",
      "Len of Validation loss: 54, Average loss: 4.295282284418742\n",
      "Epoch: 1264, Len of Training loss: 18, Average loss: 2.280201640393999\n",
      "Len of Validation loss: 54, Average loss: 4.620141351664508\n",
      "Epoch: 1265, Len of Training loss: 18, Average loss: 2.2701588537957935\n",
      "Len of Validation loss: 54, Average loss: 4.228668117964709\n",
      "Epoch: 1266, Len of Training loss: 18, Average loss: 2.2153568665186563\n",
      "Len of Validation loss: 54, Average loss: 4.310520960224999\n",
      "Epoch: 1267, Len of Training loss: 18, Average loss: 2.2383394969834223\n",
      "Len of Validation loss: 54, Average loss: 4.13973852219405\n",
      "Epoch: 1268, Len of Training loss: 18, Average loss: 2.206559273931715\n",
      "Len of Validation loss: 54, Average loss: 4.223047896667763\n",
      "Epoch: 1269, Len of Training loss: 18, Average loss: 2.221346457799276\n",
      "Len of Validation loss: 54, Average loss: 4.2144231994946795\n",
      "Epoch: 1270, Len of Training loss: 18, Average loss: 2.2313922113842435\n",
      "Len of Validation loss: 54, Average loss: 4.732115637373041\n",
      "Epoch: 1271, Len of Training loss: 18, Average loss: 2.3551138506995306\n",
      "Len of Validation loss: 54, Average loss: 4.252995212872823\n",
      "Epoch: 1272, Len of Training loss: 18, Average loss: 2.3658038907580905\n",
      "Len of Validation loss: 54, Average loss: 4.371475570731693\n",
      "Epoch: 1273, Len of Training loss: 18, Average loss: 2.343124998940362\n",
      "Len of Validation loss: 54, Average loss: 4.589398415000351\n",
      "Epoch: 1274, Len of Training loss: 18, Average loss: 2.5529714160495334\n",
      "Len of Validation loss: 54, Average loss: 4.454767330929085\n",
      "Epoch: 1275, Len of Training loss: 18, Average loss: 2.3460035853915744\n",
      "Len of Validation loss: 54, Average loss: 4.521185287722835\n",
      "Epoch: 1276, Len of Training loss: 18, Average loss: 2.4498697651757135\n",
      "Len of Validation loss: 54, Average loss: 4.3778396403348\n",
      "Epoch: 1277, Len of Training loss: 18, Average loss: 2.4494405852423773\n",
      "Len of Validation loss: 54, Average loss: 4.386180270601202\n",
      "Epoch: 1278, Len of Training loss: 18, Average loss: 2.551681306627062\n",
      "Len of Validation loss: 54, Average loss: 4.530892745212272\n",
      "Epoch: 1279, Len of Training loss: 18, Average loss: 2.429431994756063\n",
      "Len of Validation loss: 54, Average loss: 4.5506101625937\n",
      "Epoch: 1280, Len of Training loss: 18, Average loss: 2.5683970848719277\n",
      "Len of Validation loss: 54, Average loss: 4.461178554428948\n",
      "Epoch: 1281, Len of Training loss: 18, Average loss: 2.8194618887371488\n",
      "Len of Validation loss: 54, Average loss: 5.05493759225916\n",
      "Epoch: 1282, Len of Training loss: 18, Average loss: 2.863622135586209\n",
      "Len of Validation loss: 54, Average loss: 5.031436061417615\n",
      "Epoch: 1283, Len of Training loss: 18, Average loss: 2.738110105196635\n",
      "Len of Validation loss: 54, Average loss: 4.404732774805139\n",
      "Epoch: 1284, Len of Training loss: 18, Average loss: 2.6321528355280557\n",
      "Len of Validation loss: 54, Average loss: 4.380749093161689\n",
      "Epoch: 1285, Len of Training loss: 18, Average loss: 2.569838974210951\n",
      "Len of Validation loss: 54, Average loss: 4.302992602189382\n",
      "Epoch: 1286, Len of Training loss: 18, Average loss: 2.3657342990239463\n",
      "Len of Validation loss: 54, Average loss: 4.368454632935701\n",
      "Epoch: 1287, Len of Training loss: 18, Average loss: 2.3622447649637857\n",
      "Len of Validation loss: 54, Average loss: 4.259225962338625\n",
      "Epoch: 1288, Len of Training loss: 18, Average loss: 2.3390532864464655\n",
      "Len of Validation loss: 54, Average loss: 4.246811654832628\n",
      "Epoch: 1289, Len of Training loss: 18, Average loss: 2.2625272274017334\n",
      "Len of Validation loss: 54, Average loss: 4.339597772668909\n",
      "Epoch: 1290, Len of Training loss: 18, Average loss: 2.25498546494378\n",
      "Len of Validation loss: 54, Average loss: 4.2266912261645\n",
      "Epoch: 1291, Len of Training loss: 18, Average loss: 2.1904379261864557\n",
      "Len of Validation loss: 54, Average loss: 4.1501846181021795\n",
      "Epoch: 1292, Len of Training loss: 18, Average loss: 2.154772592915429\n",
      "Len of Validation loss: 54, Average loss: 4.119545960867846\n",
      "Epoch: 1293, Len of Training loss: 18, Average loss: 2.132396856943766\n",
      "Len of Validation loss: 54, Average loss: 4.200257870886061\n",
      "Epoch: 1294, Len of Training loss: 18, Average loss: 2.1245436469713845\n",
      "Len of Validation loss: 54, Average loss: 4.389126126412992\n",
      "Epoch: 1295, Len of Training loss: 18, Average loss: 2.1336616608831616\n",
      "Len of Validation loss: 54, Average loss: 4.288737188886713\n",
      "Epoch: 1296, Len of Training loss: 18, Average loss: 2.1579284270604453\n",
      "Len of Validation loss: 54, Average loss: 4.110267886409053\n",
      "Epoch: 1297, Len of Training loss: 18, Average loss: 2.078781803448995\n",
      "Len of Validation loss: 54, Average loss: 4.358596954080793\n",
      "Epoch: 1298, Len of Training loss: 18, Average loss: 2.0847518973880343\n",
      "Len of Validation loss: 54, Average loss: 3.9625654176429466\n",
      "Epoch: 1299, Len of Training loss: 18, Average loss: 2.0673242608706155\n",
      "Len of Validation loss: 54, Average loss: 4.192275259229872\n",
      "Epoch: 1300, Len of Training loss: 18, Average loss: 2.154745598634084\n",
      "Len of Validation loss: 54, Average loss: 4.4178064531750145\n",
      "Epoch: 1301, Len of Training loss: 18, Average loss: 2.3167239162656994\n",
      "Len of Validation loss: 54, Average loss: 4.330030178582227\n",
      "Epoch: 1302, Len of Training loss: 18, Average loss: 2.4806662930382624\n",
      "Len of Validation loss: 54, Average loss: 4.216679877705044\n",
      "Epoch: 1303, Len of Training loss: 18, Average loss: 2.345358841949039\n",
      "Len of Validation loss: 54, Average loss: 4.203259066299156\n",
      "Epoch: 1304, Len of Training loss: 18, Average loss: 2.1955064866277905\n",
      "Len of Validation loss: 54, Average loss: 4.529166179674643\n",
      "Epoch: 1305, Len of Training loss: 18, Average loss: 2.1927194661564298\n",
      "Len of Validation loss: 54, Average loss: 4.47630332575904\n",
      "Epoch: 1306, Len of Training loss: 18, Average loss: 2.333268496725294\n",
      "Len of Validation loss: 54, Average loss: 4.34699468921732\n",
      "Epoch: 1307, Len of Training loss: 18, Average loss: 2.2414845095740423\n",
      "Len of Validation loss: 54, Average loss: 4.322605830651742\n",
      "Epoch: 1308, Len of Training loss: 18, Average loss: 2.289607518249088\n",
      "Len of Validation loss: 54, Average loss: 4.374027912263517\n",
      "Epoch: 1309, Len of Training loss: 18, Average loss: 2.2418620917532177\n",
      "Len of Validation loss: 54, Average loss: 4.217716773351033\n",
      "Epoch: 1310, Len of Training loss: 18, Average loss: 2.280918538570404\n",
      "Len of Validation loss: 54, Average loss: 4.46614044463193\n",
      "Epoch: 1311, Len of Training loss: 18, Average loss: 2.216704229513804\n",
      "Len of Validation loss: 54, Average loss: 4.145600095943168\n",
      "Epoch: 1312, Len of Training loss: 18, Average loss: 2.205766942765978\n",
      "Len of Validation loss: 54, Average loss: 4.24974278608958\n",
      "Epoch: 1313, Len of Training loss: 18, Average loss: 2.260160254107581\n",
      "Len of Validation loss: 54, Average loss: 4.345297387352696\n",
      "Epoch: 1314, Len of Training loss: 18, Average loss: 2.398123277558221\n",
      "Len of Validation loss: 54, Average loss: 4.233399018093392\n",
      "Epoch: 1315, Len of Training loss: 18, Average loss: 2.3916145828035145\n",
      "Len of Validation loss: 54, Average loss: 4.360219622099841\n",
      "Epoch: 1316, Len of Training loss: 18, Average loss: 2.3476218978563943\n",
      "Len of Validation loss: 54, Average loss: 4.15415393864667\n",
      "Epoch: 1317, Len of Training loss: 18, Average loss: 2.191022793451945\n",
      "Len of Validation loss: 54, Average loss: 4.316481824274416\n",
      "Epoch: 1318, Len of Training loss: 18, Average loss: 2.215933640797933\n",
      "Len of Validation loss: 54, Average loss: 4.274632054346579\n",
      "Epoch: 1319, Len of Training loss: 18, Average loss: 2.3333736724323697\n",
      "Len of Validation loss: 54, Average loss: 4.323540071646373\n",
      "Epoch: 1320, Len of Training loss: 18, Average loss: 2.367871403694153\n",
      "Len of Validation loss: 54, Average loss: 4.679232462688729\n",
      "Epoch: 1321, Len of Training loss: 18, Average loss: 2.425027714835273\n",
      "Len of Validation loss: 54, Average loss: 4.223909808529748\n",
      "Epoch: 1322, Len of Training loss: 18, Average loss: 4.252265254656474\n",
      "Len of Validation loss: 54, Average loss: 13.076047950320774\n",
      "Epoch: 1323, Len of Training loss: 18, Average loss: 14.72636530134413\n",
      "Len of Validation loss: 54, Average loss: 22.29496862270214\n",
      "Epoch: 1324, Len of Training loss: 18, Average loss: 17.05643712149726\n",
      "Len of Validation loss: 54, Average loss: 11.15497675648442\n",
      "Epoch: 1325, Len of Training loss: 18, Average loss: 9.523136960135567\n",
      "Len of Validation loss: 54, Average loss: 8.55324524861795\n",
      "Epoch: 1326, Len of Training loss: 18, Average loss: 7.861435015996297\n",
      "Len of Validation loss: 54, Average loss: 7.436493948653892\n",
      "Epoch: 1327, Len of Training loss: 18, Average loss: 7.529686821831597\n",
      "Len of Validation loss: 54, Average loss: 7.407081149242543\n",
      "Epoch: 1328, Len of Training loss: 18, Average loss: 7.486368629667494\n",
      "Len of Validation loss: 54, Average loss: 7.930773478967172\n",
      "Epoch: 1329, Len of Training loss: 18, Average loss: 7.360621664259169\n",
      "Len of Validation loss: 54, Average loss: 7.22429781931418\n",
      "Epoch: 1330, Len of Training loss: 18, Average loss: 6.923436800638835\n",
      "Len of Validation loss: 54, Average loss: 7.035122178218983\n",
      "Epoch: 1331, Len of Training loss: 18, Average loss: 6.7526322735680475\n",
      "Len of Validation loss: 54, Average loss: 6.873687549873635\n",
      "Epoch: 1332, Len of Training loss: 18, Average loss: 6.745608753628201\n",
      "Len of Validation loss: 54, Average loss: 6.70554417150992\n",
      "Epoch: 1333, Len of Training loss: 18, Average loss: 6.638995859358046\n",
      "Len of Validation loss: 54, Average loss: 7.054567385602881\n",
      "Epoch: 1334, Len of Training loss: 18, Average loss: 6.648168828752306\n",
      "Len of Validation loss: 54, Average loss: 7.2153658822730735\n",
      "Epoch: 1335, Len of Training loss: 18, Average loss: 6.4688412348429365\n",
      "Len of Validation loss: 54, Average loss: 7.420078312909162\n",
      "Epoch: 1336, Len of Training loss: 18, Average loss: 6.574162986543444\n",
      "Len of Validation loss: 54, Average loss: 6.82748007774353\n",
      "Epoch: 1337, Len of Training loss: 18, Average loss: 6.336635377671984\n",
      "Len of Validation loss: 54, Average loss: 6.561617577517474\n",
      "Epoch: 1338, Len of Training loss: 18, Average loss: 6.392742262946235\n",
      "Len of Validation loss: 54, Average loss: 7.000490162107679\n",
      "Epoch: 1339, Len of Training loss: 18, Average loss: 6.371742751863268\n",
      "Len of Validation loss: 54, Average loss: 6.518365727530585\n",
      "Epoch: 1340, Len of Training loss: 18, Average loss: 6.18213415145874\n",
      "Len of Validation loss: 54, Average loss: 6.443003292436953\n",
      "Epoch: 1341, Len of Training loss: 18, Average loss: 6.183880090713501\n",
      "Len of Validation loss: 54, Average loss: 6.99374673543153\n",
      "Epoch: 1342, Len of Training loss: 18, Average loss: 6.081788116031223\n",
      "Len of Validation loss: 54, Average loss: 6.522666785452101\n",
      "Epoch: 1343, Len of Training loss: 18, Average loss: 6.097941901948717\n",
      "Len of Validation loss: 54, Average loss: 6.218813865273087\n",
      "Epoch: 1344, Len of Training loss: 18, Average loss: 5.95960185262892\n",
      "Len of Validation loss: 54, Average loss: 6.457443992296855\n",
      "Epoch: 1345, Len of Training loss: 18, Average loss: 5.847287178039551\n",
      "Len of Validation loss: 54, Average loss: 6.1459346479839745\n",
      "Epoch: 1346, Len of Training loss: 18, Average loss: 5.818505207697551\n",
      "Len of Validation loss: 54, Average loss: 6.241763097268564\n",
      "Epoch: 1347, Len of Training loss: 18, Average loss: 5.778487152523464\n",
      "Len of Validation loss: 54, Average loss: 6.99537620279524\n",
      "Epoch: 1348, Len of Training loss: 18, Average loss: 5.818263212839763\n",
      "Len of Validation loss: 54, Average loss: 6.286278252248411\n",
      "Epoch: 1349, Len of Training loss: 18, Average loss: 5.61257717344496\n",
      "Len of Validation loss: 54, Average loss: 6.31584957131633\n",
      "Epoch: 1350, Len of Training loss: 18, Average loss: 5.861055824491713\n",
      "Len of Validation loss: 54, Average loss: 6.396948624540259\n",
      "Epoch: 1351, Len of Training loss: 18, Average loss: 5.979699717627631\n",
      "Len of Validation loss: 54, Average loss: 6.546120970337479\n",
      "Epoch: 1352, Len of Training loss: 18, Average loss: 5.806276321411133\n",
      "Len of Validation loss: 54, Average loss: 6.327254193800467\n",
      "Epoch: 1353, Len of Training loss: 18, Average loss: 5.524838156170315\n",
      "Len of Validation loss: 54, Average loss: 6.006414179448728\n",
      "Epoch: 1354, Len of Training loss: 18, Average loss: 5.426380740271674\n",
      "Len of Validation loss: 54, Average loss: 5.938581095801459\n",
      "Epoch: 1355, Len of Training loss: 18, Average loss: 5.4388927883572045\n",
      "Len of Validation loss: 54, Average loss: 6.039855694329297\n",
      "Epoch: 1356, Len of Training loss: 18, Average loss: 5.474324676725599\n",
      "Len of Validation loss: 54, Average loss: 5.9725514341283725\n",
      "Epoch: 1357, Len of Training loss: 18, Average loss: 5.666198492050171\n",
      "Len of Validation loss: 54, Average loss: 6.064235510649504\n",
      "Epoch: 1358, Len of Training loss: 18, Average loss: 5.6797910001542835\n",
      "Len of Validation loss: 54, Average loss: 6.385333374694541\n",
      "Epoch: 1359, Len of Training loss: 18, Average loss: 5.445718169212341\n",
      "Len of Validation loss: 54, Average loss: 5.970659922670435\n",
      "Epoch: 1360, Len of Training loss: 18, Average loss: 5.050061437818739\n",
      "Len of Validation loss: 54, Average loss: 5.730422567438196\n",
      "Epoch: 1361, Len of Training loss: 18, Average loss: 5.030389229456584\n",
      "Len of Validation loss: 54, Average loss: 5.808019942707485\n",
      "Epoch: 1362, Len of Training loss: 18, Average loss: 5.106044583850437\n",
      "Len of Validation loss: 54, Average loss: 6.038789391517639\n",
      "Epoch: 1363, Len of Training loss: 18, Average loss: 5.046712146864997\n",
      "Len of Validation loss: 54, Average loss: 6.1733620122626975\n",
      "Epoch: 1364, Len of Training loss: 18, Average loss: 5.0858161846796675\n",
      "Len of Validation loss: 54, Average loss: 5.602103820553532\n",
      "Epoch: 1365, Len of Training loss: 18, Average loss: 4.7176213132010565\n",
      "Len of Validation loss: 54, Average loss: 5.456106141761497\n",
      "Epoch: 1366, Len of Training loss: 18, Average loss: 4.770965231789483\n",
      "Len of Validation loss: 54, Average loss: 5.694752622533728\n",
      "Epoch: 1367, Len of Training loss: 18, Average loss: 4.6202561193042335\n",
      "Len of Validation loss: 54, Average loss: 5.555231694821958\n",
      "Epoch: 1368, Len of Training loss: 18, Average loss: 4.541651474104987\n",
      "Len of Validation loss: 54, Average loss: 5.620637562539843\n",
      "Epoch: 1369, Len of Training loss: 18, Average loss: 4.512791395187378\n",
      "Len of Validation loss: 54, Average loss: 5.483799740120217\n",
      "Epoch: 1370, Len of Training loss: 18, Average loss: 4.590449240472582\n",
      "Len of Validation loss: 54, Average loss: 5.633122457398309\n",
      "Epoch: 1371, Len of Training loss: 18, Average loss: 4.406188885370891\n",
      "Len of Validation loss: 54, Average loss: 5.555363827281528\n",
      "Epoch: 1372, Len of Training loss: 18, Average loss: 4.401697423723009\n",
      "Len of Validation loss: 54, Average loss: 5.204614135954115\n",
      "Epoch: 1373, Len of Training loss: 18, Average loss: 4.270167337523566\n",
      "Len of Validation loss: 54, Average loss: 5.167679512942279\n",
      "Epoch: 1374, Len of Training loss: 18, Average loss: 4.25222192870246\n",
      "Len of Validation loss: 54, Average loss: 5.317683180173238\n",
      "Epoch: 1375, Len of Training loss: 18, Average loss: 4.13830214076572\n",
      "Len of Validation loss: 54, Average loss: 5.328709491976985\n",
      "Epoch: 1376, Len of Training loss: 18, Average loss: 4.323733541700575\n",
      "Len of Validation loss: 54, Average loss: 5.540721593079744\n",
      "Epoch: 1377, Len of Training loss: 18, Average loss: 4.32301844490899\n",
      "Len of Validation loss: 54, Average loss: 5.31834218678651\n",
      "Epoch: 1378, Len of Training loss: 18, Average loss: 4.246812502543132\n",
      "Len of Validation loss: 54, Average loss: 5.267983237902324\n",
      "Epoch: 1379, Len of Training loss: 18, Average loss: 4.186715112792121\n",
      "Len of Validation loss: 54, Average loss: 5.353205248161599\n",
      "Epoch: 1380, Len of Training loss: 18, Average loss: 4.053348011440701\n",
      "Len of Validation loss: 54, Average loss: 5.175769214276914\n",
      "Epoch: 1381, Len of Training loss: 18, Average loss: 3.9446437093946667\n",
      "Len of Validation loss: 54, Average loss: 5.212943386148523\n",
      "Epoch: 1382, Len of Training loss: 18, Average loss: 3.9754943582746716\n",
      "Len of Validation loss: 54, Average loss: 5.253000153435601\n",
      "Epoch: 1383, Len of Training loss: 18, Average loss: 3.8849323987960815\n",
      "Len of Validation loss: 54, Average loss: 5.109344270494249\n",
      "Epoch: 1384, Len of Training loss: 18, Average loss: 3.8745633231268988\n",
      "Len of Validation loss: 54, Average loss: 4.990813966150637\n",
      "Epoch: 1385, Len of Training loss: 18, Average loss: 3.7991316980785794\n",
      "Len of Validation loss: 54, Average loss: 4.962415465602168\n",
      "Epoch: 1386, Len of Training loss: 18, Average loss: 3.795483152071635\n",
      "Len of Validation loss: 54, Average loss: 4.845655763590777\n",
      "Epoch: 1387, Len of Training loss: 18, Average loss: 3.6983630657196045\n",
      "Len of Validation loss: 54, Average loss: 5.1307291543042215\n",
      "Epoch: 1388, Len of Training loss: 18, Average loss: 3.6785050100750394\n",
      "Len of Validation loss: 54, Average loss: 4.92759832629451\n",
      "Epoch: 1389, Len of Training loss: 18, Average loss: 3.7051888042026095\n",
      "Len of Validation loss: 54, Average loss: 4.972392914471803\n",
      "Epoch: 1390, Len of Training loss: 18, Average loss: 3.6523130469852023\n",
      "Len of Validation loss: 54, Average loss: 5.083653783356702\n",
      "Epoch: 1391, Len of Training loss: 18, Average loss: 3.6814582877688937\n",
      "Len of Validation loss: 54, Average loss: 4.97549600954409\n",
      "Epoch: 1392, Len of Training loss: 18, Average loss: 3.603964156574673\n",
      "Len of Validation loss: 54, Average loss: 4.963034095587553\n",
      "Epoch: 1393, Len of Training loss: 18, Average loss: 3.5941065549850464\n",
      "Len of Validation loss: 54, Average loss: 5.031998475392659\n",
      "Epoch: 1394, Len of Training loss: 18, Average loss: 3.6467543178134494\n",
      "Len of Validation loss: 54, Average loss: 4.906756595329002\n",
      "Epoch: 1395, Len of Training loss: 18, Average loss: 3.6414356099234686\n",
      "Len of Validation loss: 54, Average loss: 5.137286404768626\n",
      "Epoch: 1396, Len of Training loss: 18, Average loss: 3.5837926467259726\n",
      "Len of Validation loss: 54, Average loss: 5.002361469798618\n",
      "Epoch: 1397, Len of Training loss: 18, Average loss: 3.4459235535727606\n",
      "Len of Validation loss: 54, Average loss: 4.835565876077722\n",
      "Epoch: 1398, Len of Training loss: 18, Average loss: 3.464735839102003\n",
      "Len of Validation loss: 54, Average loss: 4.84227251123499\n",
      "Epoch: 1399, Len of Training loss: 18, Average loss: 3.4885571135414972\n",
      "Len of Validation loss: 54, Average loss: 4.806171600465421\n",
      "Epoch: 1400, Len of Training loss: 18, Average loss: 3.372340374522739\n",
      "Len of Validation loss: 54, Average loss: 4.741815147576509\n",
      "Epoch: 1401, Len of Training loss: 18, Average loss: 3.397148913807339\n",
      "Len of Validation loss: 54, Average loss: 4.840265276255431\n",
      "Epoch: 1402, Len of Training loss: 18, Average loss: 3.3679993947347007\n",
      "Len of Validation loss: 54, Average loss: 4.869540358031237\n",
      "Epoch: 1403, Len of Training loss: 18, Average loss: 3.320666035016378\n",
      "Len of Validation loss: 54, Average loss: 4.698177600348437\n",
      "Epoch: 1404, Len of Training loss: 18, Average loss: 3.268198980225457\n",
      "Len of Validation loss: 54, Average loss: 4.732021064669998\n",
      "Epoch: 1405, Len of Training loss: 18, Average loss: 3.3061827023824057\n",
      "Len of Validation loss: 54, Average loss: 4.8956409052566245\n",
      "Epoch: 1406, Len of Training loss: 18, Average loss: 3.264876921971639\n",
      "Len of Validation loss: 54, Average loss: 4.91950426057533\n",
      "Epoch: 1407, Len of Training loss: 18, Average loss: 3.6033962435192533\n",
      "Len of Validation loss: 54, Average loss: 4.766961578969602\n",
      "Epoch: 1408, Len of Training loss: 18, Average loss: 3.9642849233415394\n",
      "Len of Validation loss: 54, Average loss: 4.927716515682362\n",
      "Epoch: 1409, Len of Training loss: 18, Average loss: 3.9463693963156805\n",
      "Len of Validation loss: 54, Average loss: 4.838636332088047\n",
      "Epoch: 1410, Len of Training loss: 18, Average loss: 3.9392113818062677\n",
      "Len of Validation loss: 54, Average loss: 4.938133365578121\n",
      "Epoch: 1411, Len of Training loss: 18, Average loss: 3.53227874967787\n",
      "Len of Validation loss: 54, Average loss: 4.578332832566014\n",
      "Epoch: 1412, Len of Training loss: 18, Average loss: 3.5360888375176325\n",
      "Len of Validation loss: 54, Average loss: 5.0710827597865356\n",
      "Epoch: 1413, Len of Training loss: 18, Average loss: 3.3525978724161782\n",
      "Len of Validation loss: 54, Average loss: 4.801897521372195\n",
      "Epoch: 1414, Len of Training loss: 18, Average loss: 3.413943065537347\n",
      "Len of Validation loss: 54, Average loss: 4.830375618404812\n",
      "Epoch: 1415, Len of Training loss: 18, Average loss: 3.494656695259942\n",
      "Len of Validation loss: 54, Average loss: 5.188962124012135\n",
      "Epoch: 1416, Len of Training loss: 18, Average loss: 3.424975355466207\n",
      "Len of Validation loss: 54, Average loss: 5.2340526183446245\n",
      "Epoch: 1417, Len of Training loss: 18, Average loss: 3.5200825532277427\n",
      "Len of Validation loss: 54, Average loss: 4.726817989790881\n",
      "Epoch: 1418, Len of Training loss: 18, Average loss: 3.429192860921224\n",
      "Len of Validation loss: 54, Average loss: 5.157230988696769\n",
      "Epoch: 1419, Len of Training loss: 18, Average loss: 3.521935820579529\n",
      "Len of Validation loss: 54, Average loss: 4.76477234893375\n",
      "Epoch: 1420, Len of Training loss: 18, Average loss: 3.4183340999815197\n",
      "Len of Validation loss: 54, Average loss: 4.758534943615949\n",
      "Epoch: 1421, Len of Training loss: 18, Average loss: 3.2305863433414035\n",
      "Len of Validation loss: 54, Average loss: 4.681075566344791\n",
      "Epoch: 1422, Len of Training loss: 18, Average loss: 3.1038048002454968\n",
      "Len of Validation loss: 54, Average loss: 4.595171782705519\n",
      "Epoch: 1423, Len of Training loss: 18, Average loss: 3.0248922771877713\n",
      "Len of Validation loss: 54, Average loss: 4.537835386064318\n",
      "Epoch: 1424, Len of Training loss: 18, Average loss: 3.0117047097947864\n",
      "Len of Validation loss: 54, Average loss: 4.47645585404502\n",
      "Epoch: 1425, Len of Training loss: 18, Average loss: 2.9459980196423\n",
      "Len of Validation loss: 54, Average loss: 4.593839910295275\n",
      "Epoch: 1426, Len of Training loss: 18, Average loss: 3.01620344320933\n",
      "Len of Validation loss: 54, Average loss: 4.605085752628468\n",
      "Epoch: 1427, Len of Training loss: 18, Average loss: 2.935418685277303\n",
      "Len of Validation loss: 54, Average loss: 4.632289617149918\n",
      "Epoch: 1428, Len of Training loss: 18, Average loss: 2.9772701130972967\n",
      "Len of Validation loss: 54, Average loss: 4.477298672552462\n",
      "Epoch: 1429, Len of Training loss: 18, Average loss: 2.901007056236267\n",
      "Len of Validation loss: 54, Average loss: 4.496441790351161\n",
      "Epoch: 1430, Len of Training loss: 18, Average loss: 2.9216100374857583\n",
      "Len of Validation loss: 54, Average loss: 4.50581592100638\n",
      "Epoch: 1431, Len of Training loss: 18, Average loss: 3.2878184848361545\n",
      "Len of Validation loss: 54, Average loss: 4.656577611411059\n",
      "Epoch: 1432, Len of Training loss: 18, Average loss: 3.0881750848558216\n",
      "Len of Validation loss: 54, Average loss: 4.569163203239441\n",
      "Epoch: 1433, Len of Training loss: 18, Average loss: 2.903444078233507\n",
      "Len of Validation loss: 54, Average loss: 4.61177181314539\n",
      "Epoch: 1434, Len of Training loss: 18, Average loss: 2.8751925362481012\n",
      "Len of Validation loss: 54, Average loss: 4.679057708492985\n",
      "Epoch: 1435, Len of Training loss: 18, Average loss: 2.8509332603878446\n",
      "Len of Validation loss: 54, Average loss: 4.435441776558205\n",
      "Epoch: 1436, Len of Training loss: 18, Average loss: 2.8250833484861584\n",
      "Len of Validation loss: 54, Average loss: 4.348097079330021\n",
      "Epoch: 1437, Len of Training loss: 18, Average loss: 2.828581068250868\n",
      "Len of Validation loss: 54, Average loss: 4.623933650829174\n",
      "Epoch: 1438, Len of Training loss: 18, Average loss: 2.8724975056118436\n",
      "Len of Validation loss: 54, Average loss: 4.4506580123194945\n",
      "Epoch: 1439, Len of Training loss: 18, Average loss: 2.8211732572979398\n",
      "Len of Validation loss: 54, Average loss: 4.483099586433834\n",
      "Epoch: 1440, Len of Training loss: 18, Average loss: 2.7282870875464544\n",
      "Len of Validation loss: 54, Average loss: 4.3443351145143865\n",
      "Epoch: 1441, Len of Training loss: 18, Average loss: 2.7190139293670654\n",
      "Len of Validation loss: 54, Average loss: 4.2535032696194115\n",
      "Epoch: 1442, Len of Training loss: 18, Average loss: 2.7264899015426636\n",
      "Len of Validation loss: 54, Average loss: 4.405609380315851\n",
      "Epoch: 1443, Len of Training loss: 18, Average loss: 2.753908157348633\n",
      "Len of Validation loss: 54, Average loss: 4.6104343776349666\n",
      "Epoch: 1444, Len of Training loss: 18, Average loss: 3.218638261159261\n",
      "Len of Validation loss: 54, Average loss: 4.791108533188149\n",
      "Epoch: 1445, Len of Training loss: 18, Average loss: 3.0336437357796564\n",
      "Len of Validation loss: 54, Average loss: 4.656339879389162\n",
      "Epoch: 1446, Len of Training loss: 18, Average loss: 2.916370431582133\n",
      "Len of Validation loss: 54, Average loss: 4.522768183990761\n",
      "Epoch: 1447, Len of Training loss: 18, Average loss: 2.815382546848721\n",
      "Len of Validation loss: 54, Average loss: 4.4298524260520935\n",
      "Epoch: 1448, Len of Training loss: 18, Average loss: 2.824496547381083\n",
      "Len of Validation loss: 54, Average loss: 4.3741955227322045\n",
      "Epoch: 1449, Len of Training loss: 18, Average loss: 2.88923270172543\n",
      "Len of Validation loss: 54, Average loss: 4.9251118346496865\n",
      "Epoch: 1450, Len of Training loss: 18, Average loss: 3.156338506274753\n",
      "Len of Validation loss: 54, Average loss: 4.36244754658805\n",
      "Epoch: 1451, Len of Training loss: 18, Average loss: 3.292049116558499\n",
      "Len of Validation loss: 54, Average loss: 4.553170610357214\n",
      "Epoch: 1452, Len of Training loss: 18, Average loss: 3.119759758313497\n",
      "Len of Validation loss: 54, Average loss: 4.93524905928859\n",
      "Epoch: 1453, Len of Training loss: 18, Average loss: 3.166058474116855\n",
      "Len of Validation loss: 54, Average loss: 4.758271740542518\n",
      "Epoch: 1454, Len of Training loss: 18, Average loss: 2.9602985779444375\n",
      "Len of Validation loss: 54, Average loss: 4.349333745461923\n",
      "Epoch: 1455, Len of Training loss: 18, Average loss: 2.7583891418245106\n",
      "Len of Validation loss: 54, Average loss: 4.432806101110247\n",
      "Epoch: 1456, Len of Training loss: 18, Average loss: 2.6426572799682617\n",
      "Len of Validation loss: 54, Average loss: 4.369033365337937\n",
      "Epoch: 1457, Len of Training loss: 18, Average loss: 2.6675507757398815\n",
      "Len of Validation loss: 54, Average loss: 4.105276575794926\n",
      "Epoch: 1458, Len of Training loss: 18, Average loss: 2.607249048021105\n",
      "Len of Validation loss: 54, Average loss: 4.467780205938551\n",
      "Epoch: 1459, Len of Training loss: 18, Average loss: 2.613783876101176\n",
      "Len of Validation loss: 54, Average loss: 4.21786453768059\n",
      "Epoch: 1460, Len of Training loss: 18, Average loss: 2.515228033065796\n",
      "Len of Validation loss: 54, Average loss: 4.1707274759257285\n",
      "Epoch: 1461, Len of Training loss: 18, Average loss: 2.520943284034729\n",
      "Len of Validation loss: 54, Average loss: 4.2630836897426185\n",
      "Epoch: 1462, Len of Training loss: 18, Average loss: 2.6759022606743708\n",
      "Len of Validation loss: 54, Average loss: 4.444171561135186\n",
      "Epoch: 1463, Len of Training loss: 18, Average loss: 2.581417997678121\n",
      "Len of Validation loss: 54, Average loss: 4.246343182192908\n",
      "Epoch: 1464, Len of Training loss: 18, Average loss: 2.561857501665751\n",
      "Len of Validation loss: 54, Average loss: 4.243369221687317\n",
      "Epoch: 1465, Len of Training loss: 18, Average loss: 2.5186591148376465\n",
      "Len of Validation loss: 54, Average loss: 4.1712438420013145\n",
      "Epoch: 1466, Len of Training loss: 18, Average loss: 2.4891537295447455\n",
      "Len of Validation loss: 54, Average loss: 4.345121507291441\n",
      "Epoch: 1467, Len of Training loss: 18, Average loss: 2.4966016080644398\n",
      "Len of Validation loss: 54, Average loss: 4.343160465911582\n",
      "Epoch: 1468, Len of Training loss: 18, Average loss: 2.539373755455017\n",
      "Len of Validation loss: 54, Average loss: 4.355844755967458\n",
      "Epoch: 1469, Len of Training loss: 18, Average loss: 2.51268204053243\n",
      "Len of Validation loss: 54, Average loss: 4.18286003669103\n",
      "Epoch: 1470, Len of Training loss: 18, Average loss: 2.528640945752462\n",
      "Len of Validation loss: 54, Average loss: 4.314446553036019\n",
      "Epoch: 1471, Len of Training loss: 18, Average loss: 2.654473662376404\n",
      "Len of Validation loss: 54, Average loss: 4.147931765626978\n",
      "Epoch: 1472, Len of Training loss: 18, Average loss: 2.5874497493108115\n",
      "Len of Validation loss: 54, Average loss: 4.408525528731169\n",
      "Epoch: 1473, Len of Training loss: 18, Average loss: 2.6001218954722085\n",
      "Len of Validation loss: 54, Average loss: 4.152371104116793\n",
      "Epoch: 1474, Len of Training loss: 18, Average loss: 2.6010703113343983\n",
      "Len of Validation loss: 54, Average loss: 4.319646117863832\n",
      "Epoch: 1475, Len of Training loss: 18, Average loss: 2.4943077829149036\n",
      "Len of Validation loss: 54, Average loss: 4.544491043797246\n",
      "Epoch: 1476, Len of Training loss: 18, Average loss: 2.5429843929078846\n",
      "Len of Validation loss: 54, Average loss: 4.201489799552494\n",
      "Epoch: 1477, Len of Training loss: 18, Average loss: 2.4373639159732394\n",
      "Len of Validation loss: 54, Average loss: 4.371143934903322\n",
      "Epoch: 1478, Len of Training loss: 18, Average loss: 2.5339751773410373\n",
      "Len of Validation loss: 54, Average loss: 4.195539790171164\n",
      "Epoch: 1479, Len of Training loss: 18, Average loss: 2.5202876461876764\n",
      "Len of Validation loss: 54, Average loss: 4.370375249120924\n",
      "Epoch: 1480, Len of Training loss: 18, Average loss: 2.543065561188592\n",
      "Len of Validation loss: 54, Average loss: 4.223002352096416\n",
      "Epoch: 1481, Len of Training loss: 18, Average loss: 2.6172428925832114\n",
      "Len of Validation loss: 54, Average loss: 4.209718097139288\n",
      "Epoch: 1482, Len of Training loss: 18, Average loss: 3.024843176205953\n",
      "Len of Validation loss: 54, Average loss: 4.513211296664344\n",
      "Epoch: 1483, Len of Training loss: 18, Average loss: 2.815234515402052\n",
      "Len of Validation loss: 54, Average loss: 4.483669640841307\n",
      "Epoch: 1484, Len of Training loss: 18, Average loss: 2.6889838907453747\n",
      "Len of Validation loss: 54, Average loss: 4.238289179625334\n",
      "Epoch: 1485, Len of Training loss: 18, Average loss: 2.6735364066229925\n",
      "Len of Validation loss: 54, Average loss: 4.321767471454762\n",
      "Epoch: 1486, Len of Training loss: 18, Average loss: 2.5656848351160684\n",
      "Len of Validation loss: 54, Average loss: 4.396432457146822\n",
      "Epoch: 1487, Len of Training loss: 18, Average loss: 2.495542460017734\n",
      "Len of Validation loss: 54, Average loss: 4.0813081485253795\n",
      "Epoch: 1488, Len of Training loss: 18, Average loss: 2.3447839154137506\n",
      "Len of Validation loss: 54, Average loss: 4.230359179002267\n",
      "Epoch: 1489, Len of Training loss: 18, Average loss: 2.3050722082455954\n",
      "Len of Validation loss: 54, Average loss: 4.1484048167864485\n",
      "Epoch: 1490, Len of Training loss: 18, Average loss: 2.408726361062792\n",
      "Len of Validation loss: 54, Average loss: 4.322836456475435\n",
      "Epoch: 1491, Len of Training loss: 18, Average loss: 2.384897814856635\n",
      "Len of Validation loss: 54, Average loss: 4.1846122432638095\n",
      "Epoch: 1492, Len of Training loss: 18, Average loss: 2.291271766026815\n",
      "Len of Validation loss: 54, Average loss: 4.161263527693571\n",
      "Epoch: 1493, Len of Training loss: 18, Average loss: 2.346442792150709\n",
      "Len of Validation loss: 54, Average loss: 4.1209709269029124\n",
      "Epoch: 1494, Len of Training loss: 18, Average loss: 2.4557047155168323\n",
      "Len of Validation loss: 54, Average loss: 4.413446483788667\n",
      "Epoch: 1495, Len of Training loss: 18, Average loss: 3.149593962563409\n",
      "Len of Validation loss: 54, Average loss: 4.810454425988374\n",
      "Epoch: 1496, Len of Training loss: 18, Average loss: 2.758679840299818\n",
      "Len of Validation loss: 54, Average loss: 4.482335148034273\n",
      "Epoch: 1497, Len of Training loss: 18, Average loss: 2.540264950858222\n",
      "Len of Validation loss: 54, Average loss: 4.373616410626306\n",
      "Epoch: 1498, Len of Training loss: 18, Average loss: 2.4544550577799478\n",
      "Len of Validation loss: 54, Average loss: 4.189802821035738\n",
      "Epoch: 1499, Len of Training loss: 18, Average loss: 2.443689227104187\n",
      "Len of Validation loss: 54, Average loss: 4.288625798843525\n",
      "Epoch: 1500, Len of Training loss: 18, Average loss: 2.376627802848816\n",
      "Len of Validation loss: 54, Average loss: 4.345989523110567\n",
      "Epoch: 1501, Len of Training loss: 18, Average loss: 2.6198052565256753\n",
      "Len of Validation loss: 54, Average loss: 4.5697308425550105\n",
      "Epoch: 1502, Len of Training loss: 18, Average loss: 2.708709028032091\n",
      "Len of Validation loss: 54, Average loss: 4.300715159486841\n",
      "Epoch: 1503, Len of Training loss: 18, Average loss: 2.5774165789286294\n",
      "Len of Validation loss: 54, Average loss: 4.159488704469469\n",
      "Epoch: 1504, Len of Training loss: 18, Average loss: 2.4154092735714383\n",
      "Len of Validation loss: 54, Average loss: 4.34013068455237\n",
      "Epoch: 1505, Len of Training loss: 18, Average loss: 2.453500363561842\n",
      "Len of Validation loss: 54, Average loss: 4.229851577017042\n",
      "Epoch: 1506, Len of Training loss: 18, Average loss: 2.410955296622382\n",
      "Len of Validation loss: 54, Average loss: 4.1370527633914245\n",
      "Epoch: 1507, Len of Training loss: 18, Average loss: 2.3976809448666043\n",
      "Len of Validation loss: 54, Average loss: 4.26065460840861\n",
      "Epoch: 1508, Len of Training loss: 18, Average loss: 2.2857541773054333\n",
      "Len of Validation loss: 54, Average loss: 4.104333235157861\n",
      "Epoch: 1509, Len of Training loss: 18, Average loss: 2.2616404030058117\n",
      "Len of Validation loss: 54, Average loss: 4.031612416108449\n",
      "Epoch: 1510, Len of Training loss: 18, Average loss: 2.382589783933428\n",
      "Len of Validation loss: 54, Average loss: 4.112820459736718\n",
      "Epoch: 1511, Len of Training loss: 18, Average loss: 2.4487542708714805\n",
      "Len of Validation loss: 54, Average loss: 4.516856359110938\n",
      "Epoch: 1512, Len of Training loss: 18, Average loss: 2.4681415955225625\n",
      "Len of Validation loss: 54, Average loss: 4.236212227079603\n",
      "Epoch: 1513, Len of Training loss: 18, Average loss: 2.633986552556356\n",
      "Len of Validation loss: 54, Average loss: 4.548916584915585\n",
      "Epoch: 1514, Len of Training loss: 18, Average loss: 2.471266984939575\n",
      "Len of Validation loss: 54, Average loss: 4.337072460739701\n",
      "Epoch: 1515, Len of Training loss: 18, Average loss: 2.553694314426846\n",
      "Len of Validation loss: 54, Average loss: 4.237966959123258\n",
      "Epoch: 1516, Len of Training loss: 18, Average loss: 2.403063337008158\n",
      "Len of Validation loss: 54, Average loss: 3.9766920275158353\n",
      "Epoch: 1517, Len of Training loss: 18, Average loss: 2.3502418531311884\n",
      "Len of Validation loss: 54, Average loss: 4.391530217947783\n",
      "Epoch: 1518, Len of Training loss: 18, Average loss: 2.2760037978490195\n",
      "Len of Validation loss: 54, Average loss: 4.1177768994260715\n",
      "Epoch: 1519, Len of Training loss: 18, Average loss: 2.297682417763604\n",
      "Len of Validation loss: 54, Average loss: 4.064554271874605\n",
      "Epoch: 1520, Len of Training loss: 18, Average loss: 2.3022074699401855\n",
      "Len of Validation loss: 54, Average loss: 4.041337097132647\n",
      "Epoch: 1521, Len of Training loss: 18, Average loss: 2.366271694501241\n",
      "Len of Validation loss: 54, Average loss: 4.020393084596704\n",
      "Epoch: 1522, Len of Training loss: 18, Average loss: 2.3838684029049344\n",
      "Len of Validation loss: 54, Average loss: 4.310177081161076\n",
      "Epoch: 1523, Len of Training loss: 18, Average loss: 2.4889969560835095\n",
      "Len of Validation loss: 54, Average loss: 4.175250720094751\n",
      "Epoch: 1524, Len of Training loss: 18, Average loss: 2.390154686239031\n",
      "Len of Validation loss: 54, Average loss: 4.364249304488853\n",
      "Epoch: 1525, Len of Training loss: 18, Average loss: 2.293053229649862\n",
      "Len of Validation loss: 54, Average loss: 4.022214949131012\n",
      "Epoch: 1526, Len of Training loss: 18, Average loss: 2.26681289407942\n",
      "Len of Validation loss: 54, Average loss: 3.8958840149420277\n",
      "Epoch: 1527, Len of Training loss: 18, Average loss: 2.2762751380602517\n",
      "Len of Validation loss: 54, Average loss: 4.009373859122947\n",
      "Epoch: 1528, Len of Training loss: 18, Average loss: 2.336864822440677\n",
      "Len of Validation loss: 54, Average loss: 4.127914510391377\n",
      "Epoch: 1529, Len of Training loss: 18, Average loss: 2.3270055452982583\n",
      "Len of Validation loss: 54, Average loss: 4.209637149616524\n",
      "Epoch: 1530, Len of Training loss: 18, Average loss: 2.2498551739586725\n",
      "Len of Validation loss: 54, Average loss: 3.938495538852833\n",
      "Epoch: 1531, Len of Training loss: 18, Average loss: 2.2540694342719183\n",
      "Len of Validation loss: 54, Average loss: 4.199698461426629\n",
      "Epoch: 1532, Len of Training loss: 18, Average loss: 2.238030036290487\n",
      "Len of Validation loss: 54, Average loss: 4.173373869171849\n",
      "Epoch: 1533, Len of Training loss: 18, Average loss: 2.36544402440389\n",
      "Len of Validation loss: 54, Average loss: 4.232194026311238\n",
      "Epoch: 1534, Len of Training loss: 18, Average loss: 2.4080569744110107\n",
      "Len of Validation loss: 54, Average loss: 4.233668234613207\n",
      "Epoch: 1535, Len of Training loss: 18, Average loss: 2.3944950103759766\n",
      "Len of Validation loss: 54, Average loss: 4.506737773065214\n",
      "Epoch: 1536, Len of Training loss: 18, Average loss: 2.4015791018803916\n",
      "Len of Validation loss: 54, Average loss: 4.167048732439677\n",
      "Epoch: 1537, Len of Training loss: 18, Average loss: 2.247012972831726\n",
      "Len of Validation loss: 54, Average loss: 4.147900941195311\n",
      "Epoch: 1538, Len of Training loss: 18, Average loss: 2.250574482811822\n",
      "Len of Validation loss: 54, Average loss: 4.04917675477487\n",
      "Epoch: 1539, Len of Training loss: 18, Average loss: 2.2430059578683643\n",
      "Len of Validation loss: 54, Average loss: 4.090774401470467\n",
      "Epoch: 1540, Len of Training loss: 18, Average loss: 2.250936441951328\n",
      "Len of Validation loss: 54, Average loss: 4.402613291033992\n",
      "Epoch: 1541, Len of Training loss: 18, Average loss: 2.3614838653140597\n",
      "Len of Validation loss: 54, Average loss: 4.140795431755207\n",
      "Epoch: 1542, Len of Training loss: 18, Average loss: 2.3730075359344482\n",
      "Len of Validation loss: 54, Average loss: 4.3445520003636675\n",
      "Epoch: 1543, Len of Training loss: 18, Average loss: 2.3315439687834845\n",
      "Len of Validation loss: 54, Average loss: 4.274814559353723\n",
      "Epoch: 1544, Len of Training loss: 18, Average loss: 2.3173077238930597\n",
      "Len of Validation loss: 54, Average loss: 4.296856655014886\n",
      "Epoch: 1545, Len of Training loss: 18, Average loss: 2.207271224922604\n",
      "Len of Validation loss: 54, Average loss: 4.291044846728996\n",
      "Epoch: 1546, Len of Training loss: 18, Average loss: 2.346194240781996\n",
      "Len of Validation loss: 54, Average loss: 4.111156353244075\n",
      "Epoch: 1547, Len of Training loss: 18, Average loss: 2.342688818772634\n",
      "Len of Validation loss: 54, Average loss: 4.060438394546509\n",
      "Epoch: 1548, Len of Training loss: 18, Average loss: 2.308765596813626\n",
      "Len of Validation loss: 54, Average loss: 4.2306077811453076\n",
      "Epoch: 1549, Len of Training loss: 18, Average loss: 2.1497098671065435\n",
      "Len of Validation loss: 54, Average loss: 4.297602445990951\n",
      "Epoch: 1550, Len of Training loss: 18, Average loss: 2.1231495009528265\n",
      "Len of Validation loss: 54, Average loss: 4.133363661942659\n",
      "Epoch: 1551, Len of Training loss: 18, Average loss: 2.114898681640625\n",
      "Len of Validation loss: 54, Average loss: 3.949294403747276\n",
      "Epoch: 1552, Len of Training loss: 18, Average loss: 2.085141056113773\n",
      "Len of Validation loss: 54, Average loss: 4.0569211001749395\n",
      "Epoch: 1553, Len of Training loss: 18, Average loss: 2.232596649063958\n",
      "Len of Validation loss: 54, Average loss: 3.9714164623507746\n",
      "Epoch: 1554, Len of Training loss: 18, Average loss: 2.0744380354881287\n",
      "Len of Validation loss: 54, Average loss: 4.033169501357609\n",
      "Epoch: 1555, Len of Training loss: 18, Average loss: 1.9876900182829962\n",
      "Len of Validation loss: 54, Average loss: 4.115805829012835\n",
      "Epoch: 1556, Len of Training loss: 18, Average loss: 2.0430943104955883\n",
      "Len of Validation loss: 54, Average loss: 4.148513593055584\n",
      "Epoch: 1557, Len of Training loss: 18, Average loss: 2.446607682440016\n",
      "Len of Validation loss: 54, Average loss: 4.621069967746735\n",
      "Epoch: 1558, Len of Training loss: 18, Average loss: 2.84938637415568\n",
      "Len of Validation loss: 54, Average loss: 4.428853321958472\n",
      "Epoch: 1559, Len of Training loss: 18, Average loss: 3.2598819865120783\n",
      "Len of Validation loss: 54, Average loss: 4.888980649135731\n",
      "Epoch: 1560, Len of Training loss: 18, Average loss: 2.9376058181126914\n",
      "Len of Validation loss: 54, Average loss: 4.715687862148991\n",
      "Epoch: 1561, Len of Training loss: 18, Average loss: 2.580296966764662\n",
      "Len of Validation loss: 54, Average loss: 3.9994821217325\n",
      "Epoch: 1562, Len of Training loss: 18, Average loss: 2.4285199642181396\n",
      "Len of Validation loss: 54, Average loss: 4.08484912360156\n",
      "Epoch: 1563, Len of Training loss: 18, Average loss: 2.2906145784589977\n",
      "Len of Validation loss: 54, Average loss: 4.208324299918281\n",
      "Epoch: 1564, Len of Training loss: 18, Average loss: 2.2671930458810596\n",
      "Len of Validation loss: 54, Average loss: 4.172267765910537\n",
      "Epoch: 1565, Len of Training loss: 18, Average loss: 2.346624573071798\n",
      "Len of Validation loss: 54, Average loss: 4.109307509881479\n",
      "Epoch: 1566, Len of Training loss: 18, Average loss: 2.2187831335597568\n",
      "Len of Validation loss: 54, Average loss: 4.134694015538251\n",
      "Epoch: 1567, Len of Training loss: 18, Average loss: 2.2167145874765186\n",
      "Len of Validation loss: 54, Average loss: 4.355853661342904\n",
      "Epoch: 1568, Len of Training loss: 18, Average loss: 2.4910119308365717\n",
      "Len of Validation loss: 54, Average loss: 4.4222414316954435\n",
      "Epoch: 1569, Len of Training loss: 18, Average loss: 2.271893752945794\n",
      "Len of Validation loss: 54, Average loss: 4.068981309731801\n",
      "Epoch: 1570, Len of Training loss: 18, Average loss: 2.1307175291909113\n",
      "Len of Validation loss: 54, Average loss: 3.9744595730746233\n",
      "Epoch: 1571, Len of Training loss: 18, Average loss: 2.1040575239393444\n",
      "Len of Validation loss: 54, Average loss: 4.026565242696692\n",
      "Epoch: 1572, Len of Training loss: 18, Average loss: 2.088416669103834\n",
      "Len of Validation loss: 54, Average loss: 4.189093631726724\n",
      "Epoch: 1573, Len of Training loss: 18, Average loss: 2.192425502671136\n",
      "Len of Validation loss: 54, Average loss: 3.95096324329023\n",
      "Epoch: 1574, Len of Training loss: 18, Average loss: 2.0085886584387884\n",
      "Len of Validation loss: 54, Average loss: 3.9057642817497253\n",
      "Epoch: 1575, Len of Training loss: 18, Average loss: 1.9953263799349468\n",
      "Len of Validation loss: 54, Average loss: 3.931420997337059\n",
      "Epoch: 1576, Len of Training loss: 18, Average loss: 2.0484258201387195\n",
      "Len of Validation loss: 54, Average loss: 4.062448702476643\n",
      "Epoch: 1577, Len of Training loss: 18, Average loss: 2.145310832394494\n",
      "Len of Validation loss: 54, Average loss: 4.030428990169808\n",
      "Epoch: 1578, Len of Training loss: 18, Average loss: 2.104983369509379\n",
      "Len of Validation loss: 54, Average loss: 4.186071221475248\n",
      "Epoch: 1579, Len of Training loss: 18, Average loss: 2.2686094774140253\n",
      "Len of Validation loss: 54, Average loss: 3.879226121637556\n",
      "Epoch: 1580, Len of Training loss: 18, Average loss: 2.195605536301931\n",
      "Len of Validation loss: 54, Average loss: 4.0929304780783475\n",
      "Epoch: 1581, Len of Training loss: 18, Average loss: 2.207534478770362\n",
      "Len of Validation loss: 54, Average loss: 4.282279281704514\n",
      "Epoch: 1582, Len of Training loss: 18, Average loss: 2.345322198337979\n",
      "Len of Validation loss: 54, Average loss: 4.128686492089872\n",
      "Epoch: 1583, Len of Training loss: 18, Average loss: 2.454795413547092\n",
      "Len of Validation loss: 54, Average loss: 4.2403462485030845\n",
      "Epoch: 1584, Len of Training loss: 18, Average loss: 2.233342991934882\n",
      "Len of Validation loss: 54, Average loss: 4.112251140453197\n",
      "Epoch: 1585, Len of Training loss: 18, Average loss: 2.0701755616399975\n",
      "Len of Validation loss: 54, Average loss: 4.048919596053936\n",
      "Epoch: 1586, Len of Training loss: 18, Average loss: 2.0921723577711315\n",
      "Len of Validation loss: 54, Average loss: 3.859680555484913\n",
      "Epoch: 1587, Len of Training loss: 18, Average loss: 2.043689250946045\n",
      "Len of Validation loss: 54, Average loss: 4.1217254907996566\n",
      "Epoch: 1588, Len of Training loss: 18, Average loss: 1.993517021338145\n",
      "Len of Validation loss: 54, Average loss: 4.037882334656185\n",
      "Epoch: 1589, Len of Training loss: 18, Average loss: 2.0227736168437533\n",
      "Len of Validation loss: 54, Average loss: 3.926020785614296\n",
      "Epoch: 1590, Len of Training loss: 18, Average loss: 2.013335492875841\n",
      "Len of Validation loss: 54, Average loss: 3.8826449822496483\n",
      "Epoch: 1591, Len of Training loss: 18, Average loss: 2.1219219499164157\n",
      "Len of Validation loss: 54, Average loss: 4.103071109012321\n",
      "Epoch: 1592, Len of Training loss: 18, Average loss: 2.1015474465158253\n",
      "Len of Validation loss: 54, Average loss: 4.112878956176616\n",
      "Epoch: 1593, Len of Training loss: 18, Average loss: 2.02107181151708\n",
      "Len of Validation loss: 54, Average loss: 4.192558743335582\n",
      "Epoch: 1594, Len of Training loss: 18, Average loss: 2.125475095378028\n",
      "Len of Validation loss: 54, Average loss: 4.0223603049914045\n",
      "Epoch: 1595, Len of Training loss: 18, Average loss: 2.295463807053036\n",
      "Len of Validation loss: 54, Average loss: 4.0034693369159\n",
      "Epoch: 1596, Len of Training loss: 18, Average loss: 2.1591259837150574\n",
      "Len of Validation loss: 54, Average loss: 4.011228821895741\n",
      "Epoch: 1597, Len of Training loss: 18, Average loss: 2.093566271993849\n",
      "Len of Validation loss: 54, Average loss: 4.0136380261845055\n",
      "Epoch: 1598, Len of Training loss: 18, Average loss: 2.0358608298831515\n",
      "Len of Validation loss: 54, Average loss: 4.047380314932929\n",
      "Epoch: 1599, Len of Training loss: 18, Average loss: 2.0119401613871255\n",
      "Len of Validation loss: 54, Average loss: 4.114123887485928\n",
      "Epoch: 1600, Len of Training loss: 18, Average loss: 2.063901272084978\n",
      "Len of Validation loss: 54, Average loss: 3.9140307064409607\n",
      "Epoch: 1601, Len of Training loss: 18, Average loss: 2.1387679908010693\n",
      "Len of Validation loss: 54, Average loss: 4.0912206172943115\n",
      "Epoch: 1602, Len of Training loss: 18, Average loss: 2.2842012180222406\n",
      "Len of Validation loss: 54, Average loss: 4.179556625860709\n",
      "Epoch: 1603, Len of Training loss: 18, Average loss: 2.1772392193476358\n",
      "Len of Validation loss: 54, Average loss: 4.359832035170661\n",
      "Epoch: 1604, Len of Training loss: 18, Average loss: 2.1798164976967707\n",
      "Len of Validation loss: 54, Average loss: 4.142824939003697\n",
      "Epoch: 1605, Len of Training loss: 18, Average loss: 2.01971654759513\n",
      "Len of Validation loss: 54, Average loss: 4.066434756473258\n",
      "Epoch: 1606, Len of Training loss: 18, Average loss: 2.103118512365553\n",
      "Len of Validation loss: 54, Average loss: 3.9702902016816317\n",
      "Epoch: 1607, Len of Training loss: 18, Average loss: 2.1862535542911954\n",
      "Len of Validation loss: 54, Average loss: 4.040327345883405\n",
      "Epoch: 1608, Len of Training loss: 18, Average loss: 2.3727057774861655\n",
      "Len of Validation loss: 54, Average loss: 3.9988152804198087\n",
      "Epoch: 1609, Len of Training loss: 18, Average loss: 2.3349578777949014\n",
      "Len of Validation loss: 54, Average loss: 4.51735653700652\n",
      "Epoch: 1610, Len of Training loss: 18, Average loss: 2.3922445244259305\n",
      "Len of Validation loss: 54, Average loss: 4.1457535867337825\n",
      "Epoch: 1611, Len of Training loss: 18, Average loss: 2.234254479408264\n",
      "Len of Validation loss: 54, Average loss: 4.119808782030035\n",
      "Epoch: 1612, Len of Training loss: 18, Average loss: 2.0232288572523327\n",
      "Len of Validation loss: 54, Average loss: 4.077103184329139\n",
      "Epoch: 1613, Len of Training loss: 18, Average loss: 1.9702096051639981\n",
      "Len of Validation loss: 54, Average loss: 4.019131362438202\n",
      "Epoch: 1614, Len of Training loss: 18, Average loss: 1.9190170566240947\n",
      "Len of Validation loss: 54, Average loss: 3.734357681539324\n",
      "Epoch: 1615, Len of Training loss: 18, Average loss: 1.9069141414430406\n",
      "Len of Validation loss: 54, Average loss: 3.8968490538773715\n",
      "Epoch: 1616, Len of Training loss: 18, Average loss: 1.9755483534601\n",
      "Len of Validation loss: 54, Average loss: 4.007033502614057\n",
      "Epoch: 1617, Len of Training loss: 18, Average loss: 2.106307440333896\n",
      "Len of Validation loss: 54, Average loss: 3.9496344327926636\n",
      "Epoch: 1618, Len of Training loss: 18, Average loss: 2.001380489932166\n",
      "Len of Validation loss: 54, Average loss: 3.8776456978585987\n",
      "Epoch: 1619, Len of Training loss: 18, Average loss: 2.1640719771385193\n",
      "Len of Validation loss: 54, Average loss: 4.310960785106376\n",
      "Epoch: 1620, Len of Training loss: 18, Average loss: 2.3559576670328775\n",
      "Len of Validation loss: 54, Average loss: 4.097074755915889\n",
      "Epoch: 1621, Len of Training loss: 18, Average loss: 2.150180776913961\n",
      "Len of Validation loss: 54, Average loss: 3.959864812868613\n",
      "Epoch: 1622, Len of Training loss: 18, Average loss: 2.000067028734419\n",
      "Len of Validation loss: 54, Average loss: 3.959562520186106\n",
      "Epoch: 1623, Len of Training loss: 18, Average loss: 1.9530247979693942\n",
      "Len of Validation loss: 54, Average loss: 3.859899878501892\n",
      "Epoch: 1624, Len of Training loss: 18, Average loss: 1.8709421290291681\n",
      "Len of Validation loss: 54, Average loss: 3.8504598405626087\n",
      "Epoch: 1625, Len of Training loss: 18, Average loss: 1.8945091830359564\n",
      "Len of Validation loss: 54, Average loss: 3.9414514236980014\n",
      "Epoch: 1626, Len of Training loss: 18, Average loss: 1.8729759123590257\n",
      "Len of Validation loss: 54, Average loss: 3.8010214787942393\n",
      "Epoch: 1627, Len of Training loss: 18, Average loss: 1.8947478069199457\n",
      "Len of Validation loss: 54, Average loss: 3.882996172816665\n",
      "Epoch: 1628, Len of Training loss: 18, Average loss: 1.8760491742028131\n",
      "Len of Validation loss: 54, Average loss: 4.116460332164058\n",
      "Epoch: 1629, Len of Training loss: 18, Average loss: 1.8598484661844041\n",
      "Len of Validation loss: 54, Average loss: 3.8807145246752985\n",
      "Epoch: 1630, Len of Training loss: 18, Average loss: 1.8682781192991469\n",
      "Len of Validation loss: 54, Average loss: 3.863030524165542\n",
      "Epoch: 1631, Len of Training loss: 18, Average loss: 2.0243941677941217\n",
      "Len of Validation loss: 54, Average loss: 4.127369110231046\n",
      "Epoch: 1632, Len of Training loss: 18, Average loss: 2.085598495271471\n",
      "Len of Validation loss: 54, Average loss: 3.9925380724447743\n",
      "Epoch: 1633, Len of Training loss: 18, Average loss: 2.0526113907496133\n",
      "Len of Validation loss: 54, Average loss: 3.8991150105441057\n",
      "Epoch: 1634, Len of Training loss: 18, Average loss: 1.9622065292464361\n",
      "Len of Validation loss: 54, Average loss: 4.04161916397236\n",
      "Epoch: 1635, Len of Training loss: 18, Average loss: 1.9839699665705364\n",
      "Len of Validation loss: 54, Average loss: 4.17799770169788\n",
      "Epoch: 1636, Len of Training loss: 18, Average loss: 2.1824951171875\n",
      "Len of Validation loss: 54, Average loss: 4.2052940130233765\n",
      "Epoch: 1637, Len of Training loss: 18, Average loss: 2.394257730907864\n",
      "Len of Validation loss: 54, Average loss: 4.2700748509830895\n",
      "Epoch: 1638, Len of Training loss: 18, Average loss: 2.403566771083408\n",
      "Len of Validation loss: 54, Average loss: 3.984890964296129\n",
      "Epoch: 1639, Len of Training loss: 18, Average loss: 2.2905349930127463\n",
      "Len of Validation loss: 54, Average loss: 4.048073808352153\n",
      "Epoch: 1640, Len of Training loss: 18, Average loss: 2.052913639280531\n",
      "Len of Validation loss: 54, Average loss: 4.10383477917424\n",
      "Epoch: 1641, Len of Training loss: 18, Average loss: 2.1675926049550376\n",
      "Len of Validation loss: 54, Average loss: 3.990472892920176\n",
      "Epoch: 1642, Len of Training loss: 18, Average loss: 2.234766807821062\n",
      "Len of Validation loss: 54, Average loss: 4.076398222534745\n",
      "Epoch: 1643, Len of Training loss: 18, Average loss: 2.0279559757974415\n",
      "Len of Validation loss: 54, Average loss: 3.9882536155206187\n",
      "Epoch: 1644, Len of Training loss: 18, Average loss: 2.0551895366774664\n",
      "Len of Validation loss: 54, Average loss: 4.004708221665135\n",
      "Epoch: 1645, Len of Training loss: 18, Average loss: 2.0577564239501953\n",
      "Len of Validation loss: 54, Average loss: 4.235641245488767\n",
      "Epoch: 1646, Len of Training loss: 18, Average loss: 2.1959339446491666\n",
      "Len of Validation loss: 54, Average loss: 3.939205785592397\n",
      "Epoch: 1647, Len of Training loss: 18, Average loss: 1.9492759505907695\n",
      "Len of Validation loss: 54, Average loss: 3.8940517571237354\n",
      "Epoch: 1648, Len of Training loss: 18, Average loss: 1.852288477950626\n",
      "Len of Validation loss: 54, Average loss: 4.156532170595946\n",
      "Epoch: 1649, Len of Training loss: 18, Average loss: 1.8386285702387493\n",
      "Len of Validation loss: 54, Average loss: 3.8368946843677096\n",
      "Epoch: 1650, Len of Training loss: 18, Average loss: 2.0057771735721164\n",
      "Len of Validation loss: 54, Average loss: 4.598581402390091\n",
      "Epoch: 1651, Len of Training loss: 18, Average loss: 2.4681660996543036\n",
      "Len of Validation loss: 54, Average loss: 4.098779058014905\n",
      "Epoch: 1652, Len of Training loss: 18, Average loss: 2.5683119429482355\n",
      "Len of Validation loss: 54, Average loss: 4.269677389551092\n",
      "Epoch: 1653, Len of Training loss: 18, Average loss: 2.3929071956210666\n",
      "Len of Validation loss: 54, Average loss: 4.174451717623958\n",
      "Epoch: 1654, Len of Training loss: 18, Average loss: 2.2337380912568836\n",
      "Len of Validation loss: 54, Average loss: 4.375870611932543\n",
      "Epoch: 1655, Len of Training loss: 18, Average loss: 2.180586338043213\n",
      "Len of Validation loss: 54, Average loss: 3.9558121782762035\n",
      "Epoch: 1656, Len of Training loss: 18, Average loss: 2.1214873459604053\n",
      "Len of Validation loss: 54, Average loss: 4.156545360883077\n",
      "Epoch: 1657, Len of Training loss: 18, Average loss: 1.9683702852990892\n",
      "Len of Validation loss: 54, Average loss: 3.828129609425863\n",
      "Epoch: 1658, Len of Training loss: 18, Average loss: 1.966063956419627\n",
      "Len of Validation loss: 54, Average loss: 3.925092162909331\n",
      "Epoch: 1659, Len of Training loss: 18, Average loss: 1.900739265812768\n",
      "Len of Validation loss: 54, Average loss: 4.073665131021429\n",
      "Epoch: 1660, Len of Training loss: 18, Average loss: 1.857980363898807\n",
      "Len of Validation loss: 54, Average loss: 3.895297255780962\n",
      "Epoch: 1661, Len of Training loss: 18, Average loss: 1.968185822168986\n",
      "Len of Validation loss: 54, Average loss: 4.305834035078685\n",
      "Epoch: 1662, Len of Training loss: 18, Average loss: 2.2155957288212247\n",
      "Len of Validation loss: 54, Average loss: 4.117260429594252\n",
      "Epoch: 1663, Len of Training loss: 18, Average loss: 2.021459698677063\n",
      "Len of Validation loss: 54, Average loss: 4.115996285721108\n",
      "Epoch: 1664, Len of Training loss: 18, Average loss: 1.9112762345208063\n",
      "Len of Validation loss: 54, Average loss: 3.8156003753344216\n",
      "Epoch: 1665, Len of Training loss: 18, Average loss: 1.9146520561642117\n",
      "Len of Validation loss: 54, Average loss: 3.7940762484515154\n",
      "Epoch: 1666, Len of Training loss: 18, Average loss: 1.9428853789965312\n",
      "Len of Validation loss: 54, Average loss: 3.9527113525955766\n",
      "Epoch: 1667, Len of Training loss: 18, Average loss: 1.8943492571512859\n",
      "Len of Validation loss: 54, Average loss: 3.809711608621809\n",
      "Epoch: 1668, Len of Training loss: 18, Average loss: 1.8127785854869418\n",
      "Len of Validation loss: 54, Average loss: 3.850015006683491\n",
      "Epoch: 1669, Len of Training loss: 18, Average loss: 1.7709659735361736\n",
      "Len of Validation loss: 54, Average loss: 3.8263158842369362\n",
      "Epoch: 1670, Len of Training loss: 18, Average loss: 1.7579036156336467\n",
      "Len of Validation loss: 54, Average loss: 3.9039127826690674\n",
      "Epoch: 1671, Len of Training loss: 18, Average loss: 1.7799567315313551\n",
      "Len of Validation loss: 54, Average loss: 3.831078957628321\n",
      "Epoch: 1672, Len of Training loss: 18, Average loss: 1.7753783861796062\n",
      "Len of Validation loss: 54, Average loss: 3.8617663670469216\n",
      "Epoch: 1673, Len of Training loss: 18, Average loss: 1.8779965043067932\n",
      "Len of Validation loss: 54, Average loss: 4.424152572949727\n",
      "Epoch: 1674, Len of Training loss: 18, Average loss: 2.0493552022510104\n",
      "Len of Validation loss: 54, Average loss: 4.136230813132392\n",
      "Epoch: 1675, Len of Training loss: 18, Average loss: 1.977064271767934\n",
      "Len of Validation loss: 54, Average loss: 3.7666999167866178\n",
      "Epoch: 1676, Len of Training loss: 18, Average loss: 1.9025007883707683\n",
      "Len of Validation loss: 54, Average loss: 3.935977750354343\n",
      "Epoch: 1677, Len of Training loss: 18, Average loss: 1.823571801185608\n",
      "Len of Validation loss: 54, Average loss: 3.9845130421497204\n",
      "Epoch: 1678, Len of Training loss: 18, Average loss: 1.9517318540149264\n",
      "Len of Validation loss: 54, Average loss: 3.9444326846687883\n",
      "Epoch: 1679, Len of Training loss: 18, Average loss: 1.9760162168078952\n",
      "Len of Validation loss: 54, Average loss: 4.1607212159368725\n",
      "Epoch: 1680, Len of Training loss: 18, Average loss: 1.9303957488801744\n",
      "Len of Validation loss: 54, Average loss: 3.7256556859722845\n",
      "Epoch: 1681, Len of Training loss: 18, Average loss: 1.8502648207876418\n",
      "Len of Validation loss: 54, Average loss: 3.9082157766377486\n",
      "Epoch: 1682, Len of Training loss: 18, Average loss: 1.869040436214871\n",
      "Len of Validation loss: 54, Average loss: 3.9713854083308466\n",
      "Epoch: 1683, Len of Training loss: 18, Average loss: 1.927596198187934\n",
      "Len of Validation loss: 54, Average loss: 3.923144543612445\n",
      "Epoch: 1684, Len of Training loss: 18, Average loss: 2.002924064795176\n",
      "Len of Validation loss: 54, Average loss: 3.9165337593467147\n",
      "Epoch: 1685, Len of Training loss: 18, Average loss: 1.8235185808605618\n",
      "Len of Validation loss: 54, Average loss: 3.9842735286112183\n",
      "Epoch: 1686, Len of Training loss: 18, Average loss: 1.7947623663478427\n",
      "Len of Validation loss: 54, Average loss: 3.9274821325584695\n",
      "Epoch: 1687, Len of Training loss: 18, Average loss: 1.7533572051260207\n",
      "Len of Validation loss: 54, Average loss: 3.997819092538622\n",
      "Epoch: 1688, Len of Training loss: 18, Average loss: 1.9167684581544664\n",
      "Len of Validation loss: 54, Average loss: 3.8632080223825245\n",
      "Epoch: 1689, Len of Training loss: 18, Average loss: 1.884670290682051\n",
      "Len of Validation loss: 54, Average loss: 4.227209459852289\n",
      "Epoch: 1690, Len of Training loss: 18, Average loss: 1.9670033123758104\n",
      "Len of Validation loss: 54, Average loss: 3.8864187289167336\n",
      "Epoch: 1691, Len of Training loss: 18, Average loss: 1.9761260284317865\n",
      "Len of Validation loss: 54, Average loss: 4.284956150584751\n",
      "Epoch: 1692, Len of Training loss: 18, Average loss: 2.2525842587153115\n",
      "Len of Validation loss: 54, Average loss: 4.166846385708562\n",
      "Epoch: 1693, Len of Training loss: 18, Average loss: 2.3531312545140586\n",
      "Len of Validation loss: 54, Average loss: 4.263454463746813\n",
      "Epoch: 1694, Len of Training loss: 18, Average loss: 2.336155745718214\n",
      "Len of Validation loss: 54, Average loss: 3.9260762400097318\n",
      "Epoch: 1695, Len of Training loss: 18, Average loss: 2.2075306905640497\n",
      "Len of Validation loss: 54, Average loss: 3.9988290380548546\n",
      "Epoch: 1696, Len of Training loss: 18, Average loss: 2.2627400755882263\n",
      "Len of Validation loss: 54, Average loss: 4.070688779707308\n",
      "Epoch: 1697, Len of Training loss: 18, Average loss: 2.2221605115466647\n",
      "Len of Validation loss: 54, Average loss: 3.9004517087229975\n",
      "Epoch: 1698, Len of Training loss: 18, Average loss: 2.1207495000627308\n",
      "Len of Validation loss: 54, Average loss: 4.2100410262743635\n",
      "Epoch: 1699, Len of Training loss: 18, Average loss: 2.258639997906155\n",
      "Len of Validation loss: 54, Average loss: 3.7998485697640314\n",
      "Epoch: 1700, Len of Training loss: 18, Average loss: 2.7900705403751798\n",
      "Len of Validation loss: 54, Average loss: 5.275688224368626\n",
      "Epoch: 1701, Len of Training loss: 18, Average loss: 3.0002285374535456\n",
      "Len of Validation loss: 54, Average loss: 4.440708160400391\n",
      "Epoch: 1702, Len of Training loss: 18, Average loss: 2.40162992477417\n",
      "Len of Validation loss: 54, Average loss: 4.063315526202873\n",
      "Epoch: 1703, Len of Training loss: 18, Average loss: 2.159832497437795\n",
      "Len of Validation loss: 54, Average loss: 4.170554613625562\n",
      "Epoch: 1704, Len of Training loss: 18, Average loss: 2.0249739355511136\n",
      "Len of Validation loss: 54, Average loss: 3.8418706832108676\n",
      "Epoch: 1705, Len of Training loss: 18, Average loss: 1.9661919275919597\n",
      "Len of Validation loss: 54, Average loss: 3.9080532590548196\n",
      "Epoch: 1706, Len of Training loss: 18, Average loss: 1.8562022580040827\n",
      "Len of Validation loss: 54, Average loss: 3.9198614292674594\n",
      "Epoch: 1707, Len of Training loss: 18, Average loss: 1.7872638768619962\n",
      "Len of Validation loss: 54, Average loss: 3.9016932050387063\n",
      "Epoch: 1708, Len of Training loss: 18, Average loss: 1.8525202340549893\n",
      "Len of Validation loss: 54, Average loss: 3.8451546099450855\n",
      "Epoch: 1709, Len of Training loss: 18, Average loss: 1.7697601715723674\n",
      "Len of Validation loss: 54, Average loss: 3.9754792186948986\n",
      "Epoch: 1710, Len of Training loss: 18, Average loss: 1.6936125225490994\n",
      "Len of Validation loss: 54, Average loss: 3.831956324753938\n",
      "Epoch: 1711, Len of Training loss: 18, Average loss: 1.691814402739207\n",
      "Len of Validation loss: 54, Average loss: 3.814052793714735\n",
      "Epoch: 1712, Len of Training loss: 18, Average loss: 1.6846170756551955\n",
      "Len of Validation loss: 54, Average loss: 3.8986120687590704\n",
      "Epoch: 1713, Len of Training loss: 18, Average loss: 1.864081707265642\n",
      "Len of Validation loss: 54, Average loss: 4.224450634585486\n",
      "Epoch: 1714, Len of Training loss: 18, Average loss: 1.9943038357628717\n",
      "Len of Validation loss: 54, Average loss: 4.07427050890746\n",
      "Epoch: 1715, Len of Training loss: 18, Average loss: 1.9360867672496371\n",
      "Len of Validation loss: 54, Average loss: 3.9537105869363853\n",
      "Epoch: 1716, Len of Training loss: 18, Average loss: 1.8545157710711162\n",
      "Len of Validation loss: 54, Average loss: 3.834491890889627\n",
      "Epoch: 1717, Len of Training loss: 18, Average loss: 1.879995796415541\n",
      "Len of Validation loss: 54, Average loss: 4.185606448738663\n",
      "Epoch: 1718, Len of Training loss: 18, Average loss: 1.901521020465427\n",
      "Len of Validation loss: 54, Average loss: 3.9392454315114906\n",
      "Epoch: 1719, Len of Training loss: 18, Average loss: 1.8252765933672588\n",
      "Len of Validation loss: 54, Average loss: 3.9017067971052946\n",
      "Epoch: 1720, Len of Training loss: 18, Average loss: 1.7624270584848192\n",
      "Len of Validation loss: 54, Average loss: 3.6923522264869124\n",
      "Epoch: 1721, Len of Training loss: 18, Average loss: 1.7084739804267883\n",
      "Len of Validation loss: 54, Average loss: 3.9468980652314647\n",
      "Epoch: 1722, Len of Training loss: 18, Average loss: 1.7099016110102336\n",
      "Len of Validation loss: 54, Average loss: 3.9369858547493264\n",
      "Epoch: 1723, Len of Training loss: 18, Average loss: 1.6449770794974432\n",
      "Len of Validation loss: 54, Average loss: 3.7096297321496188\n",
      "Epoch: 1724, Len of Training loss: 18, Average loss: 1.679985761642456\n",
      "Len of Validation loss: 54, Average loss: 3.681084147206059\n",
      "Epoch: 1725, Len of Training loss: 18, Average loss: 1.666708078649309\n",
      "Len of Validation loss: 54, Average loss: 3.709825822600612\n",
      "Epoch: 1726, Len of Training loss: 18, Average loss: 1.8513439430130854\n",
      "Len of Validation loss: 54, Average loss: 3.808287258501406\n",
      "Epoch: 1727, Len of Training loss: 18, Average loss: 1.7955689364009433\n",
      "Len of Validation loss: 54, Average loss: 3.812177770667606\n",
      "Epoch: 1728, Len of Training loss: 18, Average loss: 1.758320735560523\n",
      "Len of Validation loss: 54, Average loss: 3.865390876928965\n",
      "Epoch: 1729, Len of Training loss: 18, Average loss: 1.776160458723704\n",
      "Len of Validation loss: 54, Average loss: 3.6925728453530207\n",
      "Epoch: 1730, Len of Training loss: 18, Average loss: 1.8711830178896587\n",
      "Len of Validation loss: 54, Average loss: 4.068517850504981\n",
      "Epoch: 1731, Len of Training loss: 18, Average loss: 1.8687260614501104\n",
      "Len of Validation loss: 54, Average loss: 3.8031618749653853\n",
      "Epoch: 1732, Len of Training loss: 18, Average loss: 1.7186345789167616\n",
      "Len of Validation loss: 54, Average loss: 3.8087144979724177\n",
      "Epoch: 1733, Len of Training loss: 18, Average loss: 1.7561926179462009\n",
      "Len of Validation loss: 54, Average loss: 3.746435485504292\n",
      "Epoch: 1734, Len of Training loss: 18, Average loss: 1.7349597613016765\n",
      "Len of Validation loss: 54, Average loss: 3.777625205340209\n",
      "Epoch: 1735, Len of Training loss: 18, Average loss: 1.6941623820198908\n",
      "Len of Validation loss: 54, Average loss: 3.8615831909356295\n",
      "Epoch: 1736, Len of Training loss: 18, Average loss: 1.8017229106691148\n",
      "Len of Validation loss: 54, Average loss: 3.7067148707531117\n",
      "Epoch: 1737, Len of Training loss: 18, Average loss: 1.743367162015703\n",
      "Len of Validation loss: 54, Average loss: 3.7846761169256986\n",
      "Epoch: 1738, Len of Training loss: 18, Average loss: 1.792950325542026\n",
      "Len of Validation loss: 54, Average loss: 3.7381626500023737\n",
      "Epoch: 1739, Len of Training loss: 18, Average loss: 1.7935731410980225\n",
      "Len of Validation loss: 54, Average loss: 3.94414754708608\n",
      "Epoch: 1740, Len of Training loss: 18, Average loss: 1.7769060532251995\n",
      "Len of Validation loss: 54, Average loss: 3.8789543090043246\n",
      "Epoch: 1741, Len of Training loss: 18, Average loss: 1.7545076343748305\n",
      "Len of Validation loss: 54, Average loss: 3.8442987821720265\n",
      "Epoch: 1742, Len of Training loss: 18, Average loss: 1.728763394885593\n",
      "Len of Validation loss: 54, Average loss: 3.9257650640275745\n",
      "Epoch: 1743, Len of Training loss: 18, Average loss: 1.757492098543379\n",
      "Len of Validation loss: 54, Average loss: 3.7297239943786904\n",
      "Epoch: 1744, Len of Training loss: 18, Average loss: 1.719913747575548\n",
      "Len of Validation loss: 54, Average loss: 3.974875383906894\n",
      "Epoch: 1745, Len of Training loss: 18, Average loss: 1.7140394118097093\n",
      "Len of Validation loss: 54, Average loss: 3.830342882209354\n",
      "Epoch: 1746, Len of Training loss: 18, Average loss: 1.7850858238008287\n",
      "Len of Validation loss: 54, Average loss: 3.929403660474\n",
      "Epoch: 1747, Len of Training loss: 18, Average loss: 2.1001925335990057\n",
      "Len of Validation loss: 54, Average loss: 4.2378661500083075\n",
      "Epoch: 1748, Len of Training loss: 18, Average loss: 2.196455246872372\n",
      "Len of Validation loss: 54, Average loss: 4.1866845157411365\n",
      "Epoch: 1749, Len of Training loss: 18, Average loss: 2.275406234794193\n",
      "Len of Validation loss: 54, Average loss: 4.150256516756834\n",
      "Epoch: 1750, Len of Training loss: 18, Average loss: 2.1042799618509083\n",
      "Len of Validation loss: 54, Average loss: 3.879975292417738\n",
      "Epoch: 1751, Len of Training loss: 18, Average loss: 1.9839728871981304\n",
      "Len of Validation loss: 54, Average loss: 3.8728898697429233\n",
      "Epoch: 1752, Len of Training loss: 18, Average loss: 1.8044001989894443\n",
      "Len of Validation loss: 54, Average loss: 3.8782692198400146\n",
      "Epoch: 1753, Len of Training loss: 18, Average loss: 1.7958298391766019\n",
      "Len of Validation loss: 54, Average loss: 3.8518318754655345\n",
      "Epoch: 1754, Len of Training loss: 18, Average loss: 1.9209033449490864\n",
      "Len of Validation loss: 54, Average loss: 3.8919364280170865\n",
      "Epoch: 1755, Len of Training loss: 18, Average loss: 1.9219356576601665\n",
      "Len of Validation loss: 54, Average loss: 4.117065460593612\n",
      "Epoch: 1756, Len of Training loss: 18, Average loss: 2.067656477292379\n",
      "Len of Validation loss: 54, Average loss: 4.069036598558779\n",
      "Epoch: 1757, Len of Training loss: 18, Average loss: 2.045940531624688\n",
      "Len of Validation loss: 54, Average loss: 3.9152153862847223\n",
      "Epoch: 1758, Len of Training loss: 18, Average loss: 1.910218232207828\n",
      "Len of Validation loss: 54, Average loss: 4.005471980130231\n",
      "Epoch: 1759, Len of Training loss: 18, Average loss: 1.8068709439701505\n",
      "Len of Validation loss: 54, Average loss: 4.0098524623447\n",
      "Epoch: 1760, Len of Training loss: 18, Average loss: 2.025730378097958\n",
      "Len of Validation loss: 54, Average loss: 4.176699302814625\n",
      "Epoch: 1761, Len of Training loss: 18, Average loss: 1.9281179904937744\n",
      "Len of Validation loss: 54, Average loss: 3.9027142966235124\n",
      "Epoch: 1762, Len of Training loss: 18, Average loss: 1.7626563707987468\n",
      "Len of Validation loss: 54, Average loss: 3.9071978209195315\n",
      "Epoch: 1763, Len of Training loss: 18, Average loss: 1.737498230404324\n",
      "Len of Validation loss: 54, Average loss: 3.8745438544838517\n",
      "Epoch: 1764, Len of Training loss: 18, Average loss: 1.7169020771980286\n",
      "Len of Validation loss: 54, Average loss: 3.7253387857366493\n",
      "Epoch: 1765, Len of Training loss: 18, Average loss: 1.698465175098843\n",
      "Len of Validation loss: 54, Average loss: 3.7420065999031067\n",
      "Epoch: 1766, Len of Training loss: 18, Average loss: 1.7980475491947598\n",
      "Len of Validation loss: 54, Average loss: 3.837021878472081\n",
      "Epoch: 1767, Len of Training loss: 18, Average loss: 1.7247115042474535\n",
      "Len of Validation loss: 54, Average loss: 3.7709174663932234\n",
      "Epoch: 1768, Len of Training loss: 18, Average loss: 1.669566015402476\n",
      "Len of Validation loss: 54, Average loss: 3.880972884319447\n",
      "Epoch: 1769, Len of Training loss: 18, Average loss: 1.7253358960151672\n",
      "Len of Validation loss: 54, Average loss: 3.8905663534447\n",
      "Epoch: 1770, Len of Training loss: 18, Average loss: 1.709387481212616\n",
      "Len of Validation loss: 54, Average loss: 4.179571151733398\n",
      "Epoch: 1771, Len of Training loss: 18, Average loss: 1.7780545618798997\n",
      "Len of Validation loss: 54, Average loss: 3.8319385603622154\n",
      "Epoch: 1772, Len of Training loss: 18, Average loss: 1.7468289600478277\n",
      "Len of Validation loss: 54, Average loss: 3.820047550731235\n",
      "Epoch: 1773, Len of Training loss: 18, Average loss: 1.976348380247752\n",
      "Len of Validation loss: 54, Average loss: 3.721070068853873\n",
      "Epoch: 1774, Len of Training loss: 18, Average loss: 1.9154015117221408\n",
      "Len of Validation loss: 54, Average loss: 4.087808418053168\n",
      "Epoch: 1775, Len of Training loss: 18, Average loss: 1.9288570351070828\n",
      "Len of Validation loss: 54, Average loss: 4.018318412480531\n",
      "Epoch: 1776, Len of Training loss: 18, Average loss: 1.8572029537624783\n",
      "Len of Validation loss: 54, Average loss: 3.7203940462183067\n",
      "Epoch: 1777, Len of Training loss: 18, Average loss: 1.7888547711902194\n",
      "Len of Validation loss: 54, Average loss: 4.027078652823413\n",
      "Epoch: 1778, Len of Training loss: 18, Average loss: 1.892332931359609\n",
      "Len of Validation loss: 54, Average loss: 3.9013599400167114\n",
      "Epoch: 1779, Len of Training loss: 18, Average loss: 1.808254639307658\n",
      "Len of Validation loss: 54, Average loss: 3.9148153287393077\n",
      "Epoch: 1780, Len of Training loss: 18, Average loss: 1.747673597600725\n",
      "Len of Validation loss: 54, Average loss: 3.886477752968117\n",
      "Epoch: 1781, Len of Training loss: 18, Average loss: 1.6552730401357014\n",
      "Len of Validation loss: 54, Average loss: 3.8954613871044583\n",
      "Epoch: 1782, Len of Training loss: 18, Average loss: 1.6846944358613756\n",
      "Len of Validation loss: 54, Average loss: 3.647281176514096\n",
      "Epoch: 1783, Len of Training loss: 18, Average loss: 1.647294971677992\n",
      "Len of Validation loss: 54, Average loss: 3.9029614196883307\n",
      "Epoch: 1784, Len of Training loss: 18, Average loss: 1.8156851728757222\n",
      "Len of Validation loss: 54, Average loss: 4.000082121955024\n",
      "Epoch: 1785, Len of Training loss: 18, Average loss: 1.7449689375029669\n",
      "Len of Validation loss: 54, Average loss: 3.846339691568304\n",
      "Epoch: 1786, Len of Training loss: 18, Average loss: 1.9024167789353266\n",
      "Len of Validation loss: 54, Average loss: 3.993920968638526\n",
      "Epoch: 1787, Len of Training loss: 18, Average loss: 1.8892081644799974\n",
      "Len of Validation loss: 54, Average loss: 3.882370277687355\n",
      "Epoch: 1788, Len of Training loss: 18, Average loss: 1.80192940764957\n",
      "Len of Validation loss: 54, Average loss: 3.8690095941225686\n",
      "Epoch: 1789, Len of Training loss: 18, Average loss: 2.0066660708851285\n",
      "Len of Validation loss: 54, Average loss: 4.088317857848273\n",
      "Epoch: 1790, Len of Training loss: 18, Average loss: 1.94840008020401\n",
      "Len of Validation loss: 54, Average loss: 3.7558207048310175\n",
      "Epoch: 1791, Len of Training loss: 18, Average loss: 1.8924796713723078\n",
      "Len of Validation loss: 54, Average loss: 3.7737426824039884\n",
      "Epoch: 1792, Len of Training loss: 18, Average loss: 1.8341627385881212\n",
      "Len of Validation loss: 54, Average loss: 3.984566218323178\n",
      "Epoch: 1793, Len of Training loss: 18, Average loss: 1.846079859468672\n",
      "Len of Validation loss: 54, Average loss: 3.8537705805566578\n",
      "Epoch: 1794, Len of Training loss: 18, Average loss: 1.7902661561965942\n",
      "Len of Validation loss: 54, Average loss: 3.863784549412904\n",
      "Epoch: 1795, Len of Training loss: 18, Average loss: 2.025490985976325\n",
      "Len of Validation loss: 54, Average loss: 3.7427217253932246\n",
      "Epoch: 1796, Len of Training loss: 18, Average loss: 1.93803694513109\n",
      "Len of Validation loss: 54, Average loss: 3.8014356626404657\n",
      "Epoch: 1797, Len of Training loss: 18, Average loss: 1.8556926978958979\n",
      "Len of Validation loss: 54, Average loss: 4.009805096520318\n",
      "Epoch: 1798, Len of Training loss: 18, Average loss: 1.8141309089130826\n",
      "Len of Validation loss: 54, Average loss: 3.968142593348468\n",
      "Epoch: 1799, Len of Training loss: 18, Average loss: 1.7845923039648268\n",
      "Len of Validation loss: 54, Average loss: 3.835437269122512\n",
      "Epoch: 1800, Len of Training loss: 18, Average loss: 1.8499566581514146\n",
      "Len of Validation loss: 54, Average loss: 3.775854011376699\n",
      "Epoch: 1801, Len of Training loss: 18, Average loss: 1.7606640723016527\n",
      "Len of Validation loss: 54, Average loss: 4.069774912463294\n",
      "Epoch: 1802, Len of Training loss: 18, Average loss: 1.7480399939748976\n",
      "Len of Validation loss: 54, Average loss: 3.6616203210971974\n",
      "Epoch: 1803, Len of Training loss: 18, Average loss: 1.6911577383677165\n",
      "Len of Validation loss: 54, Average loss: 3.7295491209736578\n",
      "Epoch: 1804, Len of Training loss: 18, Average loss: 1.7834380202823215\n",
      "Len of Validation loss: 54, Average loss: 3.960159072169551\n",
      "Epoch: 1805, Len of Training loss: 18, Average loss: 1.6759169101715088\n",
      "Len of Validation loss: 54, Average loss: 3.5745259523391724\n",
      "Epoch: 1806, Len of Training loss: 18, Average loss: 1.6659204297595553\n",
      "Len of Validation loss: 54, Average loss: 3.8077452889195196\n",
      "Epoch: 1807, Len of Training loss: 18, Average loss: 1.7481503619088068\n",
      "Len of Validation loss: 54, Average loss: 3.9648105369673834\n",
      "Epoch: 1808, Len of Training loss: 18, Average loss: 1.8317928049299452\n",
      "Len of Validation loss: 54, Average loss: 4.195103896988763\n",
      "Epoch: 1809, Len of Training loss: 18, Average loss: 1.8061028785175748\n",
      "Len of Validation loss: 54, Average loss: 3.9018768314962036\n",
      "Epoch: 1810, Len of Training loss: 18, Average loss: 1.744003156820933\n",
      "Len of Validation loss: 54, Average loss: 3.799444494424043\n",
      "Epoch: 1811, Len of Training loss: 18, Average loss: 1.6840978132353888\n",
      "Len of Validation loss: 54, Average loss: 3.81347699297799\n",
      "Epoch: 1812, Len of Training loss: 18, Average loss: 1.6873861749966939\n",
      "Len of Validation loss: 54, Average loss: 3.785332030720181\n",
      "Epoch: 1813, Len of Training loss: 18, Average loss: 1.6922907365692987\n",
      "Len of Validation loss: 54, Average loss: 3.911734417632774\n",
      "Epoch: 1814, Len of Training loss: 18, Average loss: 1.7768817080391779\n",
      "Len of Validation loss: 54, Average loss: 4.090704460938771\n",
      "Epoch: 1815, Len of Training loss: 18, Average loss: 1.8044532868597243\n",
      "Len of Validation loss: 54, Average loss: 3.730047974321577\n",
      "Epoch: 1816, Len of Training loss: 18, Average loss: 1.9258585174878438\n",
      "Len of Validation loss: 54, Average loss: 4.089603726510648\n",
      "Epoch: 1817, Len of Training loss: 18, Average loss: 1.838877989186181\n",
      "Len of Validation loss: 54, Average loss: 3.7172087784166687\n",
      "Epoch: 1818, Len of Training loss: 18, Average loss: 1.671628263261583\n",
      "Len of Validation loss: 54, Average loss: 3.8872402796038874\n",
      "Epoch: 1819, Len of Training loss: 18, Average loss: 1.6641528606414795\n",
      "Len of Validation loss: 54, Average loss: 3.8171660855964378\n",
      "Epoch: 1820, Len of Training loss: 18, Average loss: 1.7138245569335089\n",
      "Len of Validation loss: 54, Average loss: 3.9110675542442888\n",
      "Epoch: 1821, Len of Training loss: 18, Average loss: 1.7337860001458063\n",
      "Len of Validation loss: 54, Average loss: 3.9678342474831476\n",
      "Epoch: 1822, Len of Training loss: 18, Average loss: 1.6576834188567267\n",
      "Len of Validation loss: 54, Average loss: 3.696156583450459\n",
      "Epoch: 1823, Len of Training loss: 18, Average loss: 1.588505056169298\n",
      "Len of Validation loss: 54, Average loss: 3.8935430778397455\n",
      "Epoch: 1824, Len of Training loss: 18, Average loss: 2.1006251308653088\n",
      "Len of Validation loss: 54, Average loss: 3.9575569475138628\n",
      "Epoch: 1825, Len of Training loss: 18, Average loss: 1.7691984044180975\n",
      "Len of Validation loss: 54, Average loss: 3.8137633999188743\n",
      "Epoch: 1826, Len of Training loss: 18, Average loss: 1.7898407446013556\n",
      "Len of Validation loss: 54, Average loss: 3.963502945723357\n",
      "Epoch: 1827, Len of Training loss: 18, Average loss: 1.9173872537083096\n",
      "Len of Validation loss: 54, Average loss: 4.036347592318499\n",
      "Epoch: 1828, Len of Training loss: 18, Average loss: 2.0277620951334634\n",
      "Len of Validation loss: 54, Average loss: 3.973525906050647\n",
      "Epoch: 1829, Len of Training loss: 18, Average loss: 1.859506454732683\n",
      "Len of Validation loss: 54, Average loss: 3.932924849015695\n",
      "Epoch: 1830, Len of Training loss: 18, Average loss: 1.736700415611267\n",
      "Len of Validation loss: 54, Average loss: 3.7846263889913208\n",
      "Epoch: 1831, Len of Training loss: 18, Average loss: 1.779530856344435\n",
      "Len of Validation loss: 54, Average loss: 3.5397593908839755\n",
      "Epoch: 1832, Len of Training loss: 18, Average loss: 1.8677243987719219\n",
      "Len of Validation loss: 54, Average loss: 4.1988322690681175\n",
      "Epoch: 1833, Len of Training loss: 18, Average loss: 1.864526457256741\n",
      "Len of Validation loss: 54, Average loss: 3.8889233756948403\n",
      "Epoch: 1834, Len of Training loss: 18, Average loss: 1.6960278683238559\n",
      "Len of Validation loss: 54, Average loss: 3.9024833109643726\n",
      "Epoch: 1835, Len of Training loss: 18, Average loss: 1.6450323396258884\n",
      "Len of Validation loss: 54, Average loss: 3.88186189421901\n",
      "Epoch: 1836, Len of Training loss: 18, Average loss: 1.6551119022899203\n",
      "Len of Validation loss: 54, Average loss: 3.9957290093104043\n",
      "Epoch: 1837, Len of Training loss: 18, Average loss: 1.7328599823845758\n",
      "Len of Validation loss: 54, Average loss: 3.8631038511240923\n",
      "Epoch: 1838, Len of Training loss: 18, Average loss: 1.727924108505249\n",
      "Len of Validation loss: 54, Average loss: 3.7852902279959784\n",
      "Epoch: 1839, Len of Training loss: 18, Average loss: 1.6772266957494948\n",
      "Len of Validation loss: 54, Average loss: 3.6418251682210854\n",
      "Epoch: 1840, Len of Training loss: 18, Average loss: 1.6115481588575575\n",
      "Len of Validation loss: 54, Average loss: 3.851364254951477\n",
      "Epoch: 1841, Len of Training loss: 18, Average loss: 1.5700039201312594\n",
      "Len of Validation loss: 54, Average loss: 3.656213226141753\n",
      "Epoch: 1842, Len of Training loss: 18, Average loss: 1.9481794635454814\n",
      "Len of Validation loss: 54, Average loss: 4.23404539514471\n",
      "Epoch: 1843, Len of Training loss: 18, Average loss: 2.1250525448057385\n",
      "Len of Validation loss: 54, Average loss: 4.234444355523145\n",
      "Epoch: 1844, Len of Training loss: 18, Average loss: 2.365085893207126\n",
      "Len of Validation loss: 54, Average loss: 4.735269277184098\n",
      "Epoch: 1845, Len of Training loss: 18, Average loss: 2.1427054272757635\n",
      "Len of Validation loss: 54, Average loss: 4.030469614046591\n",
      "Epoch: 1846, Len of Training loss: 18, Average loss: 1.849753611617618\n",
      "Len of Validation loss: 54, Average loss: 3.9016838758080095\n",
      "Epoch: 1847, Len of Training loss: 18, Average loss: 1.7741669946246676\n",
      "Len of Validation loss: 54, Average loss: 3.7775773803393045\n",
      "Epoch: 1848, Len of Training loss: 18, Average loss: 2.021115369266934\n",
      "Len of Validation loss: 54, Average loss: 3.970467117097643\n",
      "Epoch: 1849, Len of Training loss: 18, Average loss: 1.9881005022260878\n",
      "Len of Validation loss: 54, Average loss: 4.194808829713751\n",
      "Epoch: 1850, Len of Training loss: 18, Average loss: 1.9912150767114427\n",
      "Len of Validation loss: 54, Average loss: 3.8795513885992543\n",
      "Epoch: 1851, Len of Training loss: 18, Average loss: 1.85132982995775\n",
      "Len of Validation loss: 54, Average loss: 3.8412383044207536\n",
      "Epoch: 1852, Len of Training loss: 18, Average loss: 1.7599369486172993\n",
      "Len of Validation loss: 54, Average loss: 3.9788865866484464\n",
      "Epoch: 1853, Len of Training loss: 18, Average loss: 1.71265443166097\n",
      "Len of Validation loss: 54, Average loss: 3.7691382103496127\n",
      "Epoch: 1854, Len of Training loss: 18, Average loss: 1.6328599055608113\n",
      "Len of Validation loss: 54, Average loss: 3.8574428690804377\n",
      "Epoch: 1855, Len of Training loss: 18, Average loss: 1.5836435887548659\n",
      "Len of Validation loss: 54, Average loss: 3.748652806988469\n",
      "Epoch: 1856, Len of Training loss: 18, Average loss: 1.5285737845632765\n",
      "Len of Validation loss: 54, Average loss: 3.8007556487012795\n",
      "Epoch: 1857, Len of Training loss: 18, Average loss: 1.4754770927959018\n",
      "Len of Validation loss: 54, Average loss: 3.732503116130829\n",
      "Epoch: 1858, Len of Training loss: 18, Average loss: 1.5237495435608759\n",
      "Len of Validation loss: 54, Average loss: 3.6739132117342064\n",
      "Epoch: 1859, Len of Training loss: 18, Average loss: 1.5142170323265924\n",
      "Len of Validation loss: 54, Average loss: 3.7321442939617016\n",
      "Epoch: 1860, Len of Training loss: 18, Average loss: 1.5383433765835233\n",
      "Len of Validation loss: 54, Average loss: 3.7648767541956016\n",
      "Epoch: 1861, Len of Training loss: 18, Average loss: 1.5345256394810147\n",
      "Len of Validation loss: 54, Average loss: 3.916770142537576\n",
      "Epoch: 1862, Len of Training loss: 18, Average loss: 1.6319564580917358\n",
      "Len of Validation loss: 54, Average loss: 4.089289214875963\n",
      "Epoch: 1863, Len of Training loss: 18, Average loss: 1.6938891013463337\n",
      "Len of Validation loss: 54, Average loss: 3.6722916254290827\n",
      "Epoch: 1864, Len of Training loss: 18, Average loss: 1.6269778211911519\n",
      "Len of Validation loss: 54, Average loss: 3.6366268197695413\n",
      "Epoch: 1865, Len of Training loss: 18, Average loss: 1.5996509459283617\n",
      "Len of Validation loss: 54, Average loss: 3.620398618556835\n",
      "Epoch: 1866, Len of Training loss: 18, Average loss: 1.5739108522733052\n",
      "Len of Validation loss: 54, Average loss: 3.766366744482959\n",
      "Epoch: 1867, Len of Training loss: 18, Average loss: 1.5796038375960455\n",
      "Len of Validation loss: 54, Average loss: 3.6239073695959867\n",
      "Epoch: 1868, Len of Training loss: 18, Average loss: 1.620615091588762\n",
      "Len of Validation loss: 54, Average loss: 3.9053095645374722\n",
      "Epoch: 1869, Len of Training loss: 18, Average loss: 1.6973986162079706\n",
      "Len of Validation loss: 54, Average loss: 3.8040572184103505\n",
      "Epoch: 1870, Len of Training loss: 18, Average loss: 1.632479727268219\n",
      "Len of Validation loss: 54, Average loss: 3.8018316008426525\n",
      "Epoch: 1871, Len of Training loss: 18, Average loss: 1.569775230354733\n",
      "Len of Validation loss: 54, Average loss: 3.656385675624565\n",
      "Epoch: 1872, Len of Training loss: 18, Average loss: 1.5894049538506403\n",
      "Len of Validation loss: 54, Average loss: 3.9399622590453536\n",
      "Epoch: 1873, Len of Training loss: 18, Average loss: 1.9204312562942505\n",
      "Len of Validation loss: 54, Average loss: 3.979005010039718\n",
      "Epoch: 1874, Len of Training loss: 18, Average loss: 1.8278349836667378\n",
      "Len of Validation loss: 54, Average loss: 3.8336485580161765\n",
      "Epoch: 1875, Len of Training loss: 18, Average loss: 1.7523166007465787\n",
      "Len of Validation loss: 54, Average loss: 4.073153937304461\n",
      "Epoch: 1876, Len of Training loss: 18, Average loss: 1.809100800090366\n",
      "Len of Validation loss: 54, Average loss: 3.942293312814501\n",
      "Epoch: 1877, Len of Training loss: 18, Average loss: 1.729588554965125\n",
      "Len of Validation loss: 54, Average loss: 3.7394404808680215\n",
      "Epoch: 1878, Len of Training loss: 18, Average loss: 1.5901881059010823\n",
      "Len of Validation loss: 54, Average loss: 3.9193563880743802\n",
      "Epoch: 1879, Len of Training loss: 18, Average loss: 1.5815012256304424\n",
      "Len of Validation loss: 54, Average loss: 3.8555541811165988\n",
      "Epoch: 1880, Len of Training loss: 18, Average loss: 1.624522864818573\n",
      "Len of Validation loss: 54, Average loss: 3.750375094237151\n",
      "Epoch: 1881, Len of Training loss: 18, Average loss: 1.542415890428755\n",
      "Len of Validation loss: 54, Average loss: 3.772186760549192\n",
      "Epoch: 1882, Len of Training loss: 18, Average loss: 1.5265338222185771\n",
      "Len of Validation loss: 54, Average loss: 3.73021439048979\n",
      "Epoch: 1883, Len of Training loss: 18, Average loss: 1.5303488704893324\n",
      "Len of Validation loss: 54, Average loss: 3.944599403275384\n",
      "Epoch: 1884, Len of Training loss: 18, Average loss: 1.5691303941938612\n",
      "Len of Validation loss: 54, Average loss: 3.8788079155815973\n",
      "Epoch: 1885, Len of Training loss: 18, Average loss: 1.7879709137810602\n",
      "Len of Validation loss: 54, Average loss: 3.787825244444388\n",
      "Epoch: 1886, Len of Training loss: 18, Average loss: 1.7090323501163058\n",
      "Len of Validation loss: 54, Average loss: 3.899397960415593\n",
      "Epoch: 1887, Len of Training loss: 18, Average loss: 1.7763834926817152\n",
      "Len of Validation loss: 54, Average loss: 3.9268396673379122\n",
      "Epoch: 1888, Len of Training loss: 18, Average loss: 1.671624117427402\n",
      "Len of Validation loss: 54, Average loss: 3.59181547164917\n",
      "Epoch: 1889, Len of Training loss: 18, Average loss: 1.6661480002933078\n",
      "Len of Validation loss: 54, Average loss: 3.785446615130813\n",
      "Epoch: 1890, Len of Training loss: 18, Average loss: 1.6956505245632596\n",
      "Len of Validation loss: 54, Average loss: 3.655715509697243\n",
      "Epoch: 1891, Len of Training loss: 18, Average loss: 1.5967910355991788\n",
      "Len of Validation loss: 54, Average loss: 3.777334714377368\n",
      "Epoch: 1892, Len of Training loss: 18, Average loss: 1.5142598748207092\n",
      "Len of Validation loss: 54, Average loss: 3.7324108613861933\n",
      "Epoch: 1893, Len of Training loss: 18, Average loss: 1.4555689361360338\n",
      "Len of Validation loss: 54, Average loss: 3.6309492036148354\n",
      "Epoch: 1894, Len of Training loss: 18, Average loss: 1.4879743059476216\n",
      "Len of Validation loss: 54, Average loss: 3.8454508472371987\n",
      "Epoch: 1895, Len of Training loss: 18, Average loss: 1.4899685051706102\n",
      "Len of Validation loss: 54, Average loss: 3.705902686825505\n",
      "Epoch: 1896, Len of Training loss: 18, Average loss: 1.6398507290416293\n",
      "Len of Validation loss: 54, Average loss: 4.4762165590568825\n",
      "Epoch: 1897, Len of Training loss: 18, Average loss: 1.9796590937508478\n",
      "Len of Validation loss: 54, Average loss: 4.027289048389152\n",
      "Epoch: 1898, Len of Training loss: 18, Average loss: 1.7566130426194932\n",
      "Len of Validation loss: 54, Average loss: 3.87608551758307\n",
      "Epoch: 1899, Len of Training loss: 18, Average loss: 1.7206293410725064\n",
      "Len of Validation loss: 54, Average loss: 3.6215857797198825\n",
      "Epoch: 1900, Len of Training loss: 18, Average loss: 1.5556831492318048\n",
      "Len of Validation loss: 54, Average loss: 3.6690520489657366\n",
      "Epoch: 1901, Len of Training loss: 18, Average loss: 1.5078979002104864\n",
      "Len of Validation loss: 54, Average loss: 3.741183042526245\n",
      "Epoch: 1902, Len of Training loss: 18, Average loss: 1.5160952011744182\n",
      "Len of Validation loss: 54, Average loss: 3.529055646172276\n",
      "Epoch: 1903, Len of Training loss: 18, Average loss: 1.5015418860647414\n",
      "Len of Validation loss: 54, Average loss: 3.7474251985549927\n",
      "Epoch: 1904, Len of Training loss: 18, Average loss: 1.5613806313938565\n",
      "Len of Validation loss: 54, Average loss: 3.619173087455608\n",
      "Epoch: 1905, Len of Training loss: 18, Average loss: 1.5770298507478502\n",
      "Len of Validation loss: 54, Average loss: 3.746197559215404\n",
      "Epoch: 1906, Len of Training loss: 18, Average loss: 1.5661168694496155\n",
      "Len of Validation loss: 54, Average loss: 3.722682978268023\n",
      "Epoch: 1907, Len of Training loss: 18, Average loss: 1.5458377401034038\n",
      "Len of Validation loss: 54, Average loss: 3.634425657766837\n",
      "Epoch: 1908, Len of Training loss: 18, Average loss: 1.530035628212823\n",
      "Len of Validation loss: 54, Average loss: 3.8389058157249734\n",
      "Epoch: 1909, Len of Training loss: 18, Average loss: 1.538545959525638\n",
      "Len of Validation loss: 54, Average loss: 3.706314680752931\n",
      "Epoch: 1910, Len of Training loss: 18, Average loss: 1.6512281099955242\n",
      "Len of Validation loss: 54, Average loss: 4.011572976907094\n",
      "Epoch: 1911, Len of Training loss: 18, Average loss: 1.9498457511266072\n",
      "Len of Validation loss: 54, Average loss: 3.9523266862939903\n",
      "Epoch: 1912, Len of Training loss: 18, Average loss: 2.0782227714856467\n",
      "Len of Validation loss: 54, Average loss: 4.624925503024349\n",
      "Epoch: 1913, Len of Training loss: 18, Average loss: 2.083871285120646\n",
      "Len of Validation loss: 54, Average loss: 4.051841300946695\n",
      "Epoch: 1914, Len of Training loss: 18, Average loss: 2.6785762972301908\n",
      "Len of Validation loss: 54, Average loss: 4.223662294723369\n",
      "Epoch: 1915, Len of Training loss: 18, Average loss: 2.528504795498318\n",
      "Len of Validation loss: 54, Average loss: 4.303246277349967\n",
      "Epoch: 1916, Len of Training loss: 18, Average loss: 2.275519073009491\n",
      "Len of Validation loss: 54, Average loss: 3.971164164719758\n",
      "Epoch: 1917, Len of Training loss: 18, Average loss: 1.9300272928343878\n",
      "Len of Validation loss: 54, Average loss: 3.8034032230023986\n",
      "Epoch: 1918, Len of Training loss: 18, Average loss: 1.7283228172196283\n",
      "Len of Validation loss: 54, Average loss: 3.7677845734137074\n",
      "Epoch: 1919, Len of Training loss: 18, Average loss: 1.5915760397911072\n",
      "Len of Validation loss: 54, Average loss: 3.8150261265260204\n",
      "Epoch: 1920, Len of Training loss: 18, Average loss: 1.7654282848040264\n",
      "Len of Validation loss: 54, Average loss: 3.723251466397886\n",
      "Epoch: 1921, Len of Training loss: 18, Average loss: 1.809761060608758\n",
      "Len of Validation loss: 54, Average loss: 3.8514142312385418\n",
      "Epoch: 1922, Len of Training loss: 18, Average loss: 1.7344030141830444\n",
      "Len of Validation loss: 54, Average loss: 3.884248850522218\n",
      "Epoch: 1923, Len of Training loss: 18, Average loss: 1.640721513165368\n",
      "Len of Validation loss: 54, Average loss: 3.805047794624611\n",
      "Epoch: 1924, Len of Training loss: 18, Average loss: 1.5372955004374187\n",
      "Len of Validation loss: 54, Average loss: 3.7257473711614257\n",
      "Epoch: 1925, Len of Training loss: 18, Average loss: 1.5404336717393663\n",
      "Len of Validation loss: 54, Average loss: 3.657898854326319\n",
      "Epoch: 1926, Len of Training loss: 18, Average loss: 1.5205819871690538\n",
      "Len of Validation loss: 54, Average loss: 3.934367146756914\n",
      "Epoch: 1927, Len of Training loss: 18, Average loss: 1.5657742155922785\n",
      "Len of Validation loss: 54, Average loss: 3.6719630051542214\n",
      "Epoch: 1928, Len of Training loss: 18, Average loss: 1.5085789495044284\n",
      "Len of Validation loss: 54, Average loss: 3.7780373615247234\n",
      "Epoch: 1929, Len of Training loss: 18, Average loss: 1.5390338632795546\n",
      "Len of Validation loss: 54, Average loss: 3.998252327795382\n",
      "Epoch: 1930, Len of Training loss: 18, Average loss: 1.5756863753000896\n",
      "Len of Validation loss: 54, Average loss: 3.7689983403241194\n",
      "Epoch: 1931, Len of Training loss: 18, Average loss: 1.6121078464719985\n",
      "Len of Validation loss: 54, Average loss: 3.643655869695875\n",
      "Epoch: 1932, Len of Training loss: 18, Average loss: 1.5786578787697687\n",
      "Len of Validation loss: 54, Average loss: 3.7914375773182623\n",
      "Epoch: 1933, Len of Training loss: 18, Average loss: 1.5427811940511067\n",
      "Len of Validation loss: 54, Average loss: 3.7215829204629967\n",
      "Epoch: 1934, Len of Training loss: 18, Average loss: 1.499078220791287\n",
      "Len of Validation loss: 54, Average loss: 3.745246662033929\n",
      "Epoch: 1935, Len of Training loss: 18, Average loss: 1.7321295473310683\n",
      "Len of Validation loss: 54, Average loss: 3.590806773415318\n",
      "Epoch: 1936, Len of Training loss: 18, Average loss: 1.6160470710860357\n",
      "Len of Validation loss: 54, Average loss: 3.653385168976254\n",
      "Epoch: 1937, Len of Training loss: 18, Average loss: 1.5753472447395325\n",
      "Len of Validation loss: 54, Average loss: 3.7396412646328963\n",
      "Epoch: 1938, Len of Training loss: 18, Average loss: 1.5623875856399536\n",
      "Len of Validation loss: 54, Average loss: 3.735314495033688\n",
      "Epoch: 1939, Len of Training loss: 18, Average loss: 1.5323636598057218\n",
      "Len of Validation loss: 54, Average loss: 3.9818801085154214\n",
      "Epoch: 1940, Len of Training loss: 18, Average loss: 1.7822113898065355\n",
      "Len of Validation loss: 54, Average loss: 4.180087979193087\n",
      "Epoch: 1941, Len of Training loss: 18, Average loss: 1.729874571164449\n",
      "Len of Validation loss: 54, Average loss: 3.928211552125436\n",
      "Epoch: 1942, Len of Training loss: 18, Average loss: 1.9017044438256159\n",
      "Len of Validation loss: 54, Average loss: 4.213070019527718\n",
      "Epoch: 1943, Len of Training loss: 18, Average loss: 1.9608201252089605\n",
      "Len of Validation loss: 54, Average loss: 4.0062128592420505\n",
      "Epoch: 1944, Len of Training loss: 18, Average loss: 1.8992239303059049\n",
      "Len of Validation loss: 54, Average loss: 3.8155539985056275\n",
      "Epoch: 1945, Len of Training loss: 18, Average loss: 1.701500568124983\n",
      "Len of Validation loss: 54, Average loss: 4.066059801313612\n",
      "Epoch: 1946, Len of Training loss: 18, Average loss: 1.6731212072902255\n",
      "Len of Validation loss: 54, Average loss: 3.677258328155235\n",
      "Epoch: 1947, Len of Training loss: 18, Average loss: 1.649328390757243\n",
      "Len of Validation loss: 54, Average loss: 3.8123135136233435\n",
      "Epoch: 1948, Len of Training loss: 18, Average loss: 1.650629500548045\n",
      "Len of Validation loss: 54, Average loss: 3.772110433490188\n",
      "Epoch: 1949, Len of Training loss: 18, Average loss: 1.554977986547682\n",
      "Len of Validation loss: 54, Average loss: 3.8159522193449513\n",
      "Epoch: 1950, Len of Training loss: 18, Average loss: 1.5680990152888827\n",
      "Len of Validation loss: 54, Average loss: 3.6145461753562644\n",
      "Epoch: 1951, Len of Training loss: 18, Average loss: 1.5061933464474149\n",
      "Len of Validation loss: 54, Average loss: 3.673033151361677\n",
      "Epoch: 1952, Len of Training loss: 18, Average loss: 1.52218582895067\n",
      "Len of Validation loss: 54, Average loss: 3.839258717166053\n",
      "Epoch: 1953, Len of Training loss: 18, Average loss: 1.5573414034313626\n",
      "Len of Validation loss: 54, Average loss: 3.5909337909133345\n",
      "Epoch: 1954, Len of Training loss: 18, Average loss: 1.599385831091139\n",
      "Len of Validation loss: 54, Average loss: 3.623728319450661\n",
      "Epoch: 1955, Len of Training loss: 18, Average loss: 1.5749266743659973\n",
      "Len of Validation loss: 54, Average loss: 3.833647734589047\n",
      "Epoch: 1956, Len of Training loss: 18, Average loss: 1.5383288595411513\n",
      "Len of Validation loss: 54, Average loss: 3.557496170202891\n",
      "Epoch: 1957, Len of Training loss: 18, Average loss: 1.5585928625530667\n",
      "Len of Validation loss: 54, Average loss: 3.680247301304782\n",
      "Epoch: 1958, Len of Training loss: 18, Average loss: 1.7612819605403476\n",
      "Len of Validation loss: 54, Average loss: 3.9327557969976357\n",
      "Epoch: 1959, Len of Training loss: 18, Average loss: 1.7565288013882108\n",
      "Len of Validation loss: 54, Average loss: 4.0212407465334294\n",
      "Epoch: 1960, Len of Training loss: 18, Average loss: 1.648913217915429\n",
      "Len of Validation loss: 54, Average loss: 3.839312094229239\n",
      "Epoch: 1961, Len of Training loss: 18, Average loss: 1.7568537261750963\n",
      "Len of Validation loss: 54, Average loss: 3.9446397843184293\n",
      "Epoch: 1962, Len of Training loss: 18, Average loss: 1.7357360455724928\n",
      "Len of Validation loss: 54, Average loss: 3.916363566010087\n",
      "Epoch: 1963, Len of Training loss: 18, Average loss: 1.6291726893848844\n",
      "Len of Validation loss: 54, Average loss: 3.876837452252706\n",
      "Epoch: 1964, Len of Training loss: 18, Average loss: 1.5423521333270602\n",
      "Len of Validation loss: 54, Average loss: 3.847889436615838\n",
      "Epoch: 1965, Len of Training loss: 18, Average loss: 1.6412544316715665\n",
      "Len of Validation loss: 54, Average loss: 3.801571285283124\n",
      "Epoch: 1966, Len of Training loss: 18, Average loss: 1.7454058527946472\n",
      "Len of Validation loss: 54, Average loss: 3.883630988774476\n",
      "Epoch: 1967, Len of Training loss: 18, Average loss: 1.714447749985589\n",
      "Len of Validation loss: 54, Average loss: 3.9475105471081204\n",
      "Epoch: 1968, Len of Training loss: 18, Average loss: 1.6453915966881647\n",
      "Len of Validation loss: 54, Average loss: 3.92126449169936\n",
      "Epoch: 1969, Len of Training loss: 18, Average loss: 1.5811826917860243\n",
      "Len of Validation loss: 54, Average loss: 3.761382771862878\n",
      "Epoch: 1970, Len of Training loss: 18, Average loss: 1.5353491107622783\n",
      "Len of Validation loss: 54, Average loss: 3.5521504106345\n",
      "Epoch: 1971, Len of Training loss: 18, Average loss: 1.4905618296729193\n",
      "Len of Validation loss: 54, Average loss: 3.769289241896735\n",
      "Epoch: 1972, Len of Training loss: 18, Average loss: 1.4826887316173978\n",
      "Len of Validation loss: 54, Average loss: 3.468387144583243\n",
      "Epoch: 1973, Len of Training loss: 18, Average loss: 1.4213097625308566\n",
      "Len of Validation loss: 54, Average loss: 3.7978207689744456\n",
      "Epoch: 1974, Len of Training loss: 18, Average loss: 1.376285473505656\n",
      "Len of Validation loss: 54, Average loss: 3.611416913844921\n",
      "Epoch: 1975, Len of Training loss: 18, Average loss: 1.3808095057805378\n",
      "Len of Validation loss: 54, Average loss: 3.641311446825663\n",
      "Epoch: 1976, Len of Training loss: 18, Average loss: 1.517130646440718\n",
      "Len of Validation loss: 54, Average loss: 3.866509949719464\n",
      "Epoch: 1977, Len of Training loss: 18, Average loss: 1.5721273223559062\n",
      "Len of Validation loss: 54, Average loss: 3.804673064638067\n",
      "Epoch: 1978, Len of Training loss: 18, Average loss: 1.5516925719049242\n",
      "Len of Validation loss: 54, Average loss: 3.6852540086816856\n",
      "Epoch: 1979, Len of Training loss: 18, Average loss: 1.532615065574646\n",
      "Len of Validation loss: 54, Average loss: 3.8071763670002974\n",
      "Epoch: 1980, Len of Training loss: 18, Average loss: 1.634069283803304\n",
      "Len of Validation loss: 54, Average loss: 3.7476609349250793\n",
      "Epoch: 1981, Len of Training loss: 18, Average loss: 1.56703562868966\n",
      "Len of Validation loss: 54, Average loss: 3.7647154419510453\n",
      "Epoch: 1982, Len of Training loss: 18, Average loss: 1.563814487722185\n",
      "Len of Validation loss: 54, Average loss: 3.7973347968525357\n",
      "Epoch: 1983, Len of Training loss: 18, Average loss: 1.5269845724105835\n",
      "Len of Validation loss: 54, Average loss: 3.8382718276094505\n",
      "Epoch: 1984, Len of Training loss: 18, Average loss: 1.508331259091695\n",
      "Len of Validation loss: 54, Average loss: 3.854852807742578\n",
      "Epoch: 1985, Len of Training loss: 18, Average loss: 1.7242073085572984\n",
      "Len of Validation loss: 54, Average loss: 4.153652129349886\n",
      "Epoch: 1986, Len of Training loss: 18, Average loss: 1.965573059187995\n",
      "Len of Validation loss: 54, Average loss: 4.271697993631716\n",
      "Epoch: 1987, Len of Training loss: 18, Average loss: 1.8482622967825995\n",
      "Len of Validation loss: 54, Average loss: 3.738246754363731\n",
      "Epoch: 1988, Len of Training loss: 18, Average loss: 1.6984281142552693\n",
      "Len of Validation loss: 54, Average loss: 3.704973989062839\n",
      "Epoch: 1989, Len of Training loss: 18, Average loss: 1.6541601684358385\n",
      "Len of Validation loss: 54, Average loss: 3.7805834809939065\n",
      "Epoch: 1990, Len of Training loss: 18, Average loss: 1.4533707035912409\n",
      "Len of Validation loss: 54, Average loss: 3.5055685815987765\n",
      "Epoch: 1991, Len of Training loss: 18, Average loss: 1.495680332183838\n",
      "Len of Validation loss: 54, Average loss: 3.7447638555809304\n",
      "Epoch: 1992, Len of Training loss: 18, Average loss: 1.5131422215037875\n",
      "Len of Validation loss: 54, Average loss: 3.86863766224296\n",
      "Epoch: 1993, Len of Training loss: 18, Average loss: 1.53878562980228\n",
      "Len of Validation loss: 54, Average loss: 3.6928707891040378\n",
      "Epoch: 1994, Len of Training loss: 18, Average loss: 1.4471473627620273\n",
      "Len of Validation loss: 54, Average loss: 3.587856822543674\n",
      "Epoch: 1995, Len of Training loss: 18, Average loss: 1.56296878390842\n",
      "Len of Validation loss: 54, Average loss: 3.8462498629534685\n",
      "Epoch: 1996, Len of Training loss: 18, Average loss: 1.524730212158627\n",
      "Len of Validation loss: 54, Average loss: 3.62994036188832\n",
      "Epoch: 1997, Len of Training loss: 18, Average loss: 1.5264245602819655\n",
      "Len of Validation loss: 54, Average loss: 3.7980542381604514\n",
      "Epoch: 1998, Len of Training loss: 18, Average loss: 1.6754163172509935\n",
      "Len of Validation loss: 54, Average loss: 4.18623251164401\n",
      "Epoch: 1999, Len of Training loss: 18, Average loss: 2.142905831336975\n",
      "Len of Validation loss: 54, Average loss: 4.311731943377742\n",
      "Epoch: 2000, Len of Training loss: 18, Average loss: 1.857447690433926\n",
      "Len of Validation loss: 54, Average loss: 3.8345513498341597\n",
      "Epoch: 2001, Len of Training loss: 18, Average loss: 1.6923803488413494\n",
      "Len of Validation loss: 54, Average loss: 3.8714576959609985\n",
      "Epoch: 2002, Len of Training loss: 18, Average loss: 1.6297020117441814\n",
      "Len of Validation loss: 54, Average loss: 3.881209722271672\n",
      "Epoch: 2003, Len of Training loss: 18, Average loss: 1.5744256642129686\n",
      "Len of Validation loss: 54, Average loss: 3.668486500227893\n",
      "Epoch: 2004, Len of Training loss: 18, Average loss: 1.5059778756565518\n",
      "Len of Validation loss: 54, Average loss: 3.6569690726421498\n",
      "Epoch: 2005, Len of Training loss: 18, Average loss: 1.4157289928860135\n",
      "Len of Validation loss: 54, Average loss: 3.8158527149094477\n",
      "Epoch: 2006, Len of Training loss: 18, Average loss: 1.398031148645613\n",
      "Len of Validation loss: 54, Average loss: 3.6573987581111767\n",
      "Epoch: 2007, Len of Training loss: 18, Average loss: 1.3951701654328241\n",
      "Len of Validation loss: 54, Average loss: 3.586099088191986\n",
      "Epoch: 2008, Len of Training loss: 18, Average loss: 1.4124015437232122\n",
      "Len of Validation loss: 54, Average loss: 3.8668898277812533\n",
      "Epoch: 2009, Len of Training loss: 18, Average loss: 1.5262188646528456\n",
      "Len of Validation loss: 54, Average loss: 3.8092809407799333\n",
      "Epoch: 2010, Len of Training loss: 18, Average loss: 1.5725891126526728\n",
      "Len of Validation loss: 54, Average loss: 3.6697577900356717\n",
      "Epoch: 2011, Len of Training loss: 18, Average loss: 1.4861001902156405\n",
      "Len of Validation loss: 54, Average loss: 3.67841860320833\n",
      "Epoch: 2012, Len of Training loss: 18, Average loss: 1.5021242168214586\n",
      "Len of Validation loss: 54, Average loss: 3.8894720794977964\n",
      "Epoch: 2013, Len of Training loss: 18, Average loss: 1.487102707227071\n",
      "Len of Validation loss: 54, Average loss: 3.7639274453675307\n",
      "Epoch: 2014, Len of Training loss: 18, Average loss: 1.5962395932939317\n",
      "Len of Validation loss: 54, Average loss: 3.7481567131148443\n",
      "Epoch: 2015, Len of Training loss: 18, Average loss: 1.5109013981289334\n",
      "Len of Validation loss: 54, Average loss: 3.6881345863695496\n",
      "Epoch: 2016, Len of Training loss: 18, Average loss: 1.5915396014849346\n",
      "Len of Validation loss: 54, Average loss: 3.8491659539717213\n",
      "Epoch: 2017, Len of Training loss: 18, Average loss: 1.7245875133408441\n",
      "Len of Validation loss: 54, Average loss: 4.6682731045617\n",
      "Epoch: 2018, Len of Training loss: 18, Average loss: 1.7531191772884793\n",
      "Len of Validation loss: 54, Average loss: 3.89486778444714\n",
      "Epoch: 2019, Len of Training loss: 18, Average loss: 1.7941370275285509\n",
      "Len of Validation loss: 54, Average loss: 3.753079738881853\n",
      "Epoch: 2020, Len of Training loss: 18, Average loss: 1.6682102150387235\n",
      "Len of Validation loss: 54, Average loss: 3.6462927151609352\n",
      "Epoch: 2021, Len of Training loss: 18, Average loss: 1.5257267819510565\n",
      "Len of Validation loss: 54, Average loss: 3.850480103934253\n",
      "Epoch: 2022, Len of Training loss: 18, Average loss: 1.554675857226054\n",
      "Len of Validation loss: 54, Average loss: 3.9707034517217568\n",
      "Epoch: 2023, Len of Training loss: 18, Average loss: 1.5985184643003676\n",
      "Len of Validation loss: 54, Average loss: 3.8534783169075295\n",
      "Epoch: 2024, Len of Training loss: 18, Average loss: 2.1054250266816883\n",
      "Len of Validation loss: 54, Average loss: 4.876742351938177\n",
      "Epoch: 2025, Len of Training loss: 18, Average loss: 2.285876168145074\n",
      "Len of Validation loss: 54, Average loss: 4.043100586643925\n",
      "Epoch: 2026, Len of Training loss: 18, Average loss: 1.9266710546281602\n",
      "Len of Validation loss: 54, Average loss: 4.12609945623963\n",
      "Epoch: 2027, Len of Training loss: 18, Average loss: 1.7916678388913472\n",
      "Len of Validation loss: 54, Average loss: 3.753828638129764\n",
      "Epoch: 2028, Len of Training loss: 18, Average loss: 1.5908626980251737\n",
      "Len of Validation loss: 54, Average loss: 3.784380890704967\n",
      "Epoch: 2029, Len of Training loss: 18, Average loss: 1.6223688854111566\n",
      "Len of Validation loss: 54, Average loss: 3.8311123340218156\n",
      "Epoch: 2030, Len of Training loss: 18, Average loss: 1.6407943566640217\n",
      "Len of Validation loss: 54, Average loss: 3.684138532038088\n",
      "Epoch: 2031, Len of Training loss: 18, Average loss: 1.537591536839803\n",
      "Len of Validation loss: 54, Average loss: 4.075158028690903\n",
      "Epoch: 2032, Len of Training loss: 18, Average loss: 1.571963714228736\n",
      "Len of Validation loss: 54, Average loss: 3.8371416418640703\n",
      "Epoch: 2033, Len of Training loss: 18, Average loss: 1.5795148147477045\n",
      "Len of Validation loss: 54, Average loss: 3.683044038437031\n",
      "Epoch: 2034, Len of Training loss: 18, Average loss: 1.5491833090782166\n",
      "Len of Validation loss: 54, Average loss: 3.6786990850060075\n",
      "Epoch: 2035, Len of Training loss: 18, Average loss: 1.519166933165656\n",
      "Len of Validation loss: 54, Average loss: 3.74315564499961\n",
      "Epoch: 2036, Len of Training loss: 18, Average loss: 1.5224295987023249\n",
      "Len of Validation loss: 54, Average loss: 3.911721584973512\n",
      "Epoch: 2037, Len of Training loss: 18, Average loss: 1.7715775900416904\n",
      "Len of Validation loss: 54, Average loss: 3.828421493371328\n",
      "Epoch: 2038, Len of Training loss: 18, Average loss: 1.7370739579200745\n",
      "Len of Validation loss: 54, Average loss: 3.6956193756174156\n",
      "Epoch: 2039, Len of Training loss: 18, Average loss: 1.5993021329243977\n",
      "Len of Validation loss: 54, Average loss: 3.797235094838672\n",
      "Epoch: 2040, Len of Training loss: 18, Average loss: 1.697942813237508\n",
      "Len of Validation loss: 54, Average loss: 4.002706958187951\n",
      "Epoch: 2041, Len of Training loss: 18, Average loss: 1.5960118770599365\n",
      "Len of Validation loss: 54, Average loss: 3.8712568327232644\n",
      "Epoch: 2042, Len of Training loss: 18, Average loss: 1.4560030831231012\n",
      "Len of Validation loss: 54, Average loss: 3.593755590694922\n",
      "Epoch: 2043, Len of Training loss: 18, Average loss: 1.4661340647273593\n",
      "Len of Validation loss: 54, Average loss: 3.733457451617276\n",
      "Epoch: 2044, Len of Training loss: 18, Average loss: 1.4394688540034823\n",
      "Len of Validation loss: 54, Average loss: 3.7233407177306987\n",
      "Epoch: 2045, Len of Training loss: 18, Average loss: 1.5113607115215726\n",
      "Len of Validation loss: 54, Average loss: 3.8205915645316795\n",
      "Epoch: 2046, Len of Training loss: 18, Average loss: 1.5831440620952182\n",
      "Len of Validation loss: 54, Average loss: 3.9329555608608104\n",
      "Epoch: 2047, Len of Training loss: 18, Average loss: 1.5270189709133573\n",
      "Len of Validation loss: 54, Average loss: 3.721705460989917\n",
      "Epoch: 2048, Len of Training loss: 18, Average loss: 1.6646381153000727\n",
      "Len of Validation loss: 54, Average loss: 3.9111808549474785\n",
      "Epoch: 2049, Len of Training loss: 18, Average loss: 1.5158655444780986\n",
      "Len of Validation loss: 54, Average loss: 3.6948710558591067\n",
      "Epoch: 2050, Len of Training loss: 18, Average loss: 1.5265957315762837\n",
      "Len of Validation loss: 54, Average loss: 3.8785431407116078\n",
      "Epoch: 2051, Len of Training loss: 18, Average loss: 1.6239651772711012\n",
      "Len of Validation loss: 54, Average loss: 3.828956445058187\n",
      "Epoch: 2052, Len of Training loss: 18, Average loss: 1.9095598194334242\n",
      "Len of Validation loss: 54, Average loss: 3.889750897884369\n",
      "Epoch: 2053, Len of Training loss: 18, Average loss: 1.6738871600892808\n",
      "Len of Validation loss: 54, Average loss: 3.681105379705076\n",
      "Epoch: 2054, Len of Training loss: 18, Average loss: 1.6163127223650615\n",
      "Len of Validation loss: 54, Average loss: 3.938169139402884\n",
      "Epoch: 2055, Len of Training loss: 18, Average loss: 1.4671864642037287\n",
      "Len of Validation loss: 54, Average loss: 3.535872300465902\n",
      "Epoch: 2056, Len of Training loss: 18, Average loss: 1.5779937505722046\n",
      "Len of Validation loss: 54, Average loss: 3.7972535601368658\n",
      "Epoch: 2057, Len of Training loss: 18, Average loss: 1.4690145519044664\n",
      "Len of Validation loss: 54, Average loss: 3.738691215161924\n",
      "Epoch: 2058, Len of Training loss: 18, Average loss: 1.413075817955865\n",
      "Len of Validation loss: 54, Average loss: 3.761955799879851\n",
      "Epoch: 2059, Len of Training loss: 18, Average loss: 1.417200830247667\n",
      "Len of Validation loss: 54, Average loss: 3.600191573301951\n",
      "Epoch: 2060, Len of Training loss: 18, Average loss: 1.3690967626041837\n",
      "Len of Validation loss: 54, Average loss: 3.6644897836226002\n",
      "Epoch: 2061, Len of Training loss: 18, Average loss: 1.4499056802855597\n",
      "Len of Validation loss: 54, Average loss: 3.6424176383901528\n",
      "Epoch: 2062, Len of Training loss: 18, Average loss: 1.3706497550010681\n",
      "Len of Validation loss: 54, Average loss: 3.801889675634879\n",
      "Epoch: 2063, Len of Training loss: 18, Average loss: 1.4355531401104398\n",
      "Len of Validation loss: 54, Average loss: 3.659620976006543\n",
      "Epoch: 2064, Len of Training loss: 18, Average loss: 1.4866733021206326\n",
      "Len of Validation loss: 54, Average loss: 3.4962673761226513\n",
      "Epoch: 2065, Len of Training loss: 18, Average loss: 1.522437380419837\n",
      "Len of Validation loss: 54, Average loss: 3.7788533634609647\n",
      "Epoch: 2066, Len of Training loss: 18, Average loss: 1.4340218305587769\n",
      "Len of Validation loss: 54, Average loss: 3.5504436448768333\n",
      "Epoch: 2067, Len of Training loss: 18, Average loss: 1.6883069740401373\n",
      "Len of Validation loss: 54, Average loss: 3.85169529804477\n",
      "Epoch: 2068, Len of Training loss: 18, Average loss: 1.6602860358026292\n",
      "Len of Validation loss: 54, Average loss: 3.72794422617665\n",
      "Epoch: 2069, Len of Training loss: 18, Average loss: 1.5458430184258356\n",
      "Len of Validation loss: 54, Average loss: 3.6406222504598125\n",
      "Epoch: 2070, Len of Training loss: 18, Average loss: 1.414959192276001\n",
      "Len of Validation loss: 54, Average loss: 3.636931276983685\n",
      "Epoch: 2071, Len of Training loss: 18, Average loss: 1.3284828464190166\n",
      "Len of Validation loss: 54, Average loss: 3.6665905073836997\n",
      "Epoch: 2072, Len of Training loss: 18, Average loss: 1.3235587543911405\n",
      "Len of Validation loss: 54, Average loss: 3.5510800811979504\n",
      "Epoch: 2073, Len of Training loss: 18, Average loss: 1.295191678735945\n",
      "Len of Validation loss: 54, Average loss: 3.5488660357616566\n",
      "Epoch: 2074, Len of Training loss: 18, Average loss: 1.3199594020843506\n",
      "Len of Validation loss: 54, Average loss: 3.593731696958895\n",
      "Epoch: 2075, Len of Training loss: 18, Average loss: 1.3747360308965046\n",
      "Len of Validation loss: 54, Average loss: 3.513081594749733\n",
      "Epoch: 2076, Len of Training loss: 18, Average loss: 1.3613719476593866\n",
      "Len of Validation loss: 54, Average loss: 3.6096751071788646\n",
      "Epoch: 2077, Len of Training loss: 18, Average loss: 1.450743105676439\n",
      "Len of Validation loss: 54, Average loss: 3.975502736038632\n",
      "Epoch: 2078, Len of Training loss: 18, Average loss: 1.585675577322642\n",
      "Len of Validation loss: 54, Average loss: 3.8430829136459916\n",
      "Epoch: 2079, Len of Training loss: 18, Average loss: 1.6054053571489122\n",
      "Len of Validation loss: 54, Average loss: 3.7007691540099956\n",
      "Epoch: 2080, Len of Training loss: 18, Average loss: 1.5557165675693088\n",
      "Len of Validation loss: 54, Average loss: 3.842210974958208\n",
      "Epoch: 2081, Len of Training loss: 18, Average loss: 1.527026977803972\n",
      "Len of Validation loss: 54, Average loss: 3.611388314653326\n",
      "Epoch: 2082, Len of Training loss: 18, Average loss: 1.520434902773963\n",
      "Len of Validation loss: 54, Average loss: 3.6554229502324707\n",
      "Epoch: 2083, Len of Training loss: 18, Average loss: 1.4649106595251296\n",
      "Len of Validation loss: 54, Average loss: 3.7445241654360735\n",
      "Epoch: 2084, Len of Training loss: 18, Average loss: 1.520365681913164\n",
      "Len of Validation loss: 54, Average loss: 3.7433830433421664\n",
      "Epoch: 2085, Len of Training loss: 18, Average loss: 1.491761280430688\n",
      "Len of Validation loss: 54, Average loss: 3.7396759390830994\n",
      "Epoch: 2086, Len of Training loss: 18, Average loss: 1.5229738818274603\n",
      "Len of Validation loss: 54, Average loss: 3.727892317153789\n",
      "Epoch: 2087, Len of Training loss: 18, Average loss: 1.5169506933954027\n",
      "Len of Validation loss: 54, Average loss: 3.7555575657773903\n",
      "Epoch: 2088, Len of Training loss: 18, Average loss: 1.5294453104337056\n",
      "Len of Validation loss: 54, Average loss: 3.6816250129982278\n",
      "Epoch: 2089, Len of Training loss: 18, Average loss: 1.5717021624247234\n",
      "Len of Validation loss: 54, Average loss: 3.7375100519922047\n",
      "Epoch: 2090, Len of Training loss: 18, Average loss: 1.5239553451538086\n",
      "Len of Validation loss: 54, Average loss: 3.785441756248474\n",
      "Epoch: 2091, Len of Training loss: 18, Average loss: 1.5364034043418036\n",
      "Len of Validation loss: 54, Average loss: 3.842133738376476\n",
      "Epoch: 2092, Len of Training loss: 18, Average loss: 1.5781197945276897\n",
      "Len of Validation loss: 54, Average loss: 3.749483651585049\n",
      "Epoch: 2093, Len of Training loss: 18, Average loss: 1.5082055727640789\n",
      "Len of Validation loss: 54, Average loss: 3.7393463342278093\n",
      "Epoch: 2094, Len of Training loss: 18, Average loss: 1.515384680695004\n",
      "Len of Validation loss: 54, Average loss: 3.8612464600139194\n",
      "Epoch: 2095, Len of Training loss: 18, Average loss: 1.5241753061612446\n",
      "Len of Validation loss: 54, Average loss: 3.6996422123025963\n",
      "Epoch: 2096, Len of Training loss: 18, Average loss: 1.577918787797292\n",
      "Len of Validation loss: 54, Average loss: 3.9429438290772616\n",
      "Epoch: 2097, Len of Training loss: 18, Average loss: 1.6116805142826505\n",
      "Len of Validation loss: 54, Average loss: 3.5295458369784884\n",
      "Epoch: 2098, Len of Training loss: 18, Average loss: 1.4799555672539606\n",
      "Len of Validation loss: 54, Average loss: 3.8098537347934864\n",
      "Epoch: 2099, Len of Training loss: 18, Average loss: 1.5864927636252508\n",
      "Len of Validation loss: 54, Average loss: 3.773812472820282\n",
      "Epoch: 2100, Len of Training loss: 18, Average loss: 1.550050887796614\n",
      "Len of Validation loss: 54, Average loss: 3.8122788866360984\n",
      "Epoch: 2101, Len of Training loss: 18, Average loss: 1.5614373220337763\n",
      "Len of Validation loss: 54, Average loss: 3.596289528740777\n",
      "Epoch: 2102, Len of Training loss: 18, Average loss: 1.4630858103434246\n",
      "Len of Validation loss: 54, Average loss: 3.6900482884159795\n",
      "Epoch: 2103, Len of Training loss: 18, Average loss: 1.4726000560654535\n",
      "Len of Validation loss: 54, Average loss: 3.951226947484193\n",
      "Epoch: 2104, Len of Training loss: 18, Average loss: 1.5798634423149958\n",
      "Len of Validation loss: 54, Average loss: 3.8073511874234236\n",
      "Epoch: 2105, Len of Training loss: 18, Average loss: 1.6529725061522589\n",
      "Len of Validation loss: 54, Average loss: 4.248506510699237\n",
      "Epoch: 2106, Len of Training loss: 18, Average loss: 1.7819821106062994\n",
      "Len of Validation loss: 54, Average loss: 3.7527188813244856\n",
      "Epoch: 2107, Len of Training loss: 18, Average loss: 1.6268398298157587\n",
      "Len of Validation loss: 54, Average loss: 4.058915423022376\n",
      "Epoch: 2108, Len of Training loss: 18, Average loss: 1.6686690979533725\n",
      "Len of Validation loss: 54, Average loss: 4.013232443067762\n",
      "Epoch: 2109, Len of Training loss: 18, Average loss: 1.6493996911578708\n",
      "Len of Validation loss: 54, Average loss: 3.892940417484001\n",
      "Epoch: 2110, Len of Training loss: 18, Average loss: 1.6105877492162917\n",
      "Len of Validation loss: 54, Average loss: 3.7782276979199163\n",
      "Epoch: 2111, Len of Training loss: 18, Average loss: 1.5235700011253357\n",
      "Len of Validation loss: 54, Average loss: 3.6926208359223827\n",
      "Epoch: 2112, Len of Training loss: 18, Average loss: 1.432196319103241\n",
      "Len of Validation loss: 54, Average loss: 3.6572703697063305\n",
      "Epoch: 2113, Len of Training loss: 18, Average loss: 1.425448305077023\n",
      "Len of Validation loss: 54, Average loss: 3.759568589705008\n",
      "Epoch: 2114, Len of Training loss: 18, Average loss: 1.4819210701518588\n",
      "Len of Validation loss: 54, Average loss: 3.7121872394173234\n",
      "Epoch: 2115, Len of Training loss: 18, Average loss: 1.490050925148858\n",
      "Len of Validation loss: 54, Average loss: 3.537201267701608\n",
      "Epoch: 2116, Len of Training loss: 18, Average loss: 1.4470368689960904\n",
      "Len of Validation loss: 54, Average loss: 3.770901298081433\n",
      "Epoch: 2117, Len of Training loss: 18, Average loss: 1.4248968429035611\n",
      "Len of Validation loss: 54, Average loss: 3.735330259358441\n",
      "Epoch: 2118, Len of Training loss: 18, Average loss: 1.4129243890444438\n",
      "Len of Validation loss: 54, Average loss: 3.49706690730872\n",
      "Epoch: 2119, Len of Training loss: 18, Average loss: 1.3317696187231276\n",
      "Len of Validation loss: 54, Average loss: 3.712197716589327\n",
      "Epoch: 2120, Len of Training loss: 18, Average loss: 1.3036146495077345\n",
      "Len of Validation loss: 54, Average loss: 3.8114577211715557\n",
      "Epoch: 2121, Len of Training loss: 18, Average loss: 1.3444522155655756\n",
      "Len of Validation loss: 54, Average loss: 3.5455632220815727\n",
      "Epoch: 2122, Len of Training loss: 18, Average loss: 1.3436756465170119\n",
      "Len of Validation loss: 54, Average loss: 3.6587051219410367\n",
      "Epoch: 2123, Len of Training loss: 18, Average loss: 1.3930056028895907\n",
      "Len of Validation loss: 54, Average loss: 3.9271215752319053\n",
      "Epoch: 2124, Len of Training loss: 18, Average loss: 1.6635364558961656\n",
      "Len of Validation loss: 54, Average loss: 3.8003011257560164\n",
      "Epoch: 2125, Len of Training loss: 18, Average loss: 1.5480805502997503\n",
      "Len of Validation loss: 54, Average loss: 3.6742020977867975\n",
      "Epoch: 2126, Len of Training loss: 18, Average loss: 1.450800014866723\n",
      "Len of Validation loss: 54, Average loss: 3.519456371113106\n",
      "Epoch: 2127, Len of Training loss: 18, Average loss: 1.3635137677192688\n",
      "Len of Validation loss: 54, Average loss: 4.0824346211221485\n",
      "Epoch: 2128, Len of Training loss: 18, Average loss: 1.4360978404680889\n",
      "Len of Validation loss: 54, Average loss: 3.553849078990795\n",
      "Epoch: 2129, Len of Training loss: 18, Average loss: 1.4229641556739807\n",
      "Len of Validation loss: 54, Average loss: 3.617585062980652\n",
      "Epoch: 2130, Len of Training loss: 18, Average loss: 1.4511996176507738\n",
      "Len of Validation loss: 54, Average loss: 3.523279028910178\n",
      "Epoch: 2131, Len of Training loss: 18, Average loss: 1.415872507625156\n",
      "Len of Validation loss: 54, Average loss: 3.938142661695127\n",
      "Epoch: 2132, Len of Training loss: 18, Average loss: 1.3969722721311781\n",
      "Len of Validation loss: 54, Average loss: 3.7955944692647017\n",
      "Epoch: 2133, Len of Training loss: 18, Average loss: 1.4025012254714966\n",
      "Len of Validation loss: 54, Average loss: 3.79008800012094\n",
      "Epoch: 2134, Len of Training loss: 18, Average loss: 1.5586484207047357\n",
      "Len of Validation loss: 54, Average loss: 3.8348367832325123\n",
      "Epoch: 2135, Len of Training loss: 18, Average loss: 1.4533728824721441\n",
      "Len of Validation loss: 54, Average loss: 3.8350462350580425\n",
      "Epoch: 2136, Len of Training loss: 18, Average loss: 1.4256806903415256\n",
      "Len of Validation loss: 54, Average loss: 3.8148275437178434\n",
      "Epoch: 2137, Len of Training loss: 18, Average loss: 1.6196826100349426\n",
      "Len of Validation loss: 54, Average loss: 3.7062305399665125\n",
      "Epoch: 2138, Len of Training loss: 18, Average loss: 1.5165604286723666\n",
      "Len of Validation loss: 54, Average loss: 3.5326921630788735\n",
      "Epoch: 2139, Len of Training loss: 18, Average loss: 1.5032304657830133\n",
      "Len of Validation loss: 54, Average loss: 3.880530242566709\n",
      "Epoch: 2140, Len of Training loss: 18, Average loss: 1.4003430406252544\n",
      "Len of Validation loss: 54, Average loss: 3.641472845165818\n",
      "Epoch: 2141, Len of Training loss: 18, Average loss: 1.3150629732343886\n",
      "Len of Validation loss: 54, Average loss: 3.4181882959825023\n",
      "Epoch: 2142, Len of Training loss: 18, Average loss: 1.3485472400983174\n",
      "Len of Validation loss: 54, Average loss: 3.75583443597511\n",
      "Epoch: 2143, Len of Training loss: 18, Average loss: 1.4760685364405315\n",
      "Len of Validation loss: 54, Average loss: 3.775060305992762\n",
      "Epoch: 2144, Len of Training loss: 18, Average loss: 1.627131978670756\n",
      "Len of Validation loss: 54, Average loss: 4.0320982403225365\n",
      "Epoch: 2145, Len of Training loss: 18, Average loss: 1.5458350777626038\n",
      "Len of Validation loss: 54, Average loss: 3.9827430932610124\n",
      "Epoch: 2146, Len of Training loss: 18, Average loss: 1.6343253983391657\n",
      "Len of Validation loss: 54, Average loss: 3.8829589817259045\n",
      "Epoch: 2147, Len of Training loss: 18, Average loss: 1.6946614517105951\n",
      "Len of Validation loss: 54, Average loss: 3.60249039861891\n",
      "Epoch: 2148, Len of Training loss: 18, Average loss: 1.546645806895362\n",
      "Len of Validation loss: 54, Average loss: 3.687347847002524\n",
      "Epoch: 2149, Len of Training loss: 18, Average loss: 1.4580271906322904\n",
      "Len of Validation loss: 54, Average loss: 3.780941128730774\n",
      "Epoch: 2150, Len of Training loss: 18, Average loss: 1.4391340480910406\n",
      "Len of Validation loss: 54, Average loss: 3.5809870218789137\n",
      "Epoch: 2151, Len of Training loss: 18, Average loss: 1.3715495533413358\n",
      "Len of Validation loss: 54, Average loss: 3.502614869011773\n",
      "Epoch: 2152, Len of Training loss: 18, Average loss: 1.3503249155150518\n",
      "Len of Validation loss: 54, Average loss: 3.734218096291577\n",
      "Epoch: 2153, Len of Training loss: 18, Average loss: 1.4127375218603346\n",
      "Len of Validation loss: 54, Average loss: 3.7356265981992087\n",
      "Epoch: 2154, Len of Training loss: 18, Average loss: 1.339131944709354\n",
      "Len of Validation loss: 54, Average loss: 3.5141173236899905\n",
      "Epoch: 2155, Len of Training loss: 18, Average loss: 1.3443431324428983\n",
      "Len of Validation loss: 54, Average loss: 3.818241340142709\n",
      "Epoch: 2156, Len of Training loss: 18, Average loss: 1.5167183015081618\n",
      "Len of Validation loss: 54, Average loss: 3.8274948287893227\n",
      "Epoch: 2157, Len of Training loss: 18, Average loss: 1.4162439770168729\n",
      "Len of Validation loss: 54, Average loss: 3.8257033913223832\n",
      "Epoch: 2158, Len of Training loss: 18, Average loss: 1.5163322356012132\n",
      "Len of Validation loss: 54, Average loss: 3.6294078959359064\n",
      "Epoch: 2159, Len of Training loss: 18, Average loss: 1.4358794622951083\n",
      "Len of Validation loss: 54, Average loss: 3.730508150877776\n",
      "Epoch: 2160, Len of Training loss: 18, Average loss: 1.4485611650678847\n",
      "Len of Validation loss: 54, Average loss: 3.995829767651028\n",
      "Epoch: 2161, Len of Training loss: 18, Average loss: 1.793907152281867\n",
      "Len of Validation loss: 54, Average loss: 3.825936891414501\n",
      "Epoch: 2162, Len of Training loss: 18, Average loss: 1.6852112412452698\n",
      "Len of Validation loss: 54, Average loss: 3.891229954030779\n",
      "Epoch: 2163, Len of Training loss: 18, Average loss: 1.6914184424612257\n",
      "Len of Validation loss: 54, Average loss: 3.886553881344972\n",
      "Epoch: 2164, Len of Training loss: 18, Average loss: 1.6652242143948872\n",
      "Len of Validation loss: 54, Average loss: 3.8330318949840687\n",
      "Epoch: 2165, Len of Training loss: 18, Average loss: 1.5025997426774766\n",
      "Len of Validation loss: 54, Average loss: 3.6968124354327165\n",
      "Epoch: 2166, Len of Training loss: 18, Average loss: 1.4701485104031033\n",
      "Len of Validation loss: 54, Average loss: 3.710873594990483\n",
      "Epoch: 2167, Len of Training loss: 18, Average loss: 1.452219307422638\n",
      "Len of Validation loss: 54, Average loss: 3.5938483851927296\n",
      "Epoch: 2168, Len of Training loss: 18, Average loss: 1.51013755136066\n",
      "Len of Validation loss: 54, Average loss: 3.6596312721570334\n",
      "Epoch: 2169, Len of Training loss: 18, Average loss: 1.4846117032898798\n",
      "Len of Validation loss: 54, Average loss: 3.626282367441389\n",
      "Epoch: 2170, Len of Training loss: 18, Average loss: 1.4981472889582317\n",
      "Len of Validation loss: 54, Average loss: 4.002061667265715\n",
      "Epoch: 2171, Len of Training loss: 18, Average loss: 1.7221982412868075\n",
      "Len of Validation loss: 54, Average loss: 3.678791728284624\n",
      "Epoch: 2172, Len of Training loss: 18, Average loss: 1.517303963502248\n",
      "Len of Validation loss: 54, Average loss: 3.7246267376122653\n",
      "Epoch: 2173, Len of Training loss: 18, Average loss: 1.568326731522878\n",
      "Len of Validation loss: 54, Average loss: 3.965174533702709\n",
      "Epoch: 2174, Len of Training loss: 18, Average loss: 1.5039761198891535\n",
      "Len of Validation loss: 54, Average loss: 3.697195337878333\n",
      "Epoch: 2175, Len of Training loss: 18, Average loss: 1.3743816415468852\n",
      "Len of Validation loss: 54, Average loss: 3.6476048672640764\n",
      "Epoch: 2176, Len of Training loss: 18, Average loss: 1.3606713215510051\n",
      "Len of Validation loss: 54, Average loss: 3.6436943809191384\n",
      "Epoch: 2177, Len of Training loss: 18, Average loss: 1.4109910660319858\n",
      "Len of Validation loss: 54, Average loss: 3.556040480181023\n",
      "Epoch: 2178, Len of Training loss: 18, Average loss: 1.3773916959762573\n",
      "Len of Validation loss: 54, Average loss: 3.55919642801638\n",
      "Epoch: 2179, Len of Training loss: 18, Average loss: 1.4547486239009433\n",
      "Len of Validation loss: 54, Average loss: 3.7491774912233704\n",
      "Epoch: 2180, Len of Training loss: 18, Average loss: 1.3978753222359552\n",
      "Len of Validation loss: 54, Average loss: 3.689615346767284\n",
      "Epoch: 2181, Len of Training loss: 18, Average loss: 1.385293636057112\n",
      "Len of Validation loss: 54, Average loss: 3.6428396392751625\n",
      "Epoch: 2182, Len of Training loss: 18, Average loss: 1.305032365851932\n",
      "Len of Validation loss: 54, Average loss: 3.5398221722355596\n",
      "Epoch: 2183, Len of Training loss: 18, Average loss: 1.272433837254842\n",
      "Len of Validation loss: 54, Average loss: 3.748990476131439\n",
      "Epoch: 2184, Len of Training loss: 18, Average loss: 1.3216195238961115\n",
      "Len of Validation loss: 54, Average loss: 3.504476989860888\n",
      "Epoch: 2185, Len of Training loss: 18, Average loss: 1.316015236907535\n",
      "Len of Validation loss: 54, Average loss: 3.4804012046919928\n",
      "Epoch: 2186, Len of Training loss: 18, Average loss: 1.388127366701762\n",
      "Len of Validation loss: 54, Average loss: 3.7618284976040877\n",
      "Epoch: 2187, Len of Training loss: 18, Average loss: 1.5444265140427484\n",
      "Len of Validation loss: 54, Average loss: 3.6167844202783375\n",
      "Epoch: 2188, Len of Training loss: 18, Average loss: 1.467621922492981\n",
      "Len of Validation loss: 54, Average loss: 3.7704394084435924\n",
      "Epoch: 2189, Len of Training loss: 18, Average loss: 1.3714142971568637\n",
      "Len of Validation loss: 54, Average loss: 3.451109254801715\n",
      "Epoch: 2190, Len of Training loss: 18, Average loss: 1.3693306379848056\n",
      "Len of Validation loss: 54, Average loss: 3.753836770852407\n",
      "Epoch: 2191, Len of Training loss: 18, Average loss: 1.4459500180350409\n",
      "Len of Validation loss: 54, Average loss: 3.6466279559665256\n",
      "Epoch: 2192, Len of Training loss: 18, Average loss: 1.361287858751085\n",
      "Len of Validation loss: 54, Average loss: 3.4863316637498363\n",
      "Epoch: 2193, Len of Training loss: 18, Average loss: 1.448925322956509\n",
      "Len of Validation loss: 54, Average loss: 3.6944532659318714\n",
      "Epoch: 2194, Len of Training loss: 18, Average loss: 1.4401369161076016\n",
      "Len of Validation loss: 54, Average loss: 4.028530268757431\n",
      "Epoch: 2195, Len of Training loss: 18, Average loss: 1.8453818294737074\n",
      "Len of Validation loss: 54, Average loss: 3.8979254055906227\n",
      "Epoch: 2196, Len of Training loss: 18, Average loss: 1.836845921145545\n",
      "Len of Validation loss: 54, Average loss: 3.8677340039500483\n",
      "Epoch: 2197, Len of Training loss: 18, Average loss: 1.6121668815612793\n",
      "Len of Validation loss: 54, Average loss: 3.7304544272246183\n",
      "Epoch: 2198, Len of Training loss: 18, Average loss: 1.5063765512572393\n",
      "Len of Validation loss: 54, Average loss: 3.597577238524402\n",
      "Epoch: 2199, Len of Training loss: 18, Average loss: 1.464059763484531\n",
      "Len of Validation loss: 54, Average loss: 3.609483568756669\n",
      "Epoch: 2200, Len of Training loss: 18, Average loss: 1.4161834054523044\n",
      "Len of Validation loss: 54, Average loss: 3.599474041550248\n",
      "Epoch: 2201, Len of Training loss: 18, Average loss: 1.5160127547052171\n",
      "Len of Validation loss: 54, Average loss: 3.864485506658201\n",
      "Epoch: 2202, Len of Training loss: 18, Average loss: 1.6089820331997342\n",
      "Len of Validation loss: 54, Average loss: 3.9150381551848517\n",
      "Epoch: 2203, Len of Training loss: 18, Average loss: 1.5778451297018263\n",
      "Len of Validation loss: 54, Average loss: 3.873421340077012\n",
      "Epoch: 2204, Len of Training loss: 18, Average loss: 1.5460566348499722\n",
      "Len of Validation loss: 54, Average loss: 3.7842937707901\n",
      "Epoch: 2205, Len of Training loss: 18, Average loss: 1.571255538198683\n",
      "Len of Validation loss: 54, Average loss: 4.239431802873258\n",
      "Epoch: 2206, Len of Training loss: 18, Average loss: 1.9523827301131353\n",
      "Len of Validation loss: 54, Average loss: 3.7039694057570562\n",
      "Epoch: 2207, Len of Training loss: 18, Average loss: 1.697265452808804\n",
      "Len of Validation loss: 54, Average loss: 3.796067284213172\n",
      "Epoch: 2208, Len of Training loss: 18, Average loss: 1.553471479150984\n",
      "Len of Validation loss: 54, Average loss: 3.834827902140441\n",
      "Epoch: 2209, Len of Training loss: 18, Average loss: 1.7596611910396152\n",
      "Len of Validation loss: 54, Average loss: 3.7791259112181486\n",
      "Epoch: 2210, Len of Training loss: 18, Average loss: 1.6384914252493117\n",
      "Len of Validation loss: 54, Average loss: 3.680905389565009\n",
      "Epoch: 2211, Len of Training loss: 18, Average loss: 1.5519664221339755\n",
      "Len of Validation loss: 54, Average loss: 3.753245547965721\n",
      "Epoch: 2212, Len of Training loss: 18, Average loss: 1.5754639638794794\n",
      "Len of Validation loss: 54, Average loss: 3.8214809232287936\n",
      "Epoch: 2213, Len of Training loss: 18, Average loss: 1.525485376516978\n",
      "Len of Validation loss: 54, Average loss: 3.8071495294570923\n",
      "Epoch: 2214, Len of Training loss: 18, Average loss: 1.507912940449185\n",
      "Len of Validation loss: 54, Average loss: 3.610363068404021\n",
      "Epoch: 2215, Len of Training loss: 18, Average loss: 1.3148763709598117\n",
      "Len of Validation loss: 54, Average loss: 3.647471411360635\n",
      "Epoch: 2216, Len of Training loss: 18, Average loss: 1.3176592853334215\n",
      "Len of Validation loss: 54, Average loss: 3.856588512659073\n",
      "Epoch: 2217, Len of Training loss: 18, Average loss: 1.4712854756249323\n",
      "Len of Validation loss: 54, Average loss: 3.5199860864215426\n",
      "Epoch: 2218, Len of Training loss: 18, Average loss: 1.5661631756358676\n",
      "Len of Validation loss: 54, Average loss: 3.7351320805373014\n",
      "Epoch: 2219, Len of Training loss: 18, Average loss: 1.4839407801628113\n",
      "Len of Validation loss: 54, Average loss: 3.7542925344573126\n",
      "Epoch: 2220, Len of Training loss: 18, Average loss: 1.3946710758739047\n",
      "Len of Validation loss: 54, Average loss: 3.665740057274147\n",
      "Epoch: 2221, Len of Training loss: 18, Average loss: 1.3822224272622003\n",
      "Len of Validation loss: 54, Average loss: 3.6296975160086595\n",
      "Epoch: 2222, Len of Training loss: 18, Average loss: 1.3951216869884067\n",
      "Len of Validation loss: 54, Average loss: 3.8424928144172386\n",
      "Epoch: 2223, Len of Training loss: 18, Average loss: 1.3381018903520372\n",
      "Len of Validation loss: 54, Average loss: 3.6675940763067314\n",
      "Epoch: 2224, Len of Training loss: 18, Average loss: 1.3389742308192782\n",
      "Len of Validation loss: 54, Average loss: 3.646519605760221\n",
      "Epoch: 2225, Len of Training loss: 18, Average loss: 1.3110895487997267\n",
      "Len of Validation loss: 54, Average loss: 3.6529099764647306\n",
      "Epoch: 2226, Len of Training loss: 18, Average loss: 1.2898315787315369\n",
      "Len of Validation loss: 54, Average loss: 3.4880445456063307\n",
      "Epoch: 2227, Len of Training loss: 18, Average loss: 1.251948078473409\n",
      "Len of Validation loss: 54, Average loss: 3.598005819099921\n",
      "Epoch: 2228, Len of Training loss: 18, Average loss: 1.2950254678726196\n",
      "Len of Validation loss: 54, Average loss: 3.750198381918448\n",
      "Epoch: 2229, Len of Training loss: 18, Average loss: 1.338953779803382\n",
      "Len of Validation loss: 54, Average loss: 3.6493973820297807\n",
      "Epoch: 2230, Len of Training loss: 18, Average loss: 1.4653953512509663\n",
      "Len of Validation loss: 54, Average loss: 3.689267979727851\n",
      "Epoch: 2231, Len of Training loss: 18, Average loss: 1.7033165494600933\n",
      "Len of Validation loss: 54, Average loss: 3.8927600494137518\n",
      "Epoch: 2232, Len of Training loss: 18, Average loss: 1.6535955137676663\n",
      "Len of Validation loss: 54, Average loss: 3.9456429613961115\n",
      "Epoch: 2233, Len of Training loss: 18, Average loss: 1.5431575775146484\n",
      "Len of Validation loss: 54, Average loss: 4.098164392842187\n",
      "Epoch: 2234, Len of Training loss: 18, Average loss: 1.636393407980601\n",
      "Len of Validation loss: 54, Average loss: 3.6836753907027067\n",
      "Epoch: 2235, Len of Training loss: 18, Average loss: 1.4165072043736775\n",
      "Len of Validation loss: 54, Average loss: 3.7543869702904313\n",
      "Epoch: 2236, Len of Training loss: 18, Average loss: 1.3355834020508661\n",
      "Len of Validation loss: 54, Average loss: 3.6754857390015214\n",
      "Epoch: 2237, Len of Training loss: 18, Average loss: 1.279812965128157\n",
      "Len of Validation loss: 54, Average loss: 3.6053233764789723\n",
      "Epoch: 2238, Len of Training loss: 18, Average loss: 1.2639693948957655\n",
      "Len of Validation loss: 54, Average loss: 3.7193743343706482\n",
      "Epoch: 2239, Len of Training loss: 18, Average loss: 1.7128370602925618\n",
      "Len of Validation loss: 54, Average loss: 4.109608519960333\n",
      "Epoch: 2240, Len of Training loss: 18, Average loss: 1.6710907816886902\n",
      "Len of Validation loss: 54, Average loss: 3.5733929011556835\n",
      "Epoch: 2241, Len of Training loss: 18, Average loss: 1.4804719024234347\n",
      "Len of Validation loss: 54, Average loss: 3.5154105866396868\n",
      "Epoch: 2242, Len of Training loss: 18, Average loss: 1.3997702797253926\n",
      "Len of Validation loss: 54, Average loss: 3.676045368115107\n",
      "Epoch: 2243, Len of Training loss: 18, Average loss: 1.3607824113633897\n",
      "Len of Validation loss: 54, Average loss: 3.5670890366589583\n",
      "Epoch: 2244, Len of Training loss: 18, Average loss: 1.462691558731927\n",
      "Len of Validation loss: 54, Average loss: 3.777601089742449\n",
      "Epoch: 2245, Len of Training loss: 18, Average loss: 1.6231473551856146\n",
      "Len of Validation loss: 54, Average loss: 3.7500263761591026\n",
      "Epoch: 2246, Len of Training loss: 18, Average loss: 1.5078415208392673\n",
      "Len of Validation loss: 54, Average loss: 3.7098156633200468\n",
      "Epoch: 2247, Len of Training loss: 18, Average loss: 1.3803807629479303\n",
      "Len of Validation loss: 54, Average loss: 3.6065583008306996\n",
      "Epoch: 2248, Len of Training loss: 18, Average loss: 1.3976612355973985\n",
      "Len of Validation loss: 54, Average loss: 3.6431953973240323\n",
      "Epoch: 2249, Len of Training loss: 18, Average loss: 1.5229300061861675\n",
      "Len of Validation loss: 54, Average loss: 3.479989743895001\n",
      "Epoch: 2250, Len of Training loss: 18, Average loss: 1.3400067885716755\n",
      "Len of Validation loss: 54, Average loss: 3.575695421960619\n",
      "Epoch: 2251, Len of Training loss: 18, Average loss: 1.4147501985232036\n",
      "Len of Validation loss: 54, Average loss: 3.6788079562010587\n",
      "Epoch: 2252, Len of Training loss: 18, Average loss: 1.4013204640812345\n",
      "Len of Validation loss: 54, Average loss: 3.8504102252147816\n",
      "Epoch: 2253, Len of Training loss: 18, Average loss: 1.3645493785540264\n",
      "Len of Validation loss: 54, Average loss: 3.6772998527244285\n",
      "Epoch: 2254, Len of Training loss: 18, Average loss: 1.3237621717982822\n",
      "Len of Validation loss: 54, Average loss: 3.5091417056542857\n",
      "Epoch: 2255, Len of Training loss: 18, Average loss: 1.3761753373675876\n",
      "Len of Validation loss: 54, Average loss: 3.7738701855694807\n",
      "Epoch: 2256, Len of Training loss: 18, Average loss: 1.3569943971104093\n",
      "Len of Validation loss: 54, Average loss: 3.5030473360308894\n",
      "Epoch: 2257, Len of Training loss: 18, Average loss: 1.2800668544239469\n",
      "Len of Validation loss: 54, Average loss: 3.590266736569228\n",
      "Epoch: 2258, Len of Training loss: 18, Average loss: 1.279948393503825\n",
      "Len of Validation loss: 54, Average loss: 3.637918363014857\n",
      "Epoch: 2259, Len of Training loss: 18, Average loss: 1.3010348743862576\n",
      "Len of Validation loss: 54, Average loss: 3.534666233592563\n",
      "Epoch: 2260, Len of Training loss: 18, Average loss: 1.3577840328216553\n",
      "Len of Validation loss: 54, Average loss: 3.619705986093592\n",
      "Epoch: 2261, Len of Training loss: 18, Average loss: 1.376731018225352\n",
      "Len of Validation loss: 54, Average loss: 3.485167757228569\n",
      "Epoch: 2262, Len of Training loss: 18, Average loss: 1.3356098996268377\n",
      "Len of Validation loss: 54, Average loss: 3.659635649787055\n",
      "Epoch: 2263, Len of Training loss: 18, Average loss: 1.4536446399158902\n",
      "Len of Validation loss: 54, Average loss: 3.7002141652283846\n",
      "Epoch: 2264, Len of Training loss: 18, Average loss: 1.4945736394988165\n",
      "Len of Validation loss: 54, Average loss: 3.630827813236802\n",
      "Epoch: 2265, Len of Training loss: 18, Average loss: 1.4733481076028612\n",
      "Len of Validation loss: 54, Average loss: 4.029731108082665\n",
      "Epoch: 2266, Len of Training loss: 18, Average loss: 1.611410207218594\n",
      "Len of Validation loss: 54, Average loss: 3.6137366979210466\n",
      "Epoch: 2267, Len of Training loss: 18, Average loss: 1.52420088979933\n",
      "Len of Validation loss: 54, Average loss: 3.7995654145876565\n",
      "Epoch: 2268, Len of Training loss: 18, Average loss: 1.5050391687287226\n",
      "Len of Validation loss: 54, Average loss: 3.6187749858255738\n",
      "Epoch: 2269, Len of Training loss: 18, Average loss: 1.420535061094496\n",
      "Len of Validation loss: 54, Average loss: 3.651568539716579\n",
      "Epoch: 2270, Len of Training loss: 18, Average loss: 1.4861486819055345\n",
      "Len of Validation loss: 54, Average loss: 3.8713753400025546\n",
      "Epoch: 2271, Len of Training loss: 18, Average loss: 1.5684807300567627\n",
      "Len of Validation loss: 54, Average loss: 3.4610140411942094\n",
      "Epoch: 2272, Len of Training loss: 18, Average loss: 1.4387305511368647\n",
      "Len of Validation loss: 54, Average loss: 3.5460062711327165\n",
      "Epoch: 2273, Len of Training loss: 18, Average loss: 1.395773020055559\n",
      "Len of Validation loss: 54, Average loss: 3.5412390574260995\n",
      "Epoch: 2274, Len of Training loss: 18, Average loss: 1.5037734707196553\n",
      "Len of Validation loss: 54, Average loss: 3.798339459631178\n",
      "Epoch: 2275, Len of Training loss: 18, Average loss: 1.4000449909104242\n",
      "Len of Validation loss: 54, Average loss: 3.5838525096575418\n",
      "Epoch: 2276, Len of Training loss: 18, Average loss: 1.3376652201016743\n",
      "Len of Validation loss: 54, Average loss: 3.716422681455259\n",
      "Epoch: 2277, Len of Training loss: 18, Average loss: 1.5524534848001268\n",
      "Len of Validation loss: 54, Average loss: 3.9689022059793824\n",
      "Epoch: 2278, Len of Training loss: 18, Average loss: 1.583510524696774\n",
      "Len of Validation loss: 54, Average loss: 3.7907324808615224\n",
      "Epoch: 2279, Len of Training loss: 18, Average loss: 1.545508046944936\n",
      "Len of Validation loss: 54, Average loss: 3.7049745519955954\n",
      "Epoch: 2280, Len of Training loss: 18, Average loss: 1.3664834830496047\n",
      "Len of Validation loss: 54, Average loss: 3.5171503435682365\n",
      "Epoch: 2281, Len of Training loss: 18, Average loss: 1.2621655331717596\n",
      "Len of Validation loss: 54, Average loss: 3.499455210235384\n",
      "Epoch: 2282, Len of Training loss: 18, Average loss: 1.2255122860272725\n",
      "Len of Validation loss: 54, Average loss: 3.5171261418748787\n",
      "Epoch: 2283, Len of Training loss: 18, Average loss: 1.2383605440457661\n",
      "Len of Validation loss: 54, Average loss: 3.5483695423161543\n",
      "Epoch: 2284, Len of Training loss: 18, Average loss: 1.26215398311615\n",
      "Len of Validation loss: 54, Average loss: 3.6101694526495756\n",
      "Epoch: 2285, Len of Training loss: 18, Average loss: 1.2438021434677973\n",
      "Len of Validation loss: 54, Average loss: 3.384760045342975\n",
      "Epoch: 2286, Len of Training loss: 18, Average loss: 1.251255836751726\n",
      "Len of Validation loss: 54, Average loss: 4.048426948211811\n",
      "Epoch: 2287, Len of Training loss: 18, Average loss: 1.6472903026474848\n",
      "Len of Validation loss: 54, Average loss: 3.7354032419345997\n",
      "Epoch: 2288, Len of Training loss: 18, Average loss: 1.5713179508845012\n",
      "Len of Validation loss: 54, Average loss: 3.8406420162430517\n",
      "Epoch: 2289, Len of Training loss: 18, Average loss: 1.7080619931221008\n",
      "Len of Validation loss: 54, Average loss: 3.9142555550292686\n",
      "Epoch: 2290, Len of Training loss: 18, Average loss: 1.6137512193785772\n",
      "Len of Validation loss: 54, Average loss: 3.7782283933074385\n",
      "Epoch: 2291, Len of Training loss: 18, Average loss: 1.6826188034481473\n",
      "Len of Validation loss: 54, Average loss: 3.7990586426523\n",
      "Epoch: 2292, Len of Training loss: 18, Average loss: 1.5613974796401129\n",
      "Len of Validation loss: 54, Average loss: 3.8615418208969965\n",
      "Epoch: 2293, Len of Training loss: 18, Average loss: 1.391606072584788\n",
      "Len of Validation loss: 54, Average loss: 3.525557824858913\n",
      "Epoch: 2294, Len of Training loss: 18, Average loss: 1.3440006573994954\n",
      "Len of Validation loss: 54, Average loss: 3.66709123827793\n",
      "Epoch: 2295, Len of Training loss: 18, Average loss: 1.3655655913882785\n",
      "Len of Validation loss: 54, Average loss: 3.8252465581452406\n",
      "Epoch: 2296, Len of Training loss: 18, Average loss: 1.3172865642441645\n",
      "Len of Validation loss: 54, Average loss: 3.9494969259809563\n",
      "Epoch: 2297, Len of Training loss: 18, Average loss: 1.30824038055208\n",
      "Len of Validation loss: 54, Average loss: 3.4992835367167436\n",
      "Epoch: 2298, Len of Training loss: 18, Average loss: 1.350949227809906\n",
      "Len of Validation loss: 54, Average loss: 3.701727204852634\n",
      "Epoch: 2299, Len of Training loss: 18, Average loss: 1.515418807665507\n",
      "Len of Validation loss: 54, Average loss: 4.017654902405209\n",
      "Epoch: 2300, Len of Training loss: 18, Average loss: 1.580676840411292\n",
      "Len of Validation loss: 54, Average loss: 3.947698661574611\n",
      "Epoch: 2301, Len of Training loss: 18, Average loss: 1.4935844010776944\n",
      "Len of Validation loss: 54, Average loss: 3.9073347162317345\n",
      "Epoch: 2302, Len of Training loss: 18, Average loss: 1.4883298807673984\n",
      "Len of Validation loss: 54, Average loss: 3.5727303701418416\n",
      "Epoch: 2303, Len of Training loss: 18, Average loss: 1.5580562816725836\n",
      "Len of Validation loss: 54, Average loss: 3.6380356130776583\n",
      "Epoch: 2304, Len of Training loss: 18, Average loss: 1.6556331581539578\n",
      "Len of Validation loss: 54, Average loss: 3.6156803833113775\n",
      "Epoch: 2305, Len of Training loss: 18, Average loss: 1.6484791967603896\n",
      "Len of Validation loss: 54, Average loss: 3.7909796635309854\n",
      "Epoch: 2306, Len of Training loss: 18, Average loss: 1.443118393421173\n",
      "Len of Validation loss: 54, Average loss: 3.600945892157378\n",
      "Epoch: 2307, Len of Training loss: 18, Average loss: 1.329756173822615\n",
      "Len of Validation loss: 54, Average loss: 3.8165846687776073\n",
      "Epoch: 2308, Len of Training loss: 18, Average loss: 1.4134763942824469\n",
      "Len of Validation loss: 54, Average loss: 3.678230387193185\n",
      "Epoch: 2309, Len of Training loss: 18, Average loss: 1.4174289372232225\n",
      "Len of Validation loss: 54, Average loss: 3.567145020873458\n",
      "Epoch: 2310, Len of Training loss: 18, Average loss: 1.3726952009730868\n",
      "Len of Validation loss: 54, Average loss: 3.855477310993053\n",
      "Epoch: 2311, Len of Training loss: 18, Average loss: 1.4162937535179987\n",
      "Len of Validation loss: 54, Average loss: 3.6220083402262793\n",
      "Epoch: 2312, Len of Training loss: 18, Average loss: 1.5148762861887615\n",
      "Len of Validation loss: 54, Average loss: 3.943834256242823\n",
      "Epoch: 2313, Len of Training loss: 18, Average loss: 1.6157708764076233\n",
      "Len of Validation loss: 54, Average loss: 3.5882593680311134\n",
      "Epoch: 2314, Len of Training loss: 18, Average loss: 1.4712294273906283\n",
      "Len of Validation loss: 54, Average loss: 3.515838631877193\n",
      "Epoch: 2315, Len of Training loss: 18, Average loss: 1.4238636559910245\n",
      "Len of Validation loss: 54, Average loss: 3.710502215005733\n",
      "Epoch: 2316, Len of Training loss: 18, Average loss: 1.3278345863024394\n",
      "Len of Validation loss: 54, Average loss: 3.545878701739841\n",
      "Epoch: 2317, Len of Training loss: 18, Average loss: 1.259010374546051\n",
      "Len of Validation loss: 54, Average loss: 3.68826037534961\n",
      "Epoch: 2318, Len of Training loss: 18, Average loss: 1.221620089477963\n",
      "Len of Validation loss: 54, Average loss: 3.6337676688476845\n",
      "Epoch: 2319, Len of Training loss: 18, Average loss: 1.2299346062872145\n",
      "Len of Validation loss: 54, Average loss: 3.8824909848195537\n",
      "Epoch: 2320, Len of Training loss: 18, Average loss: 1.2416085203488667\n",
      "Len of Validation loss: 54, Average loss: 3.495198024643792\n",
      "Epoch: 2321, Len of Training loss: 18, Average loss: 1.2650656501452129\n",
      "Len of Validation loss: 54, Average loss: 3.692984476133629\n",
      "Epoch: 2322, Len of Training loss: 18, Average loss: 1.222067740228441\n",
      "Len of Validation loss: 54, Average loss: 3.5115419173682176\n",
      "Epoch: 2323, Len of Training loss: 18, Average loss: 1.2768654227256775\n",
      "Len of Validation loss: 54, Average loss: 3.7109189651630543\n",
      "Epoch: 2324, Len of Training loss: 18, Average loss: 1.4229337970415752\n",
      "Len of Validation loss: 54, Average loss: 3.9046191484839827\n",
      "Epoch: 2325, Len of Training loss: 18, Average loss: 1.4423894418610468\n",
      "Len of Validation loss: 54, Average loss: 3.6988139748573303\n",
      "Epoch: 2326, Len of Training loss: 18, Average loss: 1.4452256229188707\n",
      "Len of Validation loss: 54, Average loss: 3.6456110344992743\n",
      "Epoch: 2327, Len of Training loss: 18, Average loss: 1.3496284286181133\n",
      "Len of Validation loss: 54, Average loss: 3.6197568818374917\n",
      "Epoch: 2328, Len of Training loss: 18, Average loss: 1.381836990515391\n",
      "Len of Validation loss: 54, Average loss: 3.8363805501549333\n",
      "Epoch: 2329, Len of Training loss: 18, Average loss: 1.529129511780209\n",
      "Len of Validation loss: 54, Average loss: 3.81784298464104\n",
      "Epoch: 2330, Len of Training loss: 18, Average loss: 1.613872441980574\n",
      "Len of Validation loss: 54, Average loss: 4.1042303023514926\n",
      "Epoch: 2331, Len of Training loss: 18, Average loss: 1.478308187590705\n",
      "Len of Validation loss: 54, Average loss: 3.625603664804388\n",
      "Epoch: 2332, Len of Training loss: 18, Average loss: 1.9219643274943035\n",
      "Len of Validation loss: 54, Average loss: 3.8874983456399708\n",
      "Epoch: 2333, Len of Training loss: 18, Average loss: 1.9120902220408122\n",
      "Len of Validation loss: 54, Average loss: 3.9398184109617165\n",
      "Epoch: 2334, Len of Training loss: 18, Average loss: 1.7156213058365717\n",
      "Len of Validation loss: 54, Average loss: 3.7323184068556183\n",
      "Epoch: 2335, Len of Training loss: 18, Average loss: 1.589362429247962\n",
      "Len of Validation loss: 54, Average loss: 3.6436768527384156\n",
      "Epoch: 2336, Len of Training loss: 18, Average loss: 1.570560442076789\n",
      "Len of Validation loss: 54, Average loss: 4.1173904383624045\n",
      "Epoch: 2337, Len of Training loss: 18, Average loss: 1.7199140389760335\n",
      "Len of Validation loss: 54, Average loss: 4.033588820033604\n",
      "Epoch: 2338, Len of Training loss: 18, Average loss: 1.9525603387090895\n",
      "Len of Validation loss: 54, Average loss: 3.9756722229498402\n",
      "Epoch: 2339, Len of Training loss: 18, Average loss: 1.8356518083148532\n",
      "Len of Validation loss: 54, Average loss: 3.9632580081621804\n",
      "Epoch: 2340, Len of Training loss: 18, Average loss: 4.889218303892347\n",
      "Len of Validation loss: 54, Average loss: 4.969404520811858\n",
      "Epoch: 2341, Len of Training loss: 18, Average loss: 2.980611973338657\n",
      "Len of Validation loss: 54, Average loss: 4.165443592601353\n",
      "Epoch: 2342, Len of Training loss: 18, Average loss: 2.331795394420624\n",
      "Len of Validation loss: 54, Average loss: 4.093452460236019\n",
      "Epoch: 2343, Len of Training loss: 18, Average loss: 1.8815820945633783\n",
      "Len of Validation loss: 54, Average loss: 3.761458648575677\n",
      "Epoch: 2344, Len of Training loss: 18, Average loss: 1.7538699640168085\n",
      "Len of Validation loss: 54, Average loss: 3.6799668934610157\n",
      "Epoch: 2345, Len of Training loss: 18, Average loss: 1.6339480015966628\n",
      "Len of Validation loss: 54, Average loss: 3.734315276145935\n",
      "Epoch: 2346, Len of Training loss: 18, Average loss: 1.713373773627811\n",
      "Len of Validation loss: 54, Average loss: 3.675156396848184\n",
      "Epoch: 2347, Len of Training loss: 18, Average loss: 1.6345197624630399\n",
      "Len of Validation loss: 54, Average loss: 3.7529215260788247\n",
      "Epoch: 2348, Len of Training loss: 18, Average loss: 1.4698453810479906\n",
      "Len of Validation loss: 54, Average loss: 3.884870922123944\n",
      "Epoch: 2349, Len of Training loss: 18, Average loss: 1.6275327735477023\n",
      "Len of Validation loss: 54, Average loss: 3.8724433183670044\n",
      "Epoch: 2350, Len of Training loss: 18, Average loss: 1.6083687278959486\n",
      "Len of Validation loss: 54, Average loss: 3.9102317448015564\n",
      "Epoch: 2351, Len of Training loss: 18, Average loss: 1.4239394201172724\n",
      "Len of Validation loss: 54, Average loss: 3.5677398906813726\n",
      "Epoch: 2352, Len of Training loss: 18, Average loss: 1.3191092014312744\n",
      "Len of Validation loss: 54, Average loss: 3.6689555137245744\n",
      "Epoch: 2353, Len of Training loss: 18, Average loss: 1.3000461591614618\n",
      "Len of Validation loss: 54, Average loss: 3.775338656372494\n",
      "Epoch: 2354, Len of Training loss: 18, Average loss: 1.2662065360281203\n",
      "Len of Validation loss: 54, Average loss: 3.6784236055833324\n",
      "Epoch: 2355, Len of Training loss: 18, Average loss: 1.3050072060690985\n",
      "Len of Validation loss: 54, Average loss: 3.4439136617713504\n",
      "Epoch: 2356, Len of Training loss: 18, Average loss: 1.2767950826221042\n",
      "Len of Validation loss: 54, Average loss: 3.5850828764615237\n",
      "Epoch: 2357, Len of Training loss: 18, Average loss: 1.270275314648946\n",
      "Len of Validation loss: 54, Average loss: 3.6002173015364893\n",
      "Epoch: 2358, Len of Training loss: 18, Average loss: 1.2989417049619887\n",
      "Len of Validation loss: 54, Average loss: 3.597714484841735\n",
      "Epoch: 2359, Len of Training loss: 18, Average loss: 1.3698336879412334\n",
      "Len of Validation loss: 54, Average loss: 3.841916845904456\n",
      "Epoch: 2360, Len of Training loss: 18, Average loss: 1.4858138428794012\n",
      "Len of Validation loss: 54, Average loss: 3.8561820255385504\n",
      "Epoch: 2361, Len of Training loss: 18, Average loss: 1.8484327461984422\n",
      "Len of Validation loss: 54, Average loss: 3.783817871853157\n",
      "Epoch: 2362, Len of Training loss: 18, Average loss: 1.6742865641911824\n",
      "Len of Validation loss: 54, Average loss: 3.680042494226385\n",
      "Epoch: 2363, Len of Training loss: 18, Average loss: 1.5138377414809332\n",
      "Len of Validation loss: 54, Average loss: 3.6875462841104576\n",
      "Epoch: 2364, Len of Training loss: 18, Average loss: 1.4184258845117357\n",
      "Len of Validation loss: 54, Average loss: 3.6810026080520064\n",
      "Epoch: 2365, Len of Training loss: 18, Average loss: 1.3561196525891621\n",
      "Len of Validation loss: 54, Average loss: 3.7418944548677513\n",
      "Epoch: 2366, Len of Training loss: 18, Average loss: 1.4063717391755846\n",
      "Len of Validation loss: 54, Average loss: 3.6291835970348783\n",
      "Epoch: 2367, Len of Training loss: 18, Average loss: 1.484840664598677\n",
      "Len of Validation loss: 54, Average loss: 3.7062211919713905\n",
      "Epoch: 2368, Len of Training loss: 18, Average loss: 1.554546336332957\n",
      "Len of Validation loss: 54, Average loss: 3.6197752709741944\n",
      "Epoch: 2369, Len of Training loss: 18, Average loss: 1.5080261098013983\n",
      "Len of Validation loss: 54, Average loss: 3.7186023703327886\n",
      "Epoch: 2370, Len of Training loss: 18, Average loss: 1.5015648537211947\n",
      "Len of Validation loss: 54, Average loss: 3.754045531705574\n",
      "Epoch: 2371, Len of Training loss: 18, Average loss: 1.3877826664182875\n",
      "Len of Validation loss: 54, Average loss: 3.7322911929201195\n",
      "Epoch: 2372, Len of Training loss: 18, Average loss: 1.3468770715925429\n",
      "Len of Validation loss: 54, Average loss: 3.6276428235901728\n",
      "Epoch: 2373, Len of Training loss: 18, Average loss: 1.2987546192275152\n",
      "Len of Validation loss: 54, Average loss: 3.5653230536867073\n",
      "Epoch: 2374, Len of Training loss: 18, Average loss: 1.4026562372843425\n",
      "Len of Validation loss: 54, Average loss: 3.596787002351549\n",
      "Epoch: 2375, Len of Training loss: 18, Average loss: 1.4640019602245755\n",
      "Len of Validation loss: 54, Average loss: 3.5797532576101796\n",
      "Epoch: 2376, Len of Training loss: 18, Average loss: 1.3966200947761536\n",
      "Len of Validation loss: 54, Average loss: 3.649111098713345\n",
      "Epoch: 2377, Len of Training loss: 18, Average loss: 1.3096288508839078\n",
      "Len of Validation loss: 54, Average loss: 3.72351817952262\n",
      "Epoch: 2378, Len of Training loss: 18, Average loss: 1.5617211129930284\n",
      "Len of Validation loss: 54, Average loss: 3.8679649277969643\n",
      "Epoch: 2379, Len of Training loss: 18, Average loss: 1.4289190239376492\n",
      "Len of Validation loss: 54, Average loss: 3.6852771827468165\n",
      "Epoch: 2380, Len of Training loss: 18, Average loss: 1.403038673930698\n",
      "Len of Validation loss: 54, Average loss: 3.7830371503476745\n",
      "Epoch: 2381, Len of Training loss: 18, Average loss: 1.331312616666158\n",
      "Len of Validation loss: 54, Average loss: 3.5673752934844405\n",
      "Epoch: 2382, Len of Training loss: 18, Average loss: 1.3056380086474948\n",
      "Len of Validation loss: 54, Average loss: 3.597306741608514\n",
      "Epoch: 2383, Len of Training loss: 18, Average loss: 1.3218213121096294\n",
      "Len of Validation loss: 54, Average loss: 3.558543030862455\n",
      "Epoch: 2384, Len of Training loss: 18, Average loss: 1.3078175452020433\n",
      "Len of Validation loss: 54, Average loss: 3.656017905032193\n",
      "Epoch: 2385, Len of Training loss: 18, Average loss: 1.3342821134461298\n",
      "Len of Validation loss: 54, Average loss: 3.530089925836634\n",
      "Epoch: 2386, Len of Training loss: 18, Average loss: 1.2841155131657918\n",
      "Len of Validation loss: 54, Average loss: 3.4845541097499706\n",
      "Epoch: 2387, Len of Training loss: 18, Average loss: 1.3608721958266363\n",
      "Len of Validation loss: 54, Average loss: 3.8363068125866078\n",
      "Epoch: 2388, Len of Training loss: 18, Average loss: 1.4378917349709406\n",
      "Len of Validation loss: 54, Average loss: 3.8655926750765905\n",
      "Epoch: 2389, Len of Training loss: 18, Average loss: 1.3232139150301616\n",
      "Len of Validation loss: 54, Average loss: 3.7432336619606725\n",
      "Epoch: 2390, Len of Training loss: 18, Average loss: 1.329124391078949\n",
      "Len of Validation loss: 54, Average loss: 3.6659476580443204\n",
      "Epoch: 2391, Len of Training loss: 18, Average loss: 1.3820210960176256\n",
      "Len of Validation loss: 54, Average loss: 3.5744232734044394\n",
      "Epoch: 2392, Len of Training loss: 18, Average loss: 1.5095360146628485\n",
      "Len of Validation loss: 54, Average loss: 3.965738781072475\n",
      "Epoch: 2393, Len of Training loss: 18, Average loss: 1.5857043067614238\n",
      "Len of Validation loss: 54, Average loss: 3.529598973415516\n",
      "Epoch: 2394, Len of Training loss: 18, Average loss: 1.3919505344496832\n",
      "Len of Validation loss: 54, Average loss: 3.721227107224641\n",
      "Epoch: 2395, Len of Training loss: 18, Average loss: 1.301531235376994\n",
      "Len of Validation loss: 54, Average loss: 3.5318530367480383\n",
      "Epoch: 2396, Len of Training loss: 18, Average loss: 1.350871053006914\n",
      "Len of Validation loss: 54, Average loss: 3.7093823662510625\n",
      "Epoch: 2397, Len of Training loss: 18, Average loss: 1.362546232011583\n",
      "Len of Validation loss: 54, Average loss: 3.5283968073350414\n",
      "Epoch: 2398, Len of Training loss: 18, Average loss: 1.4241048561202154\n",
      "Len of Validation loss: 54, Average loss: 3.7112899488872952\n",
      "Epoch: 2399, Len of Training loss: 18, Average loss: 1.4054659671253629\n",
      "Len of Validation loss: 54, Average loss: 3.6686586516874806\n",
      "Epoch: 2400, Len of Training loss: 18, Average loss: 1.441274835003747\n",
      "Len of Validation loss: 54, Average loss: 3.6825946238305836\n",
      "Epoch: 2401, Len of Training loss: 18, Average loss: 1.4193878968556721\n",
      "Len of Validation loss: 54, Average loss: 3.606463019494657\n",
      "Epoch: 2402, Len of Training loss: 18, Average loss: 1.4249671167797513\n",
      "Len of Validation loss: 54, Average loss: 3.592139123766511\n",
      "Epoch: 2403, Len of Training loss: 18, Average loss: 1.3902958035469055\n",
      "Len of Validation loss: 54, Average loss: 3.9286850370742656\n",
      "Epoch: 2404, Len of Training loss: 18, Average loss: 1.477389521068997\n",
      "Len of Validation loss: 54, Average loss: 3.7505184698987892\n",
      "Epoch: 2405, Len of Training loss: 18, Average loss: 1.5186895728111267\n",
      "Len of Validation loss: 54, Average loss: 3.6295484216124922\n",
      "Epoch: 2406, Len of Training loss: 18, Average loss: 1.4196455743577745\n",
      "Len of Validation loss: 54, Average loss: 3.6366594124723366\n",
      "Epoch: 2407, Len of Training loss: 18, Average loss: 1.511898246076372\n",
      "Len of Validation loss: 54, Average loss: 3.636352757612864\n",
      "Epoch: 2408, Len of Training loss: 18, Average loss: 1.5450699859195285\n",
      "Len of Validation loss: 54, Average loss: 3.5782796012030706\n",
      "Epoch: 2409, Len of Training loss: 18, Average loss: 1.5054362548722162\n",
      "Len of Validation loss: 54, Average loss: 3.834444405855956\n",
      "Epoch: 2410, Len of Training loss: 18, Average loss: 1.389250463909573\n",
      "Len of Validation loss: 54, Average loss: 3.450037597506135\n",
      "Epoch: 2411, Len of Training loss: 18, Average loss: 1.366857773727841\n",
      "Len of Validation loss: 54, Average loss: 3.7681917581293316\n",
      "Epoch: 2412, Len of Training loss: 18, Average loss: 1.5141997403568692\n",
      "Len of Validation loss: 54, Average loss: 3.577408680209407\n",
      "Epoch: 2413, Len of Training loss: 18, Average loss: 1.6039373278617859\n",
      "Len of Validation loss: 54, Average loss: 3.655656472400383\n",
      "Epoch: 2414, Len of Training loss: 18, Average loss: 1.4766291048791673\n",
      "Len of Validation loss: 54, Average loss: 3.407600901744984\n",
      "Epoch: 2415, Len of Training loss: 18, Average loss: 1.4099815686543782\n",
      "Len of Validation loss: 54, Average loss: 3.7115025741082652\n",
      "Epoch: 2416, Len of Training loss: 18, Average loss: 1.34784730275472\n",
      "Len of Validation loss: 54, Average loss: 3.6422753543765456\n",
      "Epoch: 2417, Len of Training loss: 18, Average loss: 1.4616501198874579\n",
      "Len of Validation loss: 54, Average loss: 3.670317844108299\n",
      "Epoch: 2418, Len of Training loss: 18, Average loss: 1.4657179514567058\n",
      "Len of Validation loss: 54, Average loss: 3.8143048551347523\n",
      "Epoch: 2419, Len of Training loss: 18, Average loss: 1.553678035736084\n",
      "Len of Validation loss: 54, Average loss: 3.7725937675546715\n",
      "Epoch: 2420, Len of Training loss: 18, Average loss: 1.4187544981638591\n",
      "Len of Validation loss: 54, Average loss: 3.669205383018211\n",
      "Epoch: 2421, Len of Training loss: 18, Average loss: 1.2957516709963481\n",
      "Len of Validation loss: 54, Average loss: 3.4932727096257388\n",
      "Epoch: 2422, Len of Training loss: 18, Average loss: 1.4589023854997423\n",
      "Len of Validation loss: 54, Average loss: 3.658027254872852\n",
      "Epoch: 2423, Len of Training loss: 18, Average loss: 1.4188999732335408\n",
      "Len of Validation loss: 54, Average loss: 3.732515224704036\n",
      "Epoch: 2424, Len of Training loss: 18, Average loss: 1.5515067179997761\n",
      "Len of Validation loss: 54, Average loss: 3.955217131861934\n",
      "Epoch: 2425, Len of Training loss: 18, Average loss: 1.4805470705032349\n",
      "Len of Validation loss: 54, Average loss: 3.757893705809558\n",
      "Epoch: 2426, Len of Training loss: 18, Average loss: 1.4374220702383254\n",
      "Len of Validation loss: 54, Average loss: 3.793707573855365\n",
      "Epoch: 2427, Len of Training loss: 18, Average loss: 1.4721291330125597\n",
      "Len of Validation loss: 54, Average loss: 3.720979322989782\n",
      "Epoch: 2428, Len of Training loss: 18, Average loss: 1.3817766308784485\n",
      "Len of Validation loss: 54, Average loss: 3.73306621004034\n",
      "Epoch: 2429, Len of Training loss: 18, Average loss: 1.3725059231122334\n",
      "Len of Validation loss: 54, Average loss: 3.9054191841019525\n",
      "Epoch: 2430, Len of Training loss: 18, Average loss: 1.496903849972619\n",
      "Len of Validation loss: 54, Average loss: 3.680351943881423\n",
      "Epoch: 2431, Len of Training loss: 18, Average loss: 1.5491669045554266\n",
      "Len of Validation loss: 54, Average loss: 3.5313302907678814\n",
      "Epoch: 2432, Len of Training loss: 18, Average loss: 1.446065326531728\n",
      "Len of Validation loss: 54, Average loss: 3.8299969920405634\n",
      "Epoch: 2433, Len of Training loss: 18, Average loss: 1.5350134505165949\n",
      "Len of Validation loss: 54, Average loss: 3.6078348413661674\n",
      "Epoch: 2434, Len of Training loss: 18, Average loss: 1.3326810797055562\n",
      "Len of Validation loss: 54, Average loss: 3.561580646921087\n",
      "Epoch: 2435, Len of Training loss: 18, Average loss: 1.2412618398666382\n",
      "Len of Validation loss: 54, Average loss: 3.3599938529509084\n",
      "Epoch: 2436, Len of Training loss: 18, Average loss: 1.2457644277148776\n",
      "Len of Validation loss: 54, Average loss: 3.344277525389636\n",
      "Epoch: 2437, Len of Training loss: 18, Average loss: 1.264087266392178\n",
      "Len of Validation loss: 54, Average loss: 3.649995771823106\n",
      "Epoch: 2438, Len of Training loss: 18, Average loss: 1.3068353136380513\n",
      "Len of Validation loss: 54, Average loss: 3.5632262296146817\n",
      "Epoch: 2439, Len of Training loss: 18, Average loss: 1.2481065458721585\n",
      "Len of Validation loss: 54, Average loss: 3.5160702798101635\n",
      "Epoch: 2440, Len of Training loss: 18, Average loss: 1.2934477064344618\n",
      "Len of Validation loss: 54, Average loss: 3.4873306387000613\n",
      "Epoch: 2441, Len of Training loss: 18, Average loss: 1.2510614660051134\n",
      "Len of Validation loss: 54, Average loss: 3.475677807022024\n",
      "Epoch: 2442, Len of Training loss: 18, Average loss: 1.1877971755133734\n",
      "Len of Validation loss: 54, Average loss: 3.6090901571291463\n",
      "Epoch: 2443, Len of Training loss: 18, Average loss: 1.1503707435395982\n",
      "Len of Validation loss: 54, Average loss: 3.545308740050704\n",
      "Epoch: 2444, Len of Training loss: 18, Average loss: 1.1716469989882574\n",
      "Len of Validation loss: 54, Average loss: 3.5252028416704246\n",
      "Epoch: 2445, Len of Training loss: 18, Average loss: 1.2111429108513727\n",
      "Len of Validation loss: 54, Average loss: 3.553642064332962\n",
      "Epoch: 2446, Len of Training loss: 18, Average loss: 1.1808359622955322\n",
      "Len of Validation loss: 54, Average loss: 3.461052712466982\n",
      "Epoch: 2447, Len of Training loss: 18, Average loss: 1.2734255856937833\n",
      "Len of Validation loss: 54, Average loss: 3.5558003474164894\n",
      "Epoch: 2448, Len of Training loss: 18, Average loss: 1.4576672315597534\n",
      "Len of Validation loss: 54, Average loss: 3.777229251684966\n",
      "Epoch: 2449, Len of Training loss: 18, Average loss: 1.4109720786412556\n",
      "Len of Validation loss: 54, Average loss: 3.712216779037758\n",
      "Epoch: 2450, Len of Training loss: 18, Average loss: 1.3375925487942166\n",
      "Len of Validation loss: 54, Average loss: 3.4828684561782413\n",
      "Epoch: 2451, Len of Training loss: 18, Average loss: 1.3271920416090224\n",
      "Len of Validation loss: 54, Average loss: 3.586529314517975\n",
      "Epoch: 2452, Len of Training loss: 18, Average loss: 1.3044003380669489\n",
      "Len of Validation loss: 54, Average loss: 3.595923484475524\n",
      "Epoch: 2453, Len of Training loss: 18, Average loss: 1.2840542925728693\n",
      "Len of Validation loss: 54, Average loss: 3.4852103922102184\n",
      "Epoch: 2454, Len of Training loss: 18, Average loss: 1.2752983503871493\n",
      "Len of Validation loss: 54, Average loss: 3.7242551655681044\n",
      "Epoch: 2455, Len of Training loss: 18, Average loss: 1.306187113126119\n",
      "Len of Validation loss: 54, Average loss: 3.7452162791181496\n",
      "Epoch: 2456, Len of Training loss: 18, Average loss: 1.3816378712654114\n",
      "Len of Validation loss: 54, Average loss: 3.801075479498616\n",
      "Epoch: 2457, Len of Training loss: 18, Average loss: 1.2786829471588135\n",
      "Len of Validation loss: 54, Average loss: 3.5593934964250633\n",
      "Epoch: 2458, Len of Training loss: 18, Average loss: 1.251271042558882\n",
      "Len of Validation loss: 54, Average loss: 3.53041841144915\n",
      "Epoch: 2459, Len of Training loss: 18, Average loss: 1.252659837404887\n",
      "Len of Validation loss: 54, Average loss: 3.600319916451419\n",
      "Epoch: 2460, Len of Training loss: 18, Average loss: 1.242934889263577\n",
      "Len of Validation loss: 54, Average loss: 3.557173327163414\n",
      "Epoch: 2461, Len of Training loss: 18, Average loss: 1.3081674443350897\n",
      "Len of Validation loss: 54, Average loss: 3.897832914634987\n",
      "Epoch: 2462, Len of Training loss: 18, Average loss: 1.3619643449783325\n",
      "Len of Validation loss: 54, Average loss: 3.671742171049118\n",
      "Epoch: 2463, Len of Training loss: 18, Average loss: 1.3639066219329834\n",
      "Len of Validation loss: 54, Average loss: 3.618679717735008\n",
      "Epoch: 2464, Len of Training loss: 18, Average loss: 1.3354523446824815\n",
      "Len of Validation loss: 54, Average loss: 3.7400273515118494\n",
      "Epoch: 2465, Len of Training loss: 18, Average loss: 1.3040853871239557\n",
      "Len of Validation loss: 54, Average loss: 3.5267370950292656\n",
      "Epoch: 2466, Len of Training loss: 18, Average loss: 1.2896123462253146\n",
      "Len of Validation loss: 54, Average loss: 3.622967055550328\n",
      "Epoch: 2467, Len of Training loss: 18, Average loss: 1.2429207695855036\n",
      "Len of Validation loss: 54, Average loss: 3.702884202754056\n",
      "Epoch: 2468, Len of Training loss: 18, Average loss: 1.2317744162347581\n",
      "Len of Validation loss: 54, Average loss: 3.5427568400347673\n",
      "Epoch: 2469, Len of Training loss: 18, Average loss: 1.2411954932742648\n",
      "Len of Validation loss: 54, Average loss: 3.4492812509889954\n",
      "Epoch: 2470, Len of Training loss: 18, Average loss: 1.5810057918230693\n",
      "Len of Validation loss: 54, Average loss: 3.556639509068595\n",
      "Epoch: 2471, Len of Training loss: 18, Average loss: 1.4625079101986356\n",
      "Len of Validation loss: 54, Average loss: 3.6006533724290355\n",
      "Epoch: 2472, Len of Training loss: 18, Average loss: 1.3490240772565205\n",
      "Len of Validation loss: 54, Average loss: 3.5071384244494967\n",
      "Epoch: 2473, Len of Training loss: 18, Average loss: 1.2395332389407687\n",
      "Len of Validation loss: 54, Average loss: 3.564617541101244\n",
      "Epoch: 2474, Len of Training loss: 18, Average loss: 1.192554963959588\n",
      "Len of Validation loss: 54, Average loss: 3.415840744972229\n",
      "Epoch: 2475, Len of Training loss: 18, Average loss: 1.2307162086168926\n",
      "Len of Validation loss: 54, Average loss: 3.446043308134432\n",
      "Epoch: 2476, Len of Training loss: 18, Average loss: 1.314409163263109\n",
      "Len of Validation loss: 54, Average loss: 3.714118070072598\n",
      "Epoch: 2477, Len of Training loss: 18, Average loss: 1.2947205238872104\n",
      "Len of Validation loss: 54, Average loss: 3.536479983064863\n",
      "Epoch: 2478, Len of Training loss: 18, Average loss: 1.2754425936275058\n",
      "Len of Validation loss: 54, Average loss: 3.597627704894101\n",
      "Epoch: 2479, Len of Training loss: 18, Average loss: 1.4592003557417128\n",
      "Len of Validation loss: 54, Average loss: 3.7141301565700107\n",
      "Epoch: 2480, Len of Training loss: 18, Average loss: 1.5694002310434978\n",
      "Len of Validation loss: 54, Average loss: 4.132688544414662\n",
      "Epoch: 2481, Len of Training loss: 18, Average loss: 1.7665382226308186\n",
      "Len of Validation loss: 54, Average loss: 4.1637178747742265\n",
      "Epoch: 2482, Len of Training loss: 18, Average loss: 1.697097380956014\n",
      "Len of Validation loss: 54, Average loss: 3.862888894699238\n",
      "Epoch: 2483, Len of Training loss: 18, Average loss: 1.594974398612976\n",
      "Len of Validation loss: 54, Average loss: 3.6418653704501964\n",
      "Epoch: 2484, Len of Training loss: 18, Average loss: 1.412189741929372\n",
      "Len of Validation loss: 54, Average loss: 3.7458874494941146\n",
      "Epoch: 2485, Len of Training loss: 18, Average loss: 1.40205897225274\n",
      "Len of Validation loss: 54, Average loss: 3.556766421706588\n",
      "Epoch: 2486, Len of Training loss: 18, Average loss: 1.4842461546262105\n",
      "Len of Validation loss: 54, Average loss: 3.6553003832145974\n",
      "Epoch: 2487, Len of Training loss: 18, Average loss: 1.4281559189160664\n",
      "Len of Validation loss: 54, Average loss: 3.5671764281060963\n",
      "Epoch: 2488, Len of Training loss: 18, Average loss: 1.330510053369734\n",
      "Len of Validation loss: 54, Average loss: 3.596724751922819\n",
      "Epoch: 2489, Len of Training loss: 18, Average loss: 1.372725076145596\n",
      "Len of Validation loss: 54, Average loss: 3.683894675087046\n",
      "Epoch: 2490, Len of Training loss: 18, Average loss: 1.3983334567811754\n",
      "Len of Validation loss: 54, Average loss: 3.4827244535640434\n",
      "Epoch: 2491, Len of Training loss: 18, Average loss: 1.3512264092763264\n",
      "Len of Validation loss: 54, Average loss: 3.52754353373139\n",
      "Epoch: 2492, Len of Training loss: 18, Average loss: 1.2995268371370103\n",
      "Len of Validation loss: 54, Average loss: 3.718329737583796\n",
      "Epoch: 2493, Len of Training loss: 18, Average loss: 1.2551147275500827\n",
      "Len of Validation loss: 54, Average loss: 3.5572733724558794\n",
      "Epoch: 2494, Len of Training loss: 18, Average loss: 1.2503124409251742\n",
      "Len of Validation loss: 54, Average loss: 3.4685696915343955\n",
      "Epoch: 2495, Len of Training loss: 18, Average loss: 1.2510922816064622\n",
      "Len of Validation loss: 54, Average loss: 3.4908178790851876\n",
      "Epoch: 2496, Len of Training loss: 18, Average loss: 1.3061604036225214\n",
      "Len of Validation loss: 54, Average loss: 3.508604982384929\n",
      "Epoch: 2497, Len of Training loss: 18, Average loss: 1.3130415346887376\n",
      "Len of Validation loss: 54, Average loss: 3.413096190602691\n",
      "Epoch: 2498, Len of Training loss: 18, Average loss: 1.3401708669132657\n",
      "Len of Validation loss: 54, Average loss: 3.5806857855231673\n",
      "Epoch: 2499, Len of Training loss: 18, Average loss: 1.4620352453655667\n",
      "Len of Validation loss: 54, Average loss: 3.734966743875433\n",
      "Epoch: 2500, Len of Training loss: 18, Average loss: 1.4661483698421054\n",
      "Len of Validation loss: 54, Average loss: 3.5697158365337938\n",
      "Epoch: 2501, Len of Training loss: 18, Average loss: 1.326631161901686\n",
      "Len of Validation loss: 54, Average loss: 3.5357038798155607\n",
      "Epoch: 2502, Len of Training loss: 18, Average loss: 1.3097948630650837\n",
      "Len of Validation loss: 54, Average loss: 3.6014336248238883\n",
      "Epoch: 2503, Len of Training loss: 18, Average loss: 1.2562777797381084\n",
      "Len of Validation loss: 54, Average loss: 3.899686691937623\n",
      "Epoch: 2504, Len of Training loss: 18, Average loss: 1.4421772493256464\n",
      "Len of Validation loss: 54, Average loss: 3.700765358077155\n",
      "Epoch: 2505, Len of Training loss: 18, Average loss: 1.3634745412402682\n",
      "Len of Validation loss: 54, Average loss: 3.5588539408312903\n",
      "Epoch: 2506, Len of Training loss: 18, Average loss: 1.2603668570518494\n",
      "Len of Validation loss: 54, Average loss: 3.532944227810259\n",
      "Epoch: 2507, Len of Training loss: 18, Average loss: 1.304629663626353\n",
      "Len of Validation loss: 54, Average loss: 3.9488313540264413\n",
      "Epoch: 2508, Len of Training loss: 18, Average loss: 1.5169352624151442\n",
      "Len of Validation loss: 54, Average loss: 3.827765292591519\n",
      "Epoch: 2509, Len of Training loss: 18, Average loss: 1.4654542141490512\n",
      "Len of Validation loss: 54, Average loss: 3.5597977152577154\n",
      "Epoch: 2510, Len of Training loss: 18, Average loss: 1.4604427350891962\n",
      "Len of Validation loss: 54, Average loss: 3.9694744017389088\n",
      "Epoch: 2511, Len of Training loss: 18, Average loss: 1.5851803488201566\n",
      "Len of Validation loss: 54, Average loss: 3.825934218035804\n",
      "Epoch: 2512, Len of Training loss: 18, Average loss: 1.435480323102739\n",
      "Len of Validation loss: 54, Average loss: 3.663792288965649\n",
      "Epoch: 2513, Len of Training loss: 18, Average loss: 1.3321387502882216\n",
      "Len of Validation loss: 54, Average loss: 3.6079987993946783\n",
      "Epoch: 2514, Len of Training loss: 18, Average loss: 1.2799067232343886\n",
      "Len of Validation loss: 54, Average loss: 3.520738618241416\n",
      "Epoch: 2515, Len of Training loss: 18, Average loss: 1.2547145552105374\n",
      "Len of Validation loss: 54, Average loss: 3.649665907577232\n",
      "Epoch: 2516, Len of Training loss: 18, Average loss: 1.2947245836257935\n",
      "Len of Validation loss: 54, Average loss: 3.551491930528923\n",
      "Epoch: 2517, Len of Training loss: 18, Average loss: 1.4129887289471097\n",
      "Len of Validation loss: 54, Average loss: 3.6876308101194875\n",
      "Epoch: 2518, Len of Training loss: 18, Average loss: 1.3644928932189941\n",
      "Len of Validation loss: 54, Average loss: 3.5790519030005843\n",
      "Epoch: 2519, Len of Training loss: 18, Average loss: 1.2938916153377957\n",
      "Len of Validation loss: 54, Average loss: 3.579227551265999\n",
      "Epoch: 2520, Len of Training loss: 18, Average loss: 1.2296772731675043\n",
      "Len of Validation loss: 54, Average loss: 3.5352800963101565\n",
      "Epoch: 2521, Len of Training loss: 18, Average loss: 1.2512573666042752\n",
      "Len of Validation loss: 54, Average loss: 3.4741344849268594\n",
      "Epoch: 2522, Len of Training loss: 18, Average loss: 1.2899254229333665\n",
      "Len of Validation loss: 54, Average loss: 3.6597921274326466\n",
      "Epoch: 2523, Len of Training loss: 18, Average loss: 1.406257496939765\n",
      "Len of Validation loss: 54, Average loss: 3.6164066040957414\n",
      "Epoch: 2524, Len of Training loss: 18, Average loss: 1.2870437105496724\n",
      "Len of Validation loss: 54, Average loss: 3.6643630685629667\n",
      "Epoch: 2525, Len of Training loss: 18, Average loss: 1.216479135884179\n",
      "Len of Validation loss: 54, Average loss: 3.5106332721533597\n",
      "Epoch: 2526, Len of Training loss: 18, Average loss: 1.2365332576963637\n",
      "Len of Validation loss: 54, Average loss: 3.4276738784931324\n",
      "Epoch: 2527, Len of Training loss: 18, Average loss: 1.255301872889201\n",
      "Len of Validation loss: 54, Average loss: 3.5648254564514867\n",
      "Epoch: 2528, Len of Training loss: 18, Average loss: 1.2543554835849338\n",
      "Len of Validation loss: 54, Average loss: 3.5335722300741406\n",
      "Epoch: 2529, Len of Training loss: 18, Average loss: 1.2143680055936177\n",
      "Len of Validation loss: 54, Average loss: 3.4441293323481523\n",
      "Epoch: 2530, Len of Training loss: 18, Average loss: 1.4814748499128554\n",
      "Len of Validation loss: 54, Average loss: 3.855989866786533\n",
      "Epoch: 2531, Len of Training loss: 18, Average loss: 1.5316887299219768\n",
      "Len of Validation loss: 54, Average loss: 3.884418277828782\n",
      "Epoch: 2532, Len of Training loss: 18, Average loss: 1.4612923330730863\n",
      "Len of Validation loss: 54, Average loss: 3.7681815215834864\n",
      "Epoch: 2533, Len of Training loss: 18, Average loss: 1.3485437896516588\n",
      "Len of Validation loss: 54, Average loss: 3.681525355135953\n",
      "Epoch: 2534, Len of Training loss: 18, Average loss: 1.2754573822021484\n",
      "Len of Validation loss: 54, Average loss: 3.332925083460631\n",
      "Epoch: 2535, Len of Training loss: 18, Average loss: 1.2332389818297491\n",
      "Len of Validation loss: 54, Average loss: 3.498559216658274\n",
      "Epoch: 2536, Len of Training loss: 18, Average loss: 1.2556303938229878\n",
      "Len of Validation loss: 54, Average loss: 3.5173146018275507\n",
      "Epoch: 2537, Len of Training loss: 18, Average loss: 1.5449381404452853\n",
      "Len of Validation loss: 54, Average loss: 3.9167847169770136\n",
      "Epoch: 2538, Len of Training loss: 18, Average loss: 1.4496666656600103\n",
      "Len of Validation loss: 54, Average loss: 3.7802447566279658\n",
      "Epoch: 2539, Len of Training loss: 18, Average loss: 1.5599110921223958\n",
      "Len of Validation loss: 54, Average loss: 3.713955619820842\n",
      "Epoch: 2540, Len of Training loss: 18, Average loss: 1.2833580374717712\n",
      "Len of Validation loss: 54, Average loss: 3.6686212685373096\n",
      "Epoch: 2541, Len of Training loss: 18, Average loss: 1.4439537790086534\n",
      "Len of Validation loss: 54, Average loss: 3.8967201798050493\n",
      "Epoch: 2542, Len of Training loss: 18, Average loss: 1.5921740796830919\n",
      "Len of Validation loss: 54, Average loss: 3.7222933029686964\n",
      "Epoch: 2543, Len of Training loss: 18, Average loss: 1.513284809059567\n",
      "Len of Validation loss: 54, Average loss: 3.7198779958265797\n",
      "Epoch: 2544, Len of Training loss: 18, Average loss: 1.4136044714185927\n",
      "Len of Validation loss: 54, Average loss: 3.6269159493622958\n",
      "Epoch: 2545, Len of Training loss: 18, Average loss: 1.2498714261584811\n",
      "Len of Validation loss: 54, Average loss: 3.5181139133594654\n",
      "Epoch: 2546, Len of Training loss: 18, Average loss: 1.330951491991679\n",
      "Len of Validation loss: 54, Average loss: 3.7362434323187226\n",
      "Epoch: 2547, Len of Training loss: 18, Average loss: 1.3551416728231642\n",
      "Len of Validation loss: 54, Average loss: 3.646387971109814\n",
      "Epoch: 2548, Len of Training loss: 18, Average loss: 1.2672836780548096\n",
      "Len of Validation loss: 54, Average loss: 3.5528943836688995\n",
      "Epoch: 2549, Len of Training loss: 18, Average loss: 1.2553295758035448\n",
      "Len of Validation loss: 54, Average loss: 3.5301032728619046\n",
      "Epoch: 2550, Len of Training loss: 18, Average loss: 1.2222750054465399\n",
      "Len of Validation loss: 54, Average loss: 3.490983597658299\n",
      "Epoch: 2551, Len of Training loss: 18, Average loss: 1.2595892879698012\n",
      "Len of Validation loss: 54, Average loss: 3.81748033673675\n",
      "Epoch: 2552, Len of Training loss: 18, Average loss: 1.2218678262498643\n",
      "Len of Validation loss: 54, Average loss: 3.497786354135584\n",
      "Epoch: 2553, Len of Training loss: 18, Average loss: 1.2265927063094244\n",
      "Len of Validation loss: 54, Average loss: 3.513741034048575\n",
      "Epoch: 2554, Len of Training loss: 18, Average loss: 1.244760228527917\n",
      "Len of Validation loss: 54, Average loss: 3.87195254807119\n",
      "Epoch: 2555, Len of Training loss: 18, Average loss: 1.285425861676534\n",
      "Len of Validation loss: 54, Average loss: 3.6331520257172762\n",
      "Epoch: 2556, Len of Training loss: 18, Average loss: 1.2603566845258076\n",
      "Len of Validation loss: 54, Average loss: 3.6068521793241852\n",
      "Epoch: 2557, Len of Training loss: 18, Average loss: 1.207698815398746\n",
      "Len of Validation loss: 54, Average loss: 3.463046136829588\n",
      "Epoch: 2558, Len of Training loss: 18, Average loss: 1.2032449576589797\n",
      "Len of Validation loss: 54, Average loss: 3.398237313385363\n",
      "Epoch: 2559, Len of Training loss: 18, Average loss: 1.179899983935886\n",
      "Len of Validation loss: 54, Average loss: 3.6133677385471485\n",
      "Epoch: 2560, Len of Training loss: 18, Average loss: 1.2887115213606093\n",
      "Len of Validation loss: 54, Average loss: 3.5774021248022714\n",
      "Epoch: 2561, Len of Training loss: 18, Average loss: 1.2976457013024225\n",
      "Len of Validation loss: 54, Average loss: 3.6231610940562353\n",
      "Epoch: 2562, Len of Training loss: 18, Average loss: 1.2892642087406583\n",
      "Len of Validation loss: 54, Average loss: 3.7421990114229695\n",
      "Epoch: 2563, Len of Training loss: 18, Average loss: 1.2722020943959553\n",
      "Len of Validation loss: 54, Average loss: 3.5518971858201205\n",
      "Epoch: 2564, Len of Training loss: 18, Average loss: 1.2493055197927687\n",
      "Len of Validation loss: 54, Average loss: 3.560191838829606\n",
      "Epoch: 2565, Len of Training loss: 18, Average loss: 1.2915454308191936\n",
      "Len of Validation loss: 54, Average loss: 3.4551742606692843\n",
      "Epoch: 2566, Len of Training loss: 18, Average loss: 1.2871077391836379\n",
      "Len of Validation loss: 54, Average loss: 3.753496971395281\n",
      "Epoch: 2567, Len of Training loss: 18, Average loss: 1.2040715085135565\n",
      "Len of Validation loss: 54, Average loss: 3.618640375358087\n",
      "Epoch: 2568, Len of Training loss: 18, Average loss: 1.1773071222835116\n",
      "Len of Validation loss: 54, Average loss: 3.541543717737551\n",
      "Epoch: 2569, Len of Training loss: 18, Average loss: 1.2325562900967069\n",
      "Len of Validation loss: 54, Average loss: 3.4588582305996507\n",
      "Epoch: 2570, Len of Training loss: 18, Average loss: 1.1758185823758442\n",
      "Len of Validation loss: 54, Average loss: 3.419708631656788\n",
      "Epoch: 2571, Len of Training loss: 18, Average loss: 1.1582286291652255\n",
      "Len of Validation loss: 54, Average loss: 3.438619312312868\n",
      "Epoch: 2572, Len of Training loss: 18, Average loss: 1.1686858667267694\n",
      "Len of Validation loss: 54, Average loss: 3.593726325918127\n",
      "Epoch: 2573, Len of Training loss: 18, Average loss: 1.1289659208721585\n",
      "Len of Validation loss: 54, Average loss: 3.555982905405539\n",
      "Epoch: 2574, Len of Training loss: 18, Average loss: 1.162120282649994\n",
      "Len of Validation loss: 54, Average loss: 3.654146233090648\n",
      "Epoch: 2575, Len of Training loss: 18, Average loss: 1.1399436725510492\n",
      "Len of Validation loss: 54, Average loss: 3.4193601177798376\n",
      "Epoch: 2576, Len of Training loss: 18, Average loss: 1.2028522160318162\n",
      "Len of Validation loss: 54, Average loss: 3.7186506434723183\n",
      "Epoch: 2577, Len of Training loss: 18, Average loss: 1.194038106335534\n",
      "Len of Validation loss: 54, Average loss: 3.326292950797964\n",
      "Epoch: 2578, Len of Training loss: 18, Average loss: 1.2078497674730089\n",
      "Len of Validation loss: 54, Average loss: 3.582955730182153\n",
      "Epoch: 2579, Len of Training loss: 18, Average loss: 1.4092539813783433\n",
      "Len of Validation loss: 54, Average loss: 3.9588005675209894\n",
      "Epoch: 2580, Len of Training loss: 18, Average loss: 1.628380172782474\n",
      "Len of Validation loss: 54, Average loss: 3.6593954728709326\n",
      "Epoch: 2581, Len of Training loss: 18, Average loss: 1.3535912434260051\n",
      "Len of Validation loss: 54, Average loss: 3.6957563294304743\n",
      "Epoch: 2582, Len of Training loss: 18, Average loss: 1.2980193628205194\n",
      "Len of Validation loss: 54, Average loss: 3.5463054996949657\n",
      "Epoch: 2583, Len of Training loss: 18, Average loss: 1.3731767402754889\n",
      "Len of Validation loss: 54, Average loss: 3.6000500277236656\n",
      "Epoch: 2584, Len of Training loss: 18, Average loss: 1.2906405131022136\n",
      "Len of Validation loss: 54, Average loss: 3.7723530067337885\n",
      "Epoch: 2585, Len of Training loss: 18, Average loss: 1.435037116209666\n",
      "Len of Validation loss: 54, Average loss: 3.600953044714751\n",
      "Epoch: 2586, Len of Training loss: 18, Average loss: 1.452616122033861\n",
      "Len of Validation loss: 54, Average loss: 3.467290547158983\n",
      "Epoch: 2587, Len of Training loss: 18, Average loss: 1.285514361328549\n",
      "Len of Validation loss: 54, Average loss: 3.6177857176021293\n",
      "Epoch: 2588, Len of Training loss: 18, Average loss: 1.1999389264318678\n",
      "Len of Validation loss: 54, Average loss: 3.491201788187027\n",
      "Epoch: 2589, Len of Training loss: 18, Average loss: 1.2067061861356099\n",
      "Len of Validation loss: 54, Average loss: 3.8158372077676983\n",
      "Epoch: 2590, Len of Training loss: 18, Average loss: 1.3644305401378207\n",
      "Len of Validation loss: 54, Average loss: 3.667514350679186\n",
      "Epoch: 2591, Len of Training loss: 18, Average loss: 1.3252485659387376\n",
      "Len of Validation loss: 54, Average loss: 3.563903827358175\n",
      "Epoch: 2592, Len of Training loss: 18, Average loss: 1.3036153581407335\n",
      "Len of Validation loss: 54, Average loss: 3.6065865931687533\n",
      "Epoch: 2593, Len of Training loss: 18, Average loss: 1.3768593735165067\n",
      "Len of Validation loss: 54, Average loss: 3.5740071811057903\n",
      "Epoch: 2594, Len of Training loss: 18, Average loss: 1.3011035323143005\n",
      "Len of Validation loss: 54, Average loss: 3.5209176694905318\n",
      "Epoch: 2595, Len of Training loss: 18, Average loss: 1.3994687729411655\n",
      "Len of Validation loss: 54, Average loss: 3.643022265699175\n",
      "Epoch: 2596, Len of Training loss: 18, Average loss: 1.2984839346673753\n",
      "Len of Validation loss: 54, Average loss: 3.523106897318805\n",
      "Epoch: 2597, Len of Training loss: 18, Average loss: 1.385606931315528\n",
      "Len of Validation loss: 54, Average loss: 3.987589449794204\n",
      "Epoch: 2598, Len of Training loss: 18, Average loss: 1.7710219356748793\n",
      "Len of Validation loss: 54, Average loss: 4.097650764165102\n",
      "Epoch: 2599, Len of Training loss: 18, Average loss: 1.8866062760353088\n",
      "Len of Validation loss: 54, Average loss: 3.8038075235154896\n",
      "Epoch: 2600, Len of Training loss: 18, Average loss: 1.5463574131329854\n",
      "Len of Validation loss: 54, Average loss: 3.5042101199980134\n",
      "Epoch: 2601, Len of Training loss: 18, Average loss: 1.281764758957757\n",
      "Len of Validation loss: 54, Average loss: 3.4358013493043407\n",
      "Epoch: 2602, Len of Training loss: 18, Average loss: 1.1728652715682983\n",
      "Len of Validation loss: 54, Average loss: 3.60688696525715\n",
      "Epoch: 2603, Len of Training loss: 18, Average loss: 1.4554625219768949\n",
      "Len of Validation loss: 54, Average loss: 3.9263164268599615\n",
      "Epoch: 2604, Len of Training loss: 18, Average loss: 1.4642698632346258\n",
      "Len of Validation loss: 54, Average loss: 3.6375382796481803\n",
      "Epoch: 2605, Len of Training loss: 18, Average loss: 1.285427941216363\n",
      "Len of Validation loss: 54, Average loss: 3.7094012907257787\n",
      "Epoch: 2606, Len of Training loss: 18, Average loss: 1.3183650573094685\n",
      "Len of Validation loss: 54, Average loss: 3.6940623610107988\n",
      "Epoch: 2607, Len of Training loss: 18, Average loss: 1.3403747412893507\n",
      "Len of Validation loss: 54, Average loss: 3.7605660182458385\n",
      "Epoch: 2608, Len of Training loss: 18, Average loss: 1.422166453467475\n",
      "Len of Validation loss: 54, Average loss: 3.577520262312006\n",
      "Epoch: 2609, Len of Training loss: 18, Average loss: 1.4691692855623033\n",
      "Len of Validation loss: 54, Average loss: 3.973175847971881\n",
      "Epoch: 2610, Len of Training loss: 18, Average loss: 1.376682526535458\n",
      "Len of Validation loss: 54, Average loss: 3.5000022340703896\n",
      "Epoch: 2611, Len of Training loss: 18, Average loss: 1.2389909029006958\n",
      "Len of Validation loss: 54, Average loss: 3.542754669984182\n",
      "Epoch: 2612, Len of Training loss: 18, Average loss: 1.1998866664038763\n",
      "Len of Validation loss: 54, Average loss: 3.577016435287617\n",
      "Epoch: 2613, Len of Training loss: 18, Average loss: 1.2668359809451633\n",
      "Len of Validation loss: 54, Average loss: 3.3160123791959553\n",
      "Epoch: 2614, Len of Training loss: 18, Average loss: 1.2063699033525255\n",
      "Len of Validation loss: 54, Average loss: 3.7715209236851446\n",
      "Epoch: 2615, Len of Training loss: 18, Average loss: 1.1885442005263434\n",
      "Len of Validation loss: 54, Average loss: 3.4482269000124046\n",
      "Epoch: 2616, Len of Training loss: 18, Average loss: 1.1766263445218403\n",
      "Len of Validation loss: 54, Average loss: 3.4125183986292944\n",
      "Epoch: 2617, Len of Training loss: 18, Average loss: 1.144398291905721\n",
      "Len of Validation loss: 54, Average loss: 3.508853531546063\n",
      "Epoch: 2618, Len of Training loss: 18, Average loss: 1.151395731502109\n",
      "Len of Validation loss: 54, Average loss: 3.546777500046624\n",
      "Epoch: 2619, Len of Training loss: 18, Average loss: 1.1429146197107103\n",
      "Len of Validation loss: 54, Average loss: 3.4370481548485934\n",
      "Epoch: 2620, Len of Training loss: 18, Average loss: 1.2804953455924988\n",
      "Len of Validation loss: 54, Average loss: 3.479943452058015\n",
      "Epoch: 2621, Len of Training loss: 18, Average loss: 1.2620345287852817\n",
      "Len of Validation loss: 54, Average loss: 3.447858060951586\n",
      "Epoch: 2622, Len of Training loss: 18, Average loss: 1.270898441473643\n",
      "Len of Validation loss: 54, Average loss: 3.503283241280803\n",
      "Epoch: 2623, Len of Training loss: 18, Average loss: 1.1999518407715692\n",
      "Len of Validation loss: 54, Average loss: 3.5788168476687536\n",
      "Epoch: 2624, Len of Training loss: 18, Average loss: 1.1753344800737169\n",
      "Len of Validation loss: 54, Average loss: 3.5110203800378024\n",
      "Epoch: 2625, Len of Training loss: 18, Average loss: 1.1642647319369845\n",
      "Len of Validation loss: 54, Average loss: 3.5835475822289786\n",
      "Epoch: 2626, Len of Training loss: 18, Average loss: 1.165851480431027\n",
      "Len of Validation loss: 54, Average loss: 3.4475572263752974\n",
      "Epoch: 2627, Len of Training loss: 18, Average loss: 1.2907847828335233\n",
      "Len of Validation loss: 54, Average loss: 3.4811225798394947\n",
      "Epoch: 2628, Len of Training loss: 18, Average loss: 1.249251617325677\n",
      "Len of Validation loss: 54, Average loss: 3.34293735799966\n",
      "Epoch: 2629, Len of Training loss: 18, Average loss: 1.1710504558351305\n",
      "Len of Validation loss: 54, Average loss: 3.5099614406073534\n",
      "Epoch: 2630, Len of Training loss: 18, Average loss: 1.167741768889957\n",
      "Len of Validation loss: 54, Average loss: 3.487343469151744\n",
      "Epoch: 2631, Len of Training loss: 18, Average loss: 1.285108698738946\n",
      "Len of Validation loss: 54, Average loss: 3.6447261053102986\n",
      "Epoch: 2632, Len of Training loss: 18, Average loss: 1.318883670700921\n",
      "Len of Validation loss: 54, Average loss: 3.7374730662063316\n",
      "Epoch: 2633, Len of Training loss: 18, Average loss: 1.2699355284372966\n",
      "Len of Validation loss: 54, Average loss: 3.5701820806220725\n",
      "Epoch: 2634, Len of Training loss: 18, Average loss: 1.2651872833569844\n",
      "Len of Validation loss: 54, Average loss: 3.839303175608317\n",
      "Epoch: 2635, Len of Training loss: 18, Average loss: 1.6564467151959736\n",
      "Len of Validation loss: 54, Average loss: 3.719476059631065\n",
      "Epoch: 2636, Len of Training loss: 18, Average loss: 1.5203378332985773\n",
      "Len of Validation loss: 54, Average loss: 3.5939075924732067\n",
      "Epoch: 2637, Len of Training loss: 18, Average loss: 1.5559136668841045\n",
      "Len of Validation loss: 54, Average loss: 3.757664927729854\n",
      "Epoch: 2638, Len of Training loss: 18, Average loss: 1.4513459934128656\n",
      "Len of Validation loss: 54, Average loss: 3.472394056894161\n",
      "Epoch: 2639, Len of Training loss: 18, Average loss: 1.3776766856511433\n",
      "Len of Validation loss: 54, Average loss: 3.7407562920340784\n",
      "Epoch: 2640, Len of Training loss: 18, Average loss: 1.3470098972320557\n",
      "Len of Validation loss: 54, Average loss: 3.5472263086725166\n",
      "Epoch: 2641, Len of Training loss: 18, Average loss: 1.2471577326456706\n",
      "Len of Validation loss: 54, Average loss: 3.571789758072959\n",
      "Epoch: 2642, Len of Training loss: 18, Average loss: 1.2354470425181918\n",
      "Len of Validation loss: 54, Average loss: 3.643949184152815\n",
      "Epoch: 2643, Len of Training loss: 18, Average loss: 1.2445989052454631\n",
      "Len of Validation loss: 54, Average loss: 3.504138053567321\n",
      "Epoch: 2644, Len of Training loss: 18, Average loss: 1.2915174100134108\n",
      "Len of Validation loss: 54, Average loss: 3.798019411387267\n",
      "Epoch: 2645, Len of Training loss: 18, Average loss: 1.3085039125548468\n",
      "Len of Validation loss: 54, Average loss: 3.507625431926162\n",
      "Epoch: 2646, Len of Training loss: 18, Average loss: 1.3406250145700243\n",
      "Len of Validation loss: 54, Average loss: 3.9448056132705123\n",
      "Epoch: 2647, Len of Training loss: 18, Average loss: 1.4724660714467366\n",
      "Len of Validation loss: 54, Average loss: 3.588590065638224\n",
      "Epoch: 2648, Len of Training loss: 18, Average loss: 1.4051538440916274\n",
      "Len of Validation loss: 54, Average loss: 3.5967766355585167\n",
      "Epoch: 2649, Len of Training loss: 18, Average loss: 1.340810497601827\n",
      "Len of Validation loss: 54, Average loss: 3.855217684198309\n",
      "Epoch: 2650, Len of Training loss: 18, Average loss: 1.375890036424001\n",
      "Len of Validation loss: 54, Average loss: 3.4328362257392318\n",
      "Epoch: 2651, Len of Training loss: 18, Average loss: 1.2695603370666504\n",
      "Len of Validation loss: 54, Average loss: 3.5145249013547546\n",
      "Epoch: 2652, Len of Training loss: 18, Average loss: 1.240429355038537\n",
      "Len of Validation loss: 54, Average loss: 3.5461681330645525\n",
      "Epoch: 2653, Len of Training loss: 18, Average loss: 1.1709228224224515\n",
      "Len of Validation loss: 54, Average loss: 3.4349920793815896\n",
      "Epoch: 2654, Len of Training loss: 18, Average loss: 1.205061501926846\n",
      "Len of Validation loss: 54, Average loss: 3.519995382538548\n",
      "Epoch: 2655, Len of Training loss: 18, Average loss: 1.2482036219702826\n",
      "Len of Validation loss: 54, Average loss: 3.593993299537235\n",
      "Epoch: 2656, Len of Training loss: 18, Average loss: 1.248200323846605\n",
      "Len of Validation loss: 54, Average loss: 3.578942899350767\n",
      "Epoch: 2657, Len of Training loss: 18, Average loss: 1.3398765987820096\n",
      "Len of Validation loss: 54, Average loss: 3.6857054542612144\n",
      "Epoch: 2658, Len of Training loss: 18, Average loss: 1.3838666213883295\n",
      "Len of Validation loss: 54, Average loss: 3.6713256460648998\n",
      "Epoch: 2659, Len of Training loss: 18, Average loss: 1.292059646712409\n",
      "Len of Validation loss: 54, Average loss: 3.5350638274793273\n",
      "Epoch: 2660, Len of Training loss: 18, Average loss: 1.285901149113973\n",
      "Len of Validation loss: 54, Average loss: 3.8330180457344762\n",
      "Epoch: 2661, Len of Training loss: 18, Average loss: 1.3588137825330098\n",
      "Len of Validation loss: 54, Average loss: 3.6764133793336375\n",
      "Epoch: 2662, Len of Training loss: 18, Average loss: 1.35730955335829\n",
      "Len of Validation loss: 54, Average loss: 3.4647547370857663\n",
      "Epoch: 2663, Len of Training loss: 18, Average loss: 1.2490194572342768\n",
      "Len of Validation loss: 54, Average loss: 3.463496008404979\n",
      "Epoch: 2664, Len of Training loss: 18, Average loss: 1.2593619757228427\n",
      "Len of Validation loss: 54, Average loss: 3.7323609204203994\n",
      "Epoch: 2665, Len of Training loss: 18, Average loss: 1.242389029926724\n",
      "Len of Validation loss: 54, Average loss: 3.485616538259718\n",
      "Epoch: 2666, Len of Training loss: 18, Average loss: 1.2505724165174696\n",
      "Len of Validation loss: 54, Average loss: 3.509451393727903\n",
      "Epoch: 2667, Len of Training loss: 18, Average loss: 1.2641897466447618\n",
      "Len of Validation loss: 54, Average loss: 3.5969605136800697\n",
      "Epoch: 2668, Len of Training loss: 18, Average loss: 1.2000216046969097\n",
      "Len of Validation loss: 54, Average loss: 3.453928501517684\n",
      "Epoch: 2669, Len of Training loss: 18, Average loss: 1.1165679295857747\n",
      "Len of Validation loss: 54, Average loss: 3.5453899480678417\n",
      "Epoch: 2670, Len of Training loss: 18, Average loss: 1.284262392255995\n",
      "Len of Validation loss: 54, Average loss: 4.037698255644904\n",
      "Epoch: 2671, Len of Training loss: 18, Average loss: 1.490297966533237\n",
      "Len of Validation loss: 54, Average loss: 3.586120906803343\n",
      "Epoch: 2672, Len of Training loss: 18, Average loss: 1.2837457723087735\n",
      "Len of Validation loss: 54, Average loss: 3.519341435697344\n",
      "Epoch: 2673, Len of Training loss: 18, Average loss: 1.2043829030460782\n",
      "Len of Validation loss: 54, Average loss: 3.68503889993385\n",
      "Epoch: 2674, Len of Training loss: 18, Average loss: 1.2044307986895244\n",
      "Len of Validation loss: 54, Average loss: 3.732675156107655\n",
      "Epoch: 2675, Len of Training loss: 18, Average loss: 1.1943704817030165\n",
      "Len of Validation loss: 54, Average loss: 3.3407069555035345\n",
      "Epoch: 2676, Len of Training loss: 18, Average loss: 1.193439417415195\n",
      "Len of Validation loss: 54, Average loss: 3.518891672293345\n",
      "Epoch: 2677, Len of Training loss: 18, Average loss: 1.241598043176863\n",
      "Len of Validation loss: 54, Average loss: 3.434282585426613\n",
      "Epoch: 2678, Len of Training loss: 18, Average loss: 1.235314793056912\n",
      "Len of Validation loss: 54, Average loss: 3.640617435729062\n",
      "Epoch: 2679, Len of Training loss: 18, Average loss: 1.155411998430888\n",
      "Len of Validation loss: 54, Average loss: 3.5665049387349024\n",
      "Epoch: 2680, Len of Training loss: 18, Average loss: 1.2785775396558974\n",
      "Len of Validation loss: 54, Average loss: 3.86863323935756\n",
      "Epoch: 2681, Len of Training loss: 18, Average loss: 1.488905323876275\n",
      "Len of Validation loss: 54, Average loss: 3.654185992700082\n",
      "Epoch: 2682, Len of Training loss: 18, Average loss: 1.4701656103134155\n",
      "Len of Validation loss: 54, Average loss: 3.6211689865147627\n",
      "Epoch: 2683, Len of Training loss: 18, Average loss: 1.3596887720955744\n",
      "Len of Validation loss: 54, Average loss: 3.478607780403561\n",
      "Epoch: 2684, Len of Training loss: 18, Average loss: 1.331560214360555\n",
      "Len of Validation loss: 54, Average loss: 3.53677096079897\n",
      "Epoch: 2685, Len of Training loss: 18, Average loss: 1.282569448153178\n",
      "Len of Validation loss: 54, Average loss: 3.542351851860682\n",
      "Epoch: 2686, Len of Training loss: 18, Average loss: 1.2597664528422885\n",
      "Len of Validation loss: 54, Average loss: 3.520753974163974\n",
      "Epoch: 2687, Len of Training loss: 18, Average loss: 1.2508520748880174\n",
      "Len of Validation loss: 54, Average loss: 3.622925603831256\n",
      "Epoch: 2688, Len of Training loss: 18, Average loss: 1.2224021289083693\n",
      "Len of Validation loss: 54, Average loss: 3.5581681960158877\n",
      "Epoch: 2689, Len of Training loss: 18, Average loss: 1.2303498718473647\n",
      "Len of Validation loss: 54, Average loss: 3.8063482929159096\n",
      "Epoch: 2690, Len of Training loss: 18, Average loss: 1.2535620596673753\n",
      "Len of Validation loss: 54, Average loss: 3.4908731668083757\n",
      "Epoch: 2691, Len of Training loss: 18, Average loss: 1.3342551125420465\n",
      "Len of Validation loss: 54, Average loss: 3.7071423950018705\n",
      "Epoch: 2692, Len of Training loss: 18, Average loss: 1.5883290370305378\n",
      "Len of Validation loss: 54, Average loss: 3.8069551322195263\n",
      "Epoch: 2693, Len of Training loss: 18, Average loss: 1.3573087983661227\n",
      "Len of Validation loss: 54, Average loss: 3.528522245309971\n",
      "Epoch: 2694, Len of Training loss: 18, Average loss: 1.2284216682116191\n",
      "Len of Validation loss: 54, Average loss: 3.527092304494646\n",
      "Epoch: 2695, Len of Training loss: 18, Average loss: 1.1465200318230524\n",
      "Len of Validation loss: 54, Average loss: 3.5028131858066276\n",
      "Epoch: 2696, Len of Training loss: 18, Average loss: 1.0932632055547502\n",
      "Len of Validation loss: 54, Average loss: 3.4316185359601623\n",
      "Epoch: 2697, Len of Training loss: 18, Average loss: 1.160314487086402\n",
      "Len of Validation loss: 54, Average loss: 3.6928514672650232\n",
      "Epoch: 2698, Len of Training loss: 18, Average loss: 1.1869324379497104\n",
      "Len of Validation loss: 54, Average loss: 3.648595033972352\n",
      "Epoch: 2699, Len of Training loss: 18, Average loss: 1.2060990399784512\n",
      "Len of Validation loss: 54, Average loss: 3.522242901501832\n",
      "Epoch: 2700, Len of Training loss: 18, Average loss: 1.2926058967908223\n",
      "Len of Validation loss: 54, Average loss: 3.4536874228053622\n",
      "Epoch: 2701, Len of Training loss: 18, Average loss: 1.3840665817260742\n",
      "Len of Validation loss: 54, Average loss: 3.6896237631638846\n",
      "Epoch: 2702, Len of Training loss: 18, Average loss: 1.4709863265355427\n",
      "Len of Validation loss: 54, Average loss: 3.760250191997599\n",
      "Epoch: 2703, Len of Training loss: 18, Average loss: 1.5316769613160028\n",
      "Len of Validation loss: 54, Average loss: 3.859374002174095\n",
      "Epoch: 2704, Len of Training loss: 18, Average loss: 1.596548272503747\n",
      "Len of Validation loss: 54, Average loss: 3.817257018000991\n",
      "Epoch: 2705, Len of Training loss: 18, Average loss: 1.4649178518189325\n",
      "Len of Validation loss: 54, Average loss: 3.5917382416901766\n",
      "Epoch: 2706, Len of Training loss: 18, Average loss: 1.4573688043488398\n",
      "Len of Validation loss: 54, Average loss: 3.640040636062622\n",
      "Epoch: 2707, Len of Training loss: 18, Average loss: 1.3905022144317627\n",
      "Len of Validation loss: 54, Average loss: 3.6361520941610688\n",
      "Epoch: 2708, Len of Training loss: 18, Average loss: 1.2997770309448242\n",
      "Len of Validation loss: 54, Average loss: 3.6009818536263927\n",
      "Epoch: 2709, Len of Training loss: 18, Average loss: 1.21427987019221\n",
      "Len of Validation loss: 54, Average loss: 3.611300051212311\n",
      "Epoch: 2710, Len of Training loss: 18, Average loss: 1.1522669792175293\n",
      "Len of Validation loss: 54, Average loss: 3.560086186285372\n",
      "Epoch: 2711, Len of Training loss: 18, Average loss: 1.2429577443334792\n",
      "Len of Validation loss: 54, Average loss: 3.5448847512404122\n",
      "Epoch: 2712, Len of Training loss: 18, Average loss: 1.2012247973018222\n",
      "Len of Validation loss: 54, Average loss: 3.59899542066786\n",
      "Epoch: 2713, Len of Training loss: 18, Average loss: 1.2220170497894287\n",
      "Len of Validation loss: 54, Average loss: 3.495185668821688\n",
      "Epoch: 2714, Len of Training loss: 18, Average loss: 1.2572781377368503\n",
      "Len of Validation loss: 54, Average loss: 3.447916344360069\n",
      "Epoch: 2715, Len of Training loss: 18, Average loss: 1.2076445950402155\n",
      "Len of Validation loss: 54, Average loss: 3.519536155241507\n",
      "Epoch: 2716, Len of Training loss: 18, Average loss: 1.2935036884413824\n",
      "Len of Validation loss: 54, Average loss: 3.3322433862421246\n",
      "Epoch: 2717, Len of Training loss: 18, Average loss: 1.2441143525971308\n",
      "Len of Validation loss: 54, Average loss: 3.5781815427320973\n",
      "Epoch: 2718, Len of Training loss: 18, Average loss: 1.2229987581570942\n",
      "Len of Validation loss: 54, Average loss: 3.45940899848938\n",
      "Epoch: 2719, Len of Training loss: 18, Average loss: 1.2160680492719014\n",
      "Len of Validation loss: 54, Average loss: 3.681161403656006\n",
      "Epoch: 2720, Len of Training loss: 18, Average loss: 1.4482879837354024\n",
      "Len of Validation loss: 54, Average loss: 3.526180386543274\n",
      "Epoch: 2721, Len of Training loss: 18, Average loss: 1.4080586830774944\n",
      "Len of Validation loss: 54, Average loss: 3.609496393689403\n",
      "Epoch: 2722, Len of Training loss: 18, Average loss: 1.3307258619202509\n",
      "Len of Validation loss: 54, Average loss: 3.624164824132566\n",
      "Epoch: 2723, Len of Training loss: 18, Average loss: 1.4029863940344915\n",
      "Len of Validation loss: 54, Average loss: 3.8606171762501753\n",
      "Epoch: 2724, Len of Training loss: 18, Average loss: 1.6081744697358873\n",
      "Len of Validation loss: 54, Average loss: 3.618212808061529\n",
      "Epoch: 2725, Len of Training loss: 18, Average loss: 1.3192804455757141\n",
      "Len of Validation loss: 54, Average loss: 3.483324855566025\n",
      "Epoch: 2726, Len of Training loss: 18, Average loss: 1.2389470007684495\n",
      "Len of Validation loss: 54, Average loss: 3.6182185036164745\n",
      "Epoch: 2727, Len of Training loss: 18, Average loss: 1.4248932268884447\n",
      "Len of Validation loss: 54, Average loss: 3.4724675483173795\n",
      "Epoch: 2728, Len of Training loss: 18, Average loss: 1.2239534656206768\n",
      "Len of Validation loss: 54, Average loss: 3.4660486424410784\n",
      "Epoch: 2729, Len of Training loss: 18, Average loss: 1.1490323543548584\n",
      "Len of Validation loss: 54, Average loss: 3.5125366703227714\n",
      "Epoch: 2730, Len of Training loss: 18, Average loss: 1.1730918288230896\n",
      "Len of Validation loss: 54, Average loss: 3.4904836184448667\n",
      "Epoch: 2731, Len of Training loss: 18, Average loss: 1.1812378035651312\n",
      "Len of Validation loss: 54, Average loss: 3.46941230252937\n",
      "Epoch: 2732, Len of Training loss: 18, Average loss: 1.1124227775467768\n",
      "Len of Validation loss: 54, Average loss: 3.355752635885168\n",
      "Epoch: 2733, Len of Training loss: 18, Average loss: 1.1853765779071384\n",
      "Len of Validation loss: 54, Average loss: 3.457251998009505\n",
      "Epoch: 2734, Len of Training loss: 18, Average loss: 1.2255877653757732\n",
      "Len of Validation loss: 54, Average loss: 3.4982747766706677\n",
      "Epoch: 2735, Len of Training loss: 18, Average loss: 1.1991469595167372\n",
      "Len of Validation loss: 54, Average loss: 3.6726132807908236\n",
      "Epoch: 2736, Len of Training loss: 18, Average loss: 1.1909319559733074\n",
      "Len of Validation loss: 54, Average loss: 3.507196096358476\n",
      "Epoch: 2737, Len of Training loss: 18, Average loss: 1.2354142864545186\n",
      "Len of Validation loss: 54, Average loss: 3.4315568727475627\n",
      "Epoch: 2738, Len of Training loss: 18, Average loss: 1.1603420707914565\n",
      "Len of Validation loss: 54, Average loss: 3.5230414458998927\n",
      "Epoch: 2739, Len of Training loss: 18, Average loss: 1.2476195096969604\n",
      "Len of Validation loss: 54, Average loss: 3.4879351644604295\n",
      "Epoch: 2740, Len of Training loss: 18, Average loss: 1.1405910319752164\n",
      "Len of Validation loss: 54, Average loss: 3.5108672236954725\n",
      "Epoch: 2741, Len of Training loss: 18, Average loss: 1.2591444982422724\n",
      "Len of Validation loss: 54, Average loss: 3.548984464671877\n",
      "Epoch: 2742, Len of Training loss: 18, Average loss: 1.3723209301630657\n",
      "Len of Validation loss: 54, Average loss: 3.4917057719495563\n",
      "Epoch: 2743, Len of Training loss: 18, Average loss: 1.3053751521640353\n",
      "Len of Validation loss: 54, Average loss: 3.696167729519032\n",
      "Epoch: 2744, Len of Training loss: 18, Average loss: 1.5450774166319106\n",
      "Len of Validation loss: 54, Average loss: 3.6703580639980458\n",
      "Epoch: 2745, Len of Training loss: 18, Average loss: 1.5695838398403592\n",
      "Len of Validation loss: 54, Average loss: 3.797512447392499\n",
      "Epoch: 2746, Len of Training loss: 18, Average loss: 1.3546096351411607\n",
      "Len of Validation loss: 54, Average loss: 3.4925782051351337\n",
      "Epoch: 2747, Len of Training loss: 18, Average loss: 1.2814676496717665\n",
      "Len of Validation loss: 54, Average loss: 3.6659707371835357\n",
      "Epoch: 2748, Len of Training loss: 18, Average loss: 1.4146216180589464\n",
      "Len of Validation loss: 54, Average loss: 3.5915570038336293\n",
      "Epoch: 2749, Len of Training loss: 18, Average loss: 1.3528296152750652\n",
      "Len of Validation loss: 54, Average loss: 3.458049162670418\n",
      "Epoch: 2750, Len of Training loss: 18, Average loss: 1.2787342733807034\n",
      "Len of Validation loss: 54, Average loss: 3.5875633888774447\n",
      "Epoch: 2751, Len of Training loss: 18, Average loss: 1.3733745945824518\n",
      "Len of Validation loss: 54, Average loss: 3.535953473161768\n",
      "Epoch: 2752, Len of Training loss: 18, Average loss: 1.2079558240042791\n",
      "Len of Validation loss: 54, Average loss: 3.560527475895705\n",
      "Epoch: 2753, Len of Training loss: 18, Average loss: 1.4445594085587397\n",
      "Len of Validation loss: 54, Average loss: 3.7505248520109387\n",
      "Epoch: 2754, Len of Training loss: 18, Average loss: 1.5322184761365254\n",
      "Len of Validation loss: 54, Average loss: 3.726909359296163\n",
      "Epoch: 2755, Len of Training loss: 18, Average loss: 1.307958682378133\n",
      "Len of Validation loss: 54, Average loss: 3.5915857233383037\n",
      "Epoch: 2756, Len of Training loss: 18, Average loss: 1.247806602054172\n",
      "Len of Validation loss: 54, Average loss: 3.4577280812793307\n",
      "Epoch: 2757, Len of Training loss: 18, Average loss: 1.2317920790778265\n",
      "Len of Validation loss: 54, Average loss: 3.640018148554696\n",
      "Epoch: 2758, Len of Training loss: 18, Average loss: 1.2290569874975417\n",
      "Len of Validation loss: 54, Average loss: 3.5516517338929354\n",
      "Epoch: 2759, Len of Training loss: 18, Average loss: 1.1949154602156744\n",
      "Len of Validation loss: 54, Average loss: 3.5869547801989095\n",
      "Epoch: 2760, Len of Training loss: 18, Average loss: 1.1179780231581793\n",
      "Len of Validation loss: 54, Average loss: 3.5119826517723225\n",
      "Epoch: 2761, Len of Training loss: 18, Average loss: 1.1668789386749268\n",
      "Len of Validation loss: 54, Average loss: 3.3877463340759277\n",
      "Epoch: 2762, Len of Training loss: 18, Average loss: 1.140991535451677\n",
      "Len of Validation loss: 54, Average loss: 3.537856326059059\n",
      "Epoch: 2763, Len of Training loss: 18, Average loss: 1.2010674211714003\n",
      "Len of Validation loss: 54, Average loss: 3.6043779496793396\n",
      "Epoch: 2764, Len of Training loss: 18, Average loss: 1.2293810447057087\n",
      "Len of Validation loss: 54, Average loss: 3.658874763382806\n",
      "Epoch: 2765, Len of Training loss: 18, Average loss: 1.1644123792648315\n",
      "Len of Validation loss: 54, Average loss: 3.4019898849504964\n",
      "Epoch: 2766, Len of Training loss: 18, Average loss: 1.2197582655482822\n",
      "Len of Validation loss: 54, Average loss: 3.38749838758398\n",
      "Epoch: 2767, Len of Training loss: 18, Average loss: 1.142034073670705\n",
      "Len of Validation loss: 54, Average loss: 3.3943221701516046\n",
      "Epoch: 2768, Len of Training loss: 18, Average loss: 1.12974347670873\n",
      "Len of Validation loss: 54, Average loss: 3.500977282170896\n",
      "Epoch: 2769, Len of Training loss: 18, Average loss: 1.282559288872613\n",
      "Len of Validation loss: 54, Average loss: 3.68365415930748\n",
      "Epoch: 2770, Len of Training loss: 18, Average loss: 1.5592782431178622\n",
      "Len of Validation loss: 54, Average loss: 3.7209436275340892\n",
      "Epoch: 2771, Len of Training loss: 18, Average loss: 1.3371482027901544\n",
      "Len of Validation loss: 54, Average loss: 3.495043925665043\n",
      "Epoch: 2772, Len of Training loss: 18, Average loss: 1.33608611424764\n",
      "Len of Validation loss: 54, Average loss: 3.640048927730984\n",
      "Epoch: 2773, Len of Training loss: 18, Average loss: 1.2146603067715962\n",
      "Len of Validation loss: 54, Average loss: 3.7017380208880812\n",
      "Epoch: 2774, Len of Training loss: 18, Average loss: 1.2027029527558222\n",
      "Len of Validation loss: 54, Average loss: 3.43245467543602\n",
      "Epoch: 2775, Len of Training loss: 18, Average loss: 1.2477220363087125\n",
      "Len of Validation loss: 54, Average loss: 3.5836282030299857\n",
      "Epoch: 2776, Len of Training loss: 18, Average loss: 1.3578582670953538\n",
      "Len of Validation loss: 54, Average loss: 3.776512157033991\n",
      "Epoch: 2777, Len of Training loss: 18, Average loss: 1.3510835766792297\n",
      "Len of Validation loss: 54, Average loss: 3.567554674766682\n",
      "Epoch: 2778, Len of Training loss: 18, Average loss: 1.29025180472268\n",
      "Len of Validation loss: 54, Average loss: 3.7864293257395425\n",
      "Epoch: 2779, Len of Training loss: 18, Average loss: 1.1607422100173101\n",
      "Len of Validation loss: 54, Average loss: 3.4871789161805755\n",
      "Epoch: 2780, Len of Training loss: 18, Average loss: 1.3176459405157301\n",
      "Len of Validation loss: 54, Average loss: 3.7285781138473086\n",
      "Epoch: 2781, Len of Training loss: 18, Average loss: 1.3152940140830145\n",
      "Len of Validation loss: 54, Average loss: 3.60346480762517\n",
      "Epoch: 2782, Len of Training loss: 18, Average loss: 1.3061366478602092\n",
      "Len of Validation loss: 54, Average loss: 3.69205504655838\n",
      "Epoch: 2783, Len of Training loss: 18, Average loss: 1.5403422580824957\n",
      "Len of Validation loss: 54, Average loss: 3.7432638141844006\n",
      "Epoch: 2784, Len of Training loss: 18, Average loss: 1.7963258491622076\n",
      "Len of Validation loss: 54, Average loss: 3.8603950187012\n",
      "Epoch: 2785, Len of Training loss: 18, Average loss: 1.8800132870674133\n",
      "Len of Validation loss: 54, Average loss: 3.7128511843857943\n",
      "Epoch: 2786, Len of Training loss: 18, Average loss: 1.735542893409729\n",
      "Len of Validation loss: 54, Average loss: 3.903197862483837\n",
      "Epoch: 2787, Len of Training loss: 18, Average loss: 1.6795545551511977\n",
      "Len of Validation loss: 54, Average loss: 3.597229754483258\n",
      "Epoch: 2788, Len of Training loss: 18, Average loss: 1.6384749544991388\n",
      "Len of Validation loss: 54, Average loss: 3.831032265115667\n",
      "Epoch: 2789, Len of Training loss: 18, Average loss: 1.7398173345459833\n",
      "Len of Validation loss: 54, Average loss: 3.931852998556914\n",
      "Epoch: 2790, Len of Training loss: 18, Average loss: 1.5439160797331068\n",
      "Len of Validation loss: 54, Average loss: 3.681897194297225\n",
      "Epoch: 2791, Len of Training loss: 18, Average loss: 1.3695918056699965\n",
      "Len of Validation loss: 54, Average loss: 3.5443578483881772\n",
      "Epoch: 2792, Len of Training loss: 18, Average loss: 1.2576717138290405\n",
      "Len of Validation loss: 54, Average loss: 3.4041077891985574\n",
      "Epoch: 2793, Len of Training loss: 18, Average loss: 1.172114544444614\n",
      "Len of Validation loss: 54, Average loss: 3.3852237087708934\n",
      "Epoch: 2794, Len of Training loss: 18, Average loss: 1.1245175070232816\n",
      "Len of Validation loss: 54, Average loss: 3.551136768526501\n",
      "Epoch: 2795, Len of Training loss: 18, Average loss: 1.1749378310309515\n",
      "Len of Validation loss: 54, Average loss: 3.531854193519663\n",
      "Epoch: 2796, Len of Training loss: 18, Average loss: 1.1063339445326064\n",
      "Len of Validation loss: 54, Average loss: 3.463641783705464\n",
      "Epoch: 2797, Len of Training loss: 18, Average loss: 1.107340885533227\n",
      "Len of Validation loss: 54, Average loss: 3.566973285542594\n",
      "Epoch: 2798, Len of Training loss: 18, Average loss: 1.076446619298723\n",
      "Len of Validation loss: 54, Average loss: 3.426221079296536\n",
      "Epoch: 2799, Len of Training loss: 18, Average loss: 1.1748451391855876\n",
      "Len of Validation loss: 54, Average loss: 3.660101588125582\n",
      "Epoch: 2800, Len of Training loss: 18, Average loss: 1.1924509670999315\n",
      "Len of Validation loss: 54, Average loss: 3.622416180592996\n",
      "Epoch: 2801, Len of Training loss: 18, Average loss: 1.2244810263315837\n",
      "Len of Validation loss: 54, Average loss: 3.7848465972476535\n",
      "Epoch: 2802, Len of Training loss: 18, Average loss: 1.12215922276179\n",
      "Len of Validation loss: 54, Average loss: 3.586875668278447\n",
      "Epoch: 2803, Len of Training loss: 18, Average loss: 1.1387197640207078\n",
      "Len of Validation loss: 54, Average loss: 3.438063397451683\n",
      "Epoch: 2804, Len of Training loss: 18, Average loss: 1.1048282351758745\n",
      "Len of Validation loss: 54, Average loss: 3.525186170030523\n",
      "Epoch: 2805, Len of Training loss: 18, Average loss: 1.1636242005560133\n",
      "Len of Validation loss: 54, Average loss: 3.4115517923125513\n",
      "Epoch: 2806, Len of Training loss: 18, Average loss: 1.252364542749193\n",
      "Len of Validation loss: 54, Average loss: 3.5857386456595526\n",
      "Epoch: 2807, Len of Training loss: 18, Average loss: 1.139861371782091\n",
      "Len of Validation loss: 54, Average loss: 3.3535600039694042\n",
      "Epoch: 2808, Len of Training loss: 18, Average loss: 1.1446501778231726\n",
      "Len of Validation loss: 54, Average loss: 3.6352339530432665\n",
      "Epoch: 2809, Len of Training loss: 18, Average loss: 1.2442307207319472\n",
      "Len of Validation loss: 54, Average loss: 3.5571522370532707\n",
      "Epoch: 2810, Len of Training loss: 18, Average loss: 1.1812054614226024\n",
      "Len of Validation loss: 54, Average loss: 3.6899610426690845\n",
      "Epoch: 2811, Len of Training loss: 18, Average loss: 1.3318552043702867\n",
      "Len of Validation loss: 54, Average loss: 3.9873331277458757\n",
      "Epoch: 2812, Len of Training loss: 18, Average loss: 1.2848574651612177\n",
      "Len of Validation loss: 54, Average loss: 3.524878245812875\n",
      "Epoch: 2813, Len of Training loss: 18, Average loss: 1.3242588639259338\n",
      "Len of Validation loss: 54, Average loss: 3.7068285081121655\n",
      "Epoch: 2814, Len of Training loss: 18, Average loss: 1.5176814132266574\n",
      "Len of Validation loss: 54, Average loss: 3.570721396693477\n",
      "Epoch: 2815, Len of Training loss: 18, Average loss: 1.5748080611228943\n",
      "Len of Validation loss: 54, Average loss: 3.54818527897199\n",
      "Epoch: 2816, Len of Training loss: 18, Average loss: 1.3546597560246785\n",
      "Len of Validation loss: 54, Average loss: 3.6649834425361068\n",
      "Epoch: 2817, Len of Training loss: 18, Average loss: 1.3001828922165766\n",
      "Len of Validation loss: 54, Average loss: 3.5005272373005196\n",
      "Epoch: 2818, Len of Training loss: 18, Average loss: 1.2217329608069525\n",
      "Len of Validation loss: 54, Average loss: 3.5016959221274764\n",
      "Epoch: 2819, Len of Training loss: 18, Average loss: 1.375958502292633\n",
      "Len of Validation loss: 54, Average loss: 3.633964119134126\n",
      "Epoch: 2820, Len of Training loss: 18, Average loss: 1.3626864751180012\n",
      "Len of Validation loss: 54, Average loss: 3.642931412767481\n",
      "Epoch: 2821, Len of Training loss: 18, Average loss: 1.3632247116830614\n",
      "Len of Validation loss: 54, Average loss: 3.6920411233548767\n",
      "Epoch: 2822, Len of Training loss: 18, Average loss: 1.2508113119337294\n",
      "Len of Validation loss: 54, Average loss: 3.725091446329046\n",
      "Epoch: 2823, Len of Training loss: 18, Average loss: 1.1818828052944608\n",
      "Len of Validation loss: 54, Average loss: 3.5601095380606473\n",
      "Epoch: 2824, Len of Training loss: 18, Average loss: 1.2891311314370897\n",
      "Len of Validation loss: 54, Average loss: 3.3818636680090868\n",
      "Epoch: 2825, Len of Training loss: 18, Average loss: 1.2784185475773282\n",
      "Len of Validation loss: 54, Average loss: 3.7254026421794184\n",
      "Epoch: 2826, Len of Training loss: 18, Average loss: 1.27215842405955\n",
      "Len of Validation loss: 54, Average loss: 3.5670894516838922\n",
      "Epoch: 2827, Len of Training loss: 18, Average loss: 1.122667180167304\n",
      "Len of Validation loss: 54, Average loss: 3.576844529973136\n",
      "Epoch: 2828, Len of Training loss: 18, Average loss: 1.1445640060636733\n",
      "Len of Validation loss: 54, Average loss: 3.3659384603853577\n",
      "Epoch: 2829, Len of Training loss: 18, Average loss: 1.1324710647265117\n",
      "Len of Validation loss: 54, Average loss: 3.46464662860941\n",
      "Epoch: 2830, Len of Training loss: 18, Average loss: 1.1609662241405911\n",
      "Len of Validation loss: 54, Average loss: 3.663674184569606\n",
      "Epoch: 2831, Len of Training loss: 18, Average loss: 1.2071949574682448\n",
      "Len of Validation loss: 54, Average loss: 3.42531685144813\n",
      "Epoch: 2832, Len of Training loss: 18, Average loss: 1.1736136343744066\n",
      "Len of Validation loss: 54, Average loss: 3.420152947858528\n",
      "Epoch: 2833, Len of Training loss: 18, Average loss: 1.2209715478950076\n",
      "Len of Validation loss: 54, Average loss: 3.716397339547122\n",
      "Epoch: 2834, Len of Training loss: 18, Average loss: 1.1488622956805759\n",
      "Len of Validation loss: 54, Average loss: 3.697215125516609\n",
      "Epoch: 2835, Len of Training loss: 18, Average loss: 1.2073068618774414\n",
      "Len of Validation loss: 54, Average loss: 3.5302714153572365\n",
      "Epoch: 2836, Len of Training loss: 18, Average loss: 1.195229450861613\n",
      "Len of Validation loss: 54, Average loss: 3.744855835481926\n",
      "Epoch: 2837, Len of Training loss: 18, Average loss: 1.2372257245911493\n",
      "Len of Validation loss: 54, Average loss: 3.4053038745014756\n",
      "Epoch: 2838, Len of Training loss: 18, Average loss: 1.2471812102529738\n",
      "Len of Validation loss: 54, Average loss: 3.354714604439559\n",
      "Epoch: 2839, Len of Training loss: 18, Average loss: 1.0959822800424364\n",
      "Len of Validation loss: 54, Average loss: 3.4395153202392437\n",
      "Epoch: 2840, Len of Training loss: 18, Average loss: 1.0638097491529253\n",
      "Len of Validation loss: 54, Average loss: 3.6758938199943967\n",
      "Epoch: 2841, Len of Training loss: 18, Average loss: 1.0718047089046903\n",
      "Len of Validation loss: 54, Average loss: 3.6249975164731345\n",
      "Epoch: 2842, Len of Training loss: 18, Average loss: 1.1011456317371793\n",
      "Len of Validation loss: 54, Average loss: 3.76521983632335\n",
      "Epoch: 2843, Len of Training loss: 18, Average loss: 1.0864725609620411\n",
      "Len of Validation loss: 54, Average loss: 3.376021161123558\n",
      "Epoch: 2844, Len of Training loss: 18, Average loss: 1.1252745588620503\n",
      "Len of Validation loss: 54, Average loss: 3.5517182604030326\n",
      "Epoch: 2845, Len of Training loss: 18, Average loss: 1.199286202589671\n",
      "Len of Validation loss: 54, Average loss: 3.6343243651919894\n",
      "Epoch: 2846, Len of Training loss: 18, Average loss: 1.3350387281841702\n",
      "Len of Validation loss: 54, Average loss: 3.671162458481612\n",
      "Epoch: 2847, Len of Training loss: 18, Average loss: 1.2261328432295058\n",
      "Len of Validation loss: 54, Average loss: 3.631165260518039\n",
      "Epoch: 2848, Len of Training loss: 18, Average loss: 1.4383142391840618\n",
      "Len of Validation loss: 54, Average loss: 3.6859551160423845\n",
      "Epoch: 2849, Len of Training loss: 18, Average loss: 1.427725460794237\n",
      "Len of Validation loss: 54, Average loss: 3.662554528978136\n",
      "Epoch: 2850, Len of Training loss: 18, Average loss: 1.4736619657940335\n",
      "Len of Validation loss: 54, Average loss: 3.4930835873992354\n",
      "Epoch: 2851, Len of Training loss: 18, Average loss: 1.2805015113618639\n",
      "Len of Validation loss: 54, Average loss: 3.526833102658943\n",
      "Epoch: 2852, Len of Training loss: 18, Average loss: 1.2017651266521878\n",
      "Len of Validation loss: 54, Average loss: 3.4319859710004597\n",
      "Epoch: 2853, Len of Training loss: 18, Average loss: 1.1181114580896165\n",
      "Len of Validation loss: 54, Average loss: 3.3409513742835433\n",
      "Epoch: 2854, Len of Training loss: 18, Average loss: 1.1138894889089797\n",
      "Len of Validation loss: 54, Average loss: 3.5135080130011946\n",
      "Epoch: 2855, Len of Training loss: 18, Average loss: 1.132563312848409\n",
      "Len of Validation loss: 54, Average loss: 3.6742725979398796\n",
      "Epoch: 2856, Len of Training loss: 18, Average loss: 1.1048260066244338\n",
      "Len of Validation loss: 54, Average loss: 3.3670658623730696\n",
      "Epoch: 2857, Len of Training loss: 18, Average loss: 1.0863583419058058\n",
      "Len of Validation loss: 54, Average loss: 3.3311975675600545\n",
      "Epoch: 2858, Len of Training loss: 18, Average loss: 1.0298151870568593\n",
      "Len of Validation loss: 54, Average loss: 3.2551695384361126\n",
      "Epoch: 2859, Len of Training loss: 18, Average loss: 1.0455492337544758\n",
      "Len of Validation loss: 54, Average loss: 3.5504089649076813\n",
      "Epoch: 2860, Len of Training loss: 18, Average loss: 1.1580118536949158\n",
      "Len of Validation loss: 54, Average loss: 3.6357185675038233\n",
      "Epoch: 2861, Len of Training loss: 18, Average loss: 1.2764889068073697\n",
      "Len of Validation loss: 54, Average loss: 3.567990349398719\n",
      "Epoch: 2862, Len of Training loss: 18, Average loss: 1.1937264270252652\n",
      "Len of Validation loss: 54, Average loss: 3.289054681857427\n",
      "Epoch: 2863, Len of Training loss: 18, Average loss: 1.1896044810612996\n",
      "Len of Validation loss: 54, Average loss: 3.4325196588480913\n",
      "Epoch: 2864, Len of Training loss: 18, Average loss: 1.1929851902855768\n",
      "Len of Validation loss: 54, Average loss: 3.6164284425753133\n",
      "Epoch: 2865, Len of Training loss: 18, Average loss: 1.1306252545780606\n",
      "Len of Validation loss: 54, Average loss: 3.44256811119892\n",
      "Epoch: 2866, Len of Training loss: 18, Average loss: 1.3058257963922288\n",
      "Len of Validation loss: 54, Average loss: 3.262534800502989\n",
      "Epoch: 2867, Len of Training loss: 18, Average loss: 1.1903795136345758\n",
      "Len of Validation loss: 54, Average loss: 3.386228645289386\n",
      "Epoch: 2868, Len of Training loss: 18, Average loss: 1.173458370897505\n",
      "Len of Validation loss: 54, Average loss: 3.5836543959599956\n",
      "Epoch: 2869, Len of Training loss: 18, Average loss: 1.18515764342414\n",
      "Len of Validation loss: 54, Average loss: 3.653208425751439\n",
      "Epoch: 2870, Len of Training loss: 18, Average loss: 1.595854189660814\n",
      "Len of Validation loss: 54, Average loss: 3.9833041142534324\n",
      "Epoch: 2871, Len of Training loss: 18, Average loss: 1.5135646528667874\n",
      "Len of Validation loss: 54, Average loss: 3.7154537395194724\n",
      "Epoch: 2872, Len of Training loss: 18, Average loss: 1.2583464450306363\n",
      "Len of Validation loss: 54, Average loss: 3.6487452575454005\n",
      "Epoch: 2873, Len of Training loss: 18, Average loss: 1.1406645609272852\n",
      "Len of Validation loss: 54, Average loss: 3.3018211987283497\n",
      "Epoch: 2874, Len of Training loss: 18, Average loss: 1.2790188325775995\n",
      "Len of Validation loss: 54, Average loss: 3.6370049428056785\n",
      "Epoch: 2875, Len of Training loss: 18, Average loss: 1.288415014743805\n",
      "Len of Validation loss: 54, Average loss: 3.832694947719574\n",
      "Epoch: 2876, Len of Training loss: 18, Average loss: 1.2378817598025005\n",
      "Len of Validation loss: 54, Average loss: 3.4879667924510107\n",
      "Epoch: 2877, Len of Training loss: 18, Average loss: 1.179716365204917\n",
      "Len of Validation loss: 54, Average loss: 3.817220421852889\n",
      "Epoch: 2878, Len of Training loss: 18, Average loss: 1.1860557662116156\n",
      "Len of Validation loss: 54, Average loss: 3.473927593893475\n",
      "Epoch: 2879, Len of Training loss: 18, Average loss: 1.1284498108757868\n",
      "Len of Validation loss: 54, Average loss: 3.667068995811321\n",
      "Epoch: 2880, Len of Training loss: 18, Average loss: 1.0942882034513686\n",
      "Len of Validation loss: 54, Average loss: 3.547319910040608\n",
      "Epoch: 2881, Len of Training loss: 18, Average loss: 1.0952771570947435\n",
      "Len of Validation loss: 54, Average loss: 3.494466245174408\n",
      "Epoch: 2882, Len of Training loss: 18, Average loss: 1.1392522321807013\n",
      "Len of Validation loss: 54, Average loss: 3.4304496623851635\n",
      "Epoch: 2883, Len of Training loss: 18, Average loss: 1.2486200398868985\n",
      "Len of Validation loss: 54, Average loss: 3.4918658093169883\n",
      "Epoch: 2884, Len of Training loss: 18, Average loss: 1.2481717997127109\n",
      "Len of Validation loss: 54, Average loss: 3.3070225450727673\n",
      "Epoch: 2885, Len of Training loss: 18, Average loss: 1.2764908803833857\n",
      "Len of Validation loss: 54, Average loss: 3.605426606204775\n",
      "Epoch: 2886, Len of Training loss: 18, Average loss: 1.2440029978752136\n",
      "Len of Validation loss: 54, Average loss: 3.590852514461235\n",
      "Epoch: 2887, Len of Training loss: 18, Average loss: 1.5519800914658441\n",
      "Len of Validation loss: 54, Average loss: 3.7787441280153065\n",
      "Epoch: 2888, Len of Training loss: 18, Average loss: 1.7954582042164273\n",
      "Len of Validation loss: 54, Average loss: 3.5837031271722584\n",
      "Epoch: 2889, Len of Training loss: 18, Average loss: 1.5158189800050523\n",
      "Len of Validation loss: 54, Average loss: 3.7919700388555175\n",
      "Epoch: 2890, Len of Training loss: 18, Average loss: 1.4492597712410822\n",
      "Len of Validation loss: 54, Average loss: 3.5988906268720275\n",
      "Epoch: 2891, Len of Training loss: 18, Average loss: 1.2750000026490953\n",
      "Len of Validation loss: 54, Average loss: 3.327669531106949\n",
      "Epoch: 2892, Len of Training loss: 18, Average loss: 1.1662628716892667\n",
      "Len of Validation loss: 54, Average loss: 3.5542723249506065\n",
      "Epoch: 2893, Len of Training loss: 18, Average loss: 1.0888979997899797\n",
      "Len of Validation loss: 54, Average loss: 3.4452305712081768\n",
      "Epoch: 2894, Len of Training loss: 18, Average loss: 1.2550384865866766\n",
      "Len of Validation loss: 54, Average loss: 3.6026185287369623\n",
      "Epoch: 2895, Len of Training loss: 18, Average loss: 1.3518654306729634\n",
      "Len of Validation loss: 54, Average loss: 3.5789886282549963\n",
      "Epoch: 2896, Len of Training loss: 18, Average loss: 1.3036301467153761\n",
      "Len of Validation loss: 54, Average loss: 3.460457971802464\n",
      "Epoch: 2897, Len of Training loss: 18, Average loss: 1.2212458319134183\n",
      "Len of Validation loss: 54, Average loss: 3.6865372679851673\n",
      "Epoch: 2898, Len of Training loss: 18, Average loss: 1.4294579890039232\n",
      "Len of Validation loss: 54, Average loss: 3.784188374325081\n",
      "Epoch: 2899, Len of Training loss: 18, Average loss: 1.3801147937774658\n",
      "Len of Validation loss: 54, Average loss: 3.4924791521496243\n",
      "Epoch: 2900, Len of Training loss: 18, Average loss: 1.2282235357496474\n",
      "Len of Validation loss: 54, Average loss: 3.4744797503506697\n",
      "Epoch: 2901, Len of Training loss: 18, Average loss: 1.1440972950723436\n",
      "Len of Validation loss: 54, Average loss: 3.4540642820022724\n",
      "Epoch: 2902, Len of Training loss: 18, Average loss: 1.208207991388109\n",
      "Len of Validation loss: 54, Average loss: 3.4800252693670766\n",
      "Epoch: 2903, Len of Training loss: 18, Average loss: 1.143973218070136\n",
      "Len of Validation loss: 54, Average loss: 3.7008606173374035\n",
      "Epoch: 2904, Len of Training loss: 18, Average loss: 1.1162211696306865\n",
      "Len of Validation loss: 54, Average loss: 3.793781876564026\n",
      "Epoch: 2905, Len of Training loss: 18, Average loss: 1.2122786574893527\n",
      "Len of Validation loss: 54, Average loss: 3.782778298413312\n",
      "Epoch: 2906, Len of Training loss: 18, Average loss: 1.2618272304534912\n",
      "Len of Validation loss: 54, Average loss: 3.4529235197438135\n",
      "Epoch: 2907, Len of Training loss: 18, Average loss: 1.197026530901591\n",
      "Len of Validation loss: 54, Average loss: 3.4174250761667886\n",
      "Epoch: 2908, Len of Training loss: 18, Average loss: 1.2027360929383173\n",
      "Len of Validation loss: 54, Average loss: 3.3801384932465024\n",
      "Epoch: 2909, Len of Training loss: 18, Average loss: 1.1275884244177077\n",
      "Len of Validation loss: 54, Average loss: 3.893545506177125\n",
      "Epoch: 2910, Len of Training loss: 18, Average loss: 1.3507268296347723\n",
      "Len of Validation loss: 54, Average loss: 3.534977064088539\n",
      "Epoch: 2911, Len of Training loss: 18, Average loss: 1.2737286885579426\n",
      "Len of Validation loss: 54, Average loss: 3.4561945222042225\n",
      "Epoch: 2912, Len of Training loss: 18, Average loss: 1.2330043779479132\n",
      "Len of Validation loss: 54, Average loss: 3.40752613875601\n",
      "Epoch: 2913, Len of Training loss: 18, Average loss: 1.2816154095861647\n",
      "Len of Validation loss: 54, Average loss: 3.516299316176662\n",
      "Epoch: 2914, Len of Training loss: 18, Average loss: 1.235532091723548\n",
      "Len of Validation loss: 54, Average loss: 3.6422096016230405\n",
      "Epoch: 2915, Len of Training loss: 18, Average loss: 1.231900566154056\n",
      "Len of Validation loss: 54, Average loss: 3.729452916869411\n",
      "Epoch: 2916, Len of Training loss: 18, Average loss: 1.2964547938770719\n",
      "Len of Validation loss: 54, Average loss: 3.6898105519789235\n",
      "Epoch: 2917, Len of Training loss: 18, Average loss: 1.40306736363305\n",
      "Len of Validation loss: 54, Average loss: 3.8940654507389776\n",
      "Epoch: 2918, Len of Training loss: 18, Average loss: 1.2951246036423578\n",
      "Len of Validation loss: 54, Average loss: 3.5236783425013223\n",
      "Epoch: 2919, Len of Training loss: 18, Average loss: 1.1810854077339172\n",
      "Len of Validation loss: 54, Average loss: 3.5104532738526664\n",
      "Epoch: 2920, Len of Training loss: 18, Average loss: 1.067168954345915\n",
      "Len of Validation loss: 54, Average loss: 3.351379945322319\n",
      "Epoch: 2921, Len of Training loss: 18, Average loss: 1.049834085835351\n",
      "Len of Validation loss: 54, Average loss: 3.6813975197297557\n",
      "Epoch: 2922, Len of Training loss: 18, Average loss: 1.094303501976861\n",
      "Len of Validation loss: 54, Average loss: 3.491056776709027\n",
      "Epoch: 2923, Len of Training loss: 18, Average loss: 1.2230715420511034\n",
      "Len of Validation loss: 54, Average loss: 3.438548352983263\n",
      "Epoch: 2924, Len of Training loss: 18, Average loss: 1.2634327014287312\n",
      "Len of Validation loss: 54, Average loss: 3.5613390538427563\n",
      "Epoch: 2925, Len of Training loss: 18, Average loss: 1.1660007105933294\n",
      "Len of Validation loss: 54, Average loss: 3.408320986562305\n",
      "Epoch: 2926, Len of Training loss: 18, Average loss: 1.1290669639905293\n",
      "Len of Validation loss: 54, Average loss: 3.398406785947305\n",
      "Epoch: 2927, Len of Training loss: 18, Average loss: 1.1418714887566037\n",
      "Len of Validation loss: 54, Average loss: 3.4683700844093606\n",
      "Epoch: 2928, Len of Training loss: 18, Average loss: 1.0934298468960657\n",
      "Len of Validation loss: 54, Average loss: 3.295661468196798\n",
      "Epoch: 2929, Len of Training loss: 18, Average loss: 1.1125556098090277\n",
      "Len of Validation loss: 54, Average loss: 3.4037399634167\n",
      "Epoch: 2930, Len of Training loss: 18, Average loss: 1.1344231963157654\n",
      "Len of Validation loss: 54, Average loss: 3.5252040260367923\n",
      "Epoch: 2931, Len of Training loss: 18, Average loss: 1.0894050498803456\n",
      "Len of Validation loss: 54, Average loss: 3.559124317434099\n",
      "Epoch: 2932, Len of Training loss: 18, Average loss: 1.1746031443277996\n",
      "Len of Validation loss: 54, Average loss: 3.435490749500416\n",
      "Epoch: 2933, Len of Training loss: 18, Average loss: 1.2318240139219496\n",
      "Len of Validation loss: 54, Average loss: 3.6931360794438257\n",
      "Epoch: 2934, Len of Training loss: 18, Average loss: 1.4100085629357233\n",
      "Len of Validation loss: 54, Average loss: 3.568928729604792\n",
      "Epoch: 2935, Len of Training loss: 18, Average loss: 1.2653233541382685\n",
      "Len of Validation loss: 54, Average loss: 3.54500510957506\n",
      "Epoch: 2936, Len of Training loss: 18, Average loss: 1.2215812802314758\n",
      "Len of Validation loss: 54, Average loss: 3.5369729896386466\n",
      "Epoch: 2937, Len of Training loss: 18, Average loss: 1.2292229930559795\n",
      "Len of Validation loss: 54, Average loss: 3.5576793948809304\n",
      "Epoch: 2938, Len of Training loss: 18, Average loss: 1.172216124004788\n",
      "Len of Validation loss: 54, Average loss: 3.5097059221179396\n",
      "Epoch: 2939, Len of Training loss: 18, Average loss: 1.1581480138831668\n",
      "Len of Validation loss: 54, Average loss: 3.62904351177039\n",
      "Epoch: 2940, Len of Training loss: 18, Average loss: 1.099854313664966\n",
      "Len of Validation loss: 54, Average loss: 3.361900079029578\n",
      "Epoch: 2941, Len of Training loss: 18, Average loss: 1.1657976110776265\n",
      "Len of Validation loss: 54, Average loss: 3.4801165141441204\n",
      "Epoch: 2942, Len of Training loss: 18, Average loss: 1.2455086045795016\n",
      "Len of Validation loss: 54, Average loss: 3.583336012230979\n",
      "Epoch: 2943, Len of Training loss: 18, Average loss: 1.1777729127142165\n",
      "Len of Validation loss: 54, Average loss: 3.663468094887557\n",
      "Epoch: 2944, Len of Training loss: 18, Average loss: 1.139461014005873\n",
      "Len of Validation loss: 54, Average loss: 3.760904747026938\n",
      "Epoch: 2945, Len of Training loss: 18, Average loss: 1.118859522872501\n",
      "Len of Validation loss: 54, Average loss: 3.4436723876882485\n",
      "Epoch: 2946, Len of Training loss: 18, Average loss: 1.0893184608883328\n",
      "Len of Validation loss: 54, Average loss: 3.4435736161691173\n",
      "Epoch: 2947, Len of Training loss: 18, Average loss: 1.20830903450648\n",
      "Len of Validation loss: 54, Average loss: 3.50621615074299\n",
      "Epoch: 2948, Len of Training loss: 18, Average loss: 1.3016174965434604\n",
      "Len of Validation loss: 54, Average loss: 3.6141702135403952\n",
      "Epoch: 2949, Len of Training loss: 18, Average loss: 1.3739947345521715\n",
      "Len of Validation loss: 54, Average loss: 3.7208375643800804\n",
      "Epoch: 2950, Len of Training loss: 18, Average loss: 1.606534673107995\n",
      "Len of Validation loss: 54, Average loss: 3.5637571348084345\n",
      "Epoch: 2951, Len of Training loss: 18, Average loss: 1.424734029504988\n",
      "Len of Validation loss: 54, Average loss: 3.548615797802254\n",
      "Epoch: 2952, Len of Training loss: 18, Average loss: 1.316817647880978\n",
      "Len of Validation loss: 54, Average loss: 3.5039740767743854\n",
      "Epoch: 2953, Len of Training loss: 18, Average loss: 1.2727354897393122\n",
      "Len of Validation loss: 54, Average loss: 3.4500720975575625\n",
      "Epoch: 2954, Len of Training loss: 18, Average loss: 1.3398917118708293\n",
      "Len of Validation loss: 54, Average loss: 3.584348788967839\n",
      "Epoch: 2955, Len of Training loss: 18, Average loss: 1.2002628246943157\n",
      "Len of Validation loss: 54, Average loss: 3.5035849268789643\n",
      "Epoch: 2956, Len of Training loss: 18, Average loss: 1.2183463176091511\n",
      "Len of Validation loss: 54, Average loss: 3.6016943520969815\n",
      "Epoch: 2957, Len of Training loss: 18, Average loss: 1.2027584910392761\n",
      "Len of Validation loss: 54, Average loss: 3.491329387382225\n",
      "Epoch: 2958, Len of Training loss: 18, Average loss: 1.193186382452647\n",
      "Len of Validation loss: 54, Average loss: 3.4145106684278557\n",
      "Epoch: 2959, Len of Training loss: 18, Average loss: 1.0729922983381484\n",
      "Len of Validation loss: 54, Average loss: 3.434344876695562\n",
      "Epoch: 2960, Len of Training loss: 18, Average loss: 1.1989318662219577\n",
      "Len of Validation loss: 54, Average loss: 3.6843454550813743\n",
      "Epoch: 2961, Len of Training loss: 18, Average loss: 1.3486008246739705\n",
      "Len of Validation loss: 54, Average loss: 3.5423744320869446\n",
      "Epoch: 2962, Len of Training loss: 18, Average loss: 1.2813796003659566\n",
      "Len of Validation loss: 54, Average loss: 3.6639964315626354\n",
      "Epoch: 2963, Len of Training loss: 18, Average loss: 1.1582798494233026\n",
      "Len of Validation loss: 54, Average loss: 3.225944859010202\n",
      "Epoch: 2964, Len of Training loss: 18, Average loss: 1.1695689227845933\n",
      "Len of Validation loss: 54, Average loss: 3.5374542794845723\n",
      "Epoch: 2965, Len of Training loss: 18, Average loss: 1.1618939240773518\n",
      "Len of Validation loss: 54, Average loss: 3.4213913049962787\n",
      "Epoch: 2966, Len of Training loss: 18, Average loss: 1.4289300971561008\n",
      "Len of Validation loss: 54, Average loss: 4.018061626840521\n",
      "Epoch: 2967, Len of Training loss: 18, Average loss: 1.4105159044265747\n",
      "Len of Validation loss: 54, Average loss: 3.457323498196072\n",
      "Epoch: 2968, Len of Training loss: 18, Average loss: 1.2300368812349107\n",
      "Len of Validation loss: 54, Average loss: 3.4079212424931704\n",
      "Epoch: 2969, Len of Training loss: 18, Average loss: 1.2682299746407404\n",
      "Len of Validation loss: 54, Average loss: 3.608780050719226\n",
      "Epoch: 2970, Len of Training loss: 18, Average loss: 1.2083530690934923\n",
      "Len of Validation loss: 54, Average loss: 3.5106979178057776\n",
      "Epoch: 2971, Len of Training loss: 18, Average loss: 1.2018657856517367\n",
      "Len of Validation loss: 54, Average loss: 3.558501175156346\n",
      "Epoch: 2972, Len of Training loss: 18, Average loss: 1.3716345429420471\n",
      "Len of Validation loss: 54, Average loss: 3.700295216507382\n",
      "Epoch: 2973, Len of Training loss: 18, Average loss: 1.3995434840520222\n",
      "Len of Validation loss: 54, Average loss: 3.667473900097388\n",
      "Epoch: 2974, Len of Training loss: 18, Average loss: 1.2012363937166002\n",
      "Len of Validation loss: 54, Average loss: 3.426776284420932\n",
      "Epoch: 2975, Len of Training loss: 18, Average loss: 1.1263726883464389\n",
      "Len of Validation loss: 54, Average loss: 3.4512809261127755\n",
      "Epoch: 2976, Len of Training loss: 18, Average loss: 1.1521046161651611\n",
      "Len of Validation loss: 54, Average loss: 3.4425486436596624\n",
      "Epoch: 2977, Len of Training loss: 18, Average loss: 1.1155570911036596\n",
      "Len of Validation loss: 54, Average loss: 3.457947466108534\n",
      "Epoch: 2978, Len of Training loss: 18, Average loss: 1.1475914782947965\n",
      "Len of Validation loss: 54, Average loss: 3.4439382751782737\n",
      "Epoch: 2979, Len of Training loss: 18, Average loss: 1.080455008480284\n",
      "Len of Validation loss: 54, Average loss: 3.4983875541775316\n",
      "Epoch: 2980, Len of Training loss: 18, Average loss: 1.1956896384557087\n",
      "Len of Validation loss: 54, Average loss: 3.4345981975396476\n",
      "Epoch: 2981, Len of Training loss: 18, Average loss: 1.2384219302071466\n",
      "Len of Validation loss: 54, Average loss: 3.627631355215002\n",
      "Epoch: 2982, Len of Training loss: 18, Average loss: 1.2480550673272874\n",
      "Len of Validation loss: 54, Average loss: 3.7632219360934362\n",
      "Epoch: 2983, Len of Training loss: 18, Average loss: 1.4808632532755535\n",
      "Len of Validation loss: 54, Average loss: 3.5919113159179688\n",
      "Epoch: 2984, Len of Training loss: 18, Average loss: 1.4952342443996005\n",
      "Len of Validation loss: 54, Average loss: 3.444808472085882\n",
      "Epoch: 2985, Len of Training loss: 18, Average loss: 1.2630993392732408\n",
      "Len of Validation loss: 54, Average loss: 3.5872626105944314\n",
      "Epoch: 2986, Len of Training loss: 18, Average loss: 1.2343119978904724\n",
      "Len of Validation loss: 54, Average loss: 3.8269125852319927\n",
      "Epoch: 2987, Len of Training loss: 18, Average loss: 1.1965521971384685\n",
      "Len of Validation loss: 54, Average loss: 3.4113425292350628\n",
      "Epoch: 2988, Len of Training loss: 18, Average loss: 1.2049292193518744\n",
      "Len of Validation loss: 54, Average loss: 3.5673696520151914\n",
      "Epoch: 2989, Len of Training loss: 18, Average loss: 1.1408160792456732\n",
      "Len of Validation loss: 54, Average loss: 3.4076897126656993\n",
      "Epoch: 2990, Len of Training loss: 18, Average loss: 1.175316744380527\n",
      "Len of Validation loss: 54, Average loss: 3.5009561479091644\n",
      "Epoch: 2991, Len of Training loss: 18, Average loss: 1.2011907299359639\n",
      "Len of Validation loss: 54, Average loss: 3.3420931785194963\n",
      "Epoch: 2992, Len of Training loss: 18, Average loss: 1.0659876863161724\n",
      "Len of Validation loss: 54, Average loss: 3.5032091758869313\n",
      "Epoch: 2993, Len of Training loss: 18, Average loss: 1.0617397129535675\n",
      "Len of Validation loss: 54, Average loss: 3.4678013589647083\n",
      "Epoch: 2994, Len of Training loss: 18, Average loss: 1.0958925353156195\n",
      "Len of Validation loss: 54, Average loss: 3.333287831809786\n",
      "Epoch: 2995, Len of Training loss: 18, Average loss: 1.0914060042964087\n",
      "Len of Validation loss: 54, Average loss: 3.57550581185906\n",
      "Epoch: 2996, Len of Training loss: 18, Average loss: 1.1382669044865503\n",
      "Len of Validation loss: 54, Average loss: 3.684365371863047\n",
      "Epoch: 2997, Len of Training loss: 18, Average loss: 1.162198907799191\n",
      "Len of Validation loss: 54, Average loss: 3.4912887579864926\n",
      "Epoch: 2998, Len of Training loss: 18, Average loss: 1.1063123842080433\n",
      "Len of Validation loss: 54, Average loss: 3.5611511093598827\n",
      "Epoch: 2999, Len of Training loss: 18, Average loss: 1.121090246571435\n",
      "Len of Validation loss: 54, Average loss: 3.498839024040434\n",
      "Epoch: 3000, Len of Training loss: 18, Average loss: 1.2518751356336806\n",
      "Len of Validation loss: 54, Average loss: 3.5219693040406264\n",
      "Epoch: 3001, Len of Training loss: 18, Average loss: 1.1515013708008661\n",
      "Len of Validation loss: 54, Average loss: 3.5506125489870706\n",
      "Epoch: 3002, Len of Training loss: 18, Average loss: 1.1851989229520161\n",
      "Len of Validation loss: 54, Average loss: 3.43988048809546\n",
      "Epoch: 3003, Len of Training loss: 18, Average loss: 1.1235466930601332\n",
      "Len of Validation loss: 54, Average loss: 3.664263623732108\n",
      "Epoch: 3004, Len of Training loss: 18, Average loss: 1.3962248232629564\n",
      "Len of Validation loss: 54, Average loss: 3.710123429695765\n",
      "Epoch: 3005, Len of Training loss: 18, Average loss: 1.7078430983755324\n",
      "Len of Validation loss: 54, Average loss: 3.8464295113528215\n",
      "Epoch: 3006, Len of Training loss: 18, Average loss: 1.344918131828308\n",
      "Len of Validation loss: 54, Average loss: 3.64847805985698\n",
      "Epoch: 3007, Len of Training loss: 18, Average loss: 1.278648230764601\n",
      "Len of Validation loss: 54, Average loss: 3.493005339746122\n",
      "Epoch: 3008, Len of Training loss: 18, Average loss: 1.2644985185729132\n",
      "Len of Validation loss: 54, Average loss: 3.548644479778078\n",
      "Epoch: 3009, Len of Training loss: 18, Average loss: 1.16699441936281\n",
      "Len of Validation loss: 54, Average loss: 3.5288261671861014\n",
      "Epoch: 3010, Len of Training loss: 18, Average loss: 1.14765469233195\n",
      "Len of Validation loss: 54, Average loss: 3.4439378833329237\n",
      "Epoch: 3011, Len of Training loss: 18, Average loss: 1.144817630449931\n",
      "Len of Validation loss: 54, Average loss: 3.595520117768535\n",
      "Epoch: 3012, Len of Training loss: 18, Average loss: 1.1463668644428253\n",
      "Len of Validation loss: 54, Average loss: 3.592556380563312\n",
      "Epoch: 3013, Len of Training loss: 18, Average loss: 1.1366511086622875\n",
      "Len of Validation loss: 54, Average loss: 3.4776558103384794\n",
      "Epoch: 3014, Len of Training loss: 18, Average loss: 1.0493810739782121\n",
      "Len of Validation loss: 54, Average loss: 3.549655975015075\n",
      "Epoch: 3015, Len of Training loss: 18, Average loss: 1.1235908601019118\n",
      "Len of Validation loss: 54, Average loss: 3.3906239215974456\n",
      "Epoch: 3016, Len of Training loss: 18, Average loss: 1.333218601014879\n",
      "Len of Validation loss: 54, Average loss: 3.702689622287397\n",
      "Epoch: 3017, Len of Training loss: 18, Average loss: 1.4994480543666415\n",
      "Len of Validation loss: 54, Average loss: 4.2260947382008585\n",
      "Epoch: 3018, Len of Training loss: 18, Average loss: 1.6735414266586304\n",
      "Len of Validation loss: 54, Average loss: 3.6136970895308034\n",
      "Epoch: 3019, Len of Training loss: 18, Average loss: 1.3471550676557753\n",
      "Len of Validation loss: 54, Average loss: 3.66939416858885\n",
      "Epoch: 3020, Len of Training loss: 18, Average loss: 1.449399597114987\n",
      "Len of Validation loss: 54, Average loss: 4.137916730509864\n",
      "Epoch: 3021, Len of Training loss: 18, Average loss: 1.9316861894395616\n",
      "Len of Validation loss: 54, Average loss: 3.7557969280967005\n",
      "Epoch: 3022, Len of Training loss: 18, Average loss: 1.5286126335461934\n",
      "Len of Validation loss: 54, Average loss: 3.947409958751113\n",
      "Epoch: 3023, Len of Training loss: 18, Average loss: 1.7778765492969089\n",
      "Len of Validation loss: 54, Average loss: 3.6622712038181446\n",
      "Epoch: 3024, Len of Training loss: 18, Average loss: 1.5468228724267747\n",
      "Len of Validation loss: 54, Average loss: 3.5830490258004932\n",
      "Epoch: 3025, Len of Training loss: 18, Average loss: 1.35934227042728\n",
      "Len of Validation loss: 54, Average loss: 3.545935606514966\n",
      "Epoch: 3026, Len of Training loss: 18, Average loss: 1.3903532690472074\n",
      "Len of Validation loss: 54, Average loss: 3.9671110212802887\n",
      "Epoch: 3027, Len of Training loss: 18, Average loss: 1.394151747226715\n",
      "Len of Validation loss: 54, Average loss: 3.4443849192725287\n",
      "Epoch: 3028, Len of Training loss: 18, Average loss: 1.1923739777670965\n",
      "Len of Validation loss: 54, Average loss: 3.3611742191844516\n",
      "Epoch: 3029, Len of Training loss: 18, Average loss: 1.143790904018614\n",
      "Len of Validation loss: 54, Average loss: 3.638450570680477\n",
      "Epoch: 3030, Len of Training loss: 18, Average loss: 1.1326140960057576\n",
      "Len of Validation loss: 54, Average loss: 3.618436473387259\n",
      "Epoch: 3031, Len of Training loss: 18, Average loss: 1.1015086107783847\n",
      "Len of Validation loss: 54, Average loss: 3.5460062623023987\n",
      "Epoch: 3032, Len of Training loss: 18, Average loss: 1.092626843187544\n",
      "Len of Validation loss: 54, Average loss: 3.453949080573188\n",
      "Epoch: 3033, Len of Training loss: 18, Average loss: 1.117463231086731\n",
      "Len of Validation loss: 54, Average loss: 3.50096732378006\n",
      "Epoch: 3034, Len of Training loss: 18, Average loss: 1.2012198203139834\n",
      "Len of Validation loss: 54, Average loss: 3.4070289929707847\n",
      "Epoch: 3035, Len of Training loss: 18, Average loss: 1.0789184040493436\n",
      "Len of Validation loss: 54, Average loss: 3.6512411965264215\n",
      "Epoch: 3036, Len of Training loss: 18, Average loss: 1.0435073375701904\n",
      "Len of Validation loss: 54, Average loss: 3.6374687442073115\n",
      "Epoch: 3037, Len of Training loss: 18, Average loss: 1.3883813354704115\n",
      "Len of Validation loss: 54, Average loss: 3.5646400882138147\n",
      "Epoch: 3038, Len of Training loss: 18, Average loss: 1.258612400955624\n",
      "Len of Validation loss: 54, Average loss: 3.827458240367748\n",
      "Epoch: 3039, Len of Training loss: 18, Average loss: 1.303379840321011\n",
      "Len of Validation loss: 54, Average loss: 3.771350309804634\n",
      "Epoch: 3040, Len of Training loss: 18, Average loss: 1.1530440714624193\n",
      "Len of Validation loss: 54, Average loss: 3.4672531331026994\n",
      "Epoch: 3041, Len of Training loss: 18, Average loss: 1.1033807463116116\n",
      "Len of Validation loss: 54, Average loss: 3.570843741849617\n",
      "Epoch: 3042, Len of Training loss: 18, Average loss: 1.1569117969936795\n",
      "Len of Validation loss: 54, Average loss: 3.508600135644277\n",
      "Epoch: 3043, Len of Training loss: 18, Average loss: 1.0793981287214491\n",
      "Len of Validation loss: 54, Average loss: 3.5125844931160963\n",
      "Epoch: 3044, Len of Training loss: 18, Average loss: 1.1262307167053223\n",
      "Len of Validation loss: 54, Average loss: 3.5844883918762207\n",
      "Epoch: 3045, Len of Training loss: 18, Average loss: 1.2484210067325168\n",
      "Len of Validation loss: 54, Average loss: 3.6246461735831366\n",
      "Epoch: 3046, Len of Training loss: 18, Average loss: 1.1371964150004916\n",
      "Len of Validation loss: 54, Average loss: 3.5618956983089447\n",
      "Epoch: 3047, Len of Training loss: 18, Average loss: 1.0545781287882063\n",
      "Len of Validation loss: 54, Average loss: 3.5563337681470095\n",
      "Epoch: 3048, Len of Training loss: 18, Average loss: 1.0442240801122453\n",
      "Len of Validation loss: 54, Average loss: 3.383586467416198\n",
      "Epoch: 3049, Len of Training loss: 18, Average loss: 1.1323181721899245\n",
      "Len of Validation loss: 54, Average loss: 3.443048459512216\n",
      "Epoch: 3050, Len of Training loss: 18, Average loss: 1.111967106660207\n",
      "Len of Validation loss: 54, Average loss: 3.281362720109798\n",
      "Epoch: 3051, Len of Training loss: 18, Average loss: 1.337293588452869\n",
      "Len of Validation loss: 54, Average loss: 3.9556998389738576\n",
      "Epoch: 3052, Len of Training loss: 18, Average loss: 1.4348261687490675\n",
      "Len of Validation loss: 54, Average loss: 3.573289707854942\n",
      "Epoch: 3053, Len of Training loss: 18, Average loss: 1.3062707715564303\n",
      "Len of Validation loss: 54, Average loss: 3.6010121217480413\n",
      "Epoch: 3054, Len of Training loss: 18, Average loss: 1.198114812374115\n",
      "Len of Validation loss: 54, Average loss: 3.558114743895001\n",
      "Epoch: 3055, Len of Training loss: 18, Average loss: 1.159343010849423\n",
      "Len of Validation loss: 54, Average loss: 3.5508111814657846\n",
      "Epoch: 3056, Len of Training loss: 18, Average loss: 1.1456600162718031\n",
      "Len of Validation loss: 54, Average loss: 3.6650555950623973\n",
      "Epoch: 3057, Len of Training loss: 18, Average loss: 1.277161955833435\n",
      "Len of Validation loss: 54, Average loss: 3.75832556464054\n",
      "Epoch: 3058, Len of Training loss: 18, Average loss: 1.1499961912631989\n",
      "Len of Validation loss: 54, Average loss: 3.512182433296133\n",
      "Epoch: 3059, Len of Training loss: 18, Average loss: 1.163111686706543\n",
      "Len of Validation loss: 54, Average loss: 3.5612665315469108\n",
      "Epoch: 3060, Len of Training loss: 18, Average loss: 1.190171327855852\n",
      "Len of Validation loss: 54, Average loss: 3.6450870434443154\n",
      "Epoch: 3061, Len of Training loss: 18, Average loss: 1.2532024847136602\n",
      "Len of Validation loss: 54, Average loss: 3.7432874275578394\n",
      "Epoch: 3062, Len of Training loss: 18, Average loss: 1.557389822271135\n",
      "Len of Validation loss: 54, Average loss: 3.8191686692061246\n",
      "Epoch: 3063, Len of Training loss: 18, Average loss: 1.3208155499564276\n",
      "Len of Validation loss: 54, Average loss: 3.630070368448893\n",
      "Epoch: 3064, Len of Training loss: 18, Average loss: 1.2379155158996582\n",
      "Len of Validation loss: 54, Average loss: 3.5266223969282926\n",
      "Epoch: 3065, Len of Training loss: 18, Average loss: 1.139627069234848\n",
      "Len of Validation loss: 54, Average loss: 3.51897829329526\n",
      "Epoch: 3066, Len of Training loss: 18, Average loss: 1.0582266218132443\n",
      "Len of Validation loss: 54, Average loss: 3.452078718830038\n",
      "Epoch: 3067, Len of Training loss: 18, Average loss: 1.0436299078994327\n",
      "Len of Validation loss: 54, Average loss: 3.5085804992251926\n",
      "Epoch: 3068, Len of Training loss: 18, Average loss: 1.082318858967887\n",
      "Len of Validation loss: 54, Average loss: 3.5879116555054984\n",
      "Epoch: 3069, Len of Training loss: 18, Average loss: 1.0867821839120653\n",
      "Len of Validation loss: 54, Average loss: 3.3490908565344633\n",
      "Epoch: 3070, Len of Training loss: 18, Average loss: 1.1179397503534954\n",
      "Len of Validation loss: 54, Average loss: 3.5099843265833677\n",
      "Epoch: 3071, Len of Training loss: 18, Average loss: 1.2358228895399306\n",
      "Len of Validation loss: 54, Average loss: 3.3809714825065047\n",
      "Epoch: 3072, Len of Training loss: 18, Average loss: 1.2317956818474665\n",
      "Len of Validation loss: 54, Average loss: 3.7473737045570656\n",
      "Epoch: 3073, Len of Training loss: 18, Average loss: 1.3237875898679097\n",
      "Len of Validation loss: 54, Average loss: 3.5310047600004406\n",
      "Epoch: 3074, Len of Training loss: 18, Average loss: 1.2064360512627497\n",
      "Len of Validation loss: 54, Average loss: 11.710211118062338\n",
      "Epoch: 3075, Len of Training loss: 18, Average loss: 4.422937499152289\n",
      "Len of Validation loss: 54, Average loss: 4.409383314627188\n",
      "Epoch: 3076, Len of Training loss: 18, Average loss: 2.5754696329434714\n",
      "Len of Validation loss: 54, Average loss: 4.310567562226896\n",
      "Epoch: 3077, Len of Training loss: 18, Average loss: 2.1550879412227206\n",
      "Len of Validation loss: 54, Average loss: 4.015656219588386\n",
      "Epoch: 3078, Len of Training loss: 18, Average loss: 2.0370500683784485\n",
      "Len of Validation loss: 54, Average loss: 3.726699071901816\n",
      "Epoch: 3079, Len of Training loss: 18, Average loss: 1.7144862016042073\n",
      "Len of Validation loss: 54, Average loss: 3.5703613537329213\n",
      "Epoch: 3080, Len of Training loss: 18, Average loss: 1.4899645580185785\n",
      "Len of Validation loss: 54, Average loss: 3.4780862916398934\n",
      "Epoch: 3081, Len of Training loss: 18, Average loss: 1.3692236542701721\n",
      "Len of Validation loss: 54, Average loss: 3.485737710087388\n",
      "Epoch: 3082, Len of Training loss: 18, Average loss: 1.2306695646709866\n",
      "Len of Validation loss: 54, Average loss: 3.4003683350704335\n",
      "Epoch: 3083, Len of Training loss: 18, Average loss: 1.1620418495602078\n",
      "Len of Validation loss: 54, Average loss: 3.4658483697308435\n",
      "Epoch: 3084, Len of Training loss: 18, Average loss: 1.0824087725745306\n",
      "Len of Validation loss: 54, Average loss: 3.3862639003329806\n",
      "Epoch: 3085, Len of Training loss: 18, Average loss: 1.0314913888772328\n",
      "Len of Validation loss: 54, Average loss: 3.433606327683837\n",
      "Epoch: 3086, Len of Training loss: 18, Average loss: 1.0464148885673947\n",
      "Len of Validation loss: 54, Average loss: 3.2456841380507857\n",
      "Epoch: 3087, Len of Training loss: 18, Average loss: 1.1003737482759688\n",
      "Len of Validation loss: 54, Average loss: 3.5648637502281755\n",
      "Epoch: 3088, Len of Training loss: 18, Average loss: 1.2531568474239774\n",
      "Len of Validation loss: 54, Average loss: 3.754301804083365\n",
      "Epoch: 3089, Len of Training loss: 18, Average loss: 1.2716645465956793\n",
      "Len of Validation loss: 54, Average loss: 3.504133212345618\n",
      "Epoch: 3090, Len of Training loss: 18, Average loss: 1.1474628382258945\n",
      "Len of Validation loss: 54, Average loss: 3.3769296297320612\n",
      "Epoch: 3091, Len of Training loss: 18, Average loss: 1.1159684227572546\n",
      "Len of Validation loss: 54, Average loss: 3.427724028075183\n",
      "Epoch: 3092, Len of Training loss: 18, Average loss: 1.152287224928538\n",
      "Len of Validation loss: 54, Average loss: 3.723726162204036\n",
      "Epoch: 3093, Len of Training loss: 18, Average loss: 1.2046081291304693\n",
      "Len of Validation loss: 54, Average loss: 3.4483522883167974\n",
      "Epoch: 3094, Len of Training loss: 18, Average loss: 1.1725366645389133\n",
      "Len of Validation loss: 54, Average loss: 3.4163875215583377\n",
      "Epoch: 3095, Len of Training loss: 18, Average loss: 1.101314256588618\n",
      "Len of Validation loss: 54, Average loss: 3.578556780461912\n",
      "Epoch: 3096, Len of Training loss: 18, Average loss: 1.211640523539649\n",
      "Len of Validation loss: 54, Average loss: 3.3892986200473927\n",
      "Epoch: 3097, Len of Training loss: 18, Average loss: 1.1674640344248877\n",
      "Len of Validation loss: 54, Average loss: 3.43433525606438\n",
      "Epoch: 3098, Len of Training loss: 18, Average loss: 1.144133922126558\n",
      "Len of Validation loss: 54, Average loss: 3.611026468100371\n",
      "Epoch: 3099, Len of Training loss: 18, Average loss: 1.1127457088894315\n",
      "Len of Validation loss: 54, Average loss: 3.3690022848270558\n",
      "Epoch: 3100, Len of Training loss: 18, Average loss: 1.0772748523288302\n",
      "Len of Validation loss: 54, Average loss: 3.400729168344427\n",
      "Epoch: 3101, Len of Training loss: 18, Average loss: 1.009479324022929\n",
      "Len of Validation loss: 54, Average loss: 3.1517022020286984\n",
      "Epoch: 3102, Len of Training loss: 18, Average loss: 1.0956260595056746\n",
      "Len of Validation loss: 54, Average loss: 3.336063657645826\n",
      "Epoch: 3103, Len of Training loss: 18, Average loss: 1.069855683379703\n",
      "Len of Validation loss: 54, Average loss: 3.575767411126031\n",
      "Epoch: 3104, Len of Training loss: 18, Average loss: 1.0891804695129395\n",
      "Len of Validation loss: 54, Average loss: 3.330926169951757\n",
      "Epoch: 3105, Len of Training loss: 18, Average loss: 1.0534270107746124\n",
      "Len of Validation loss: 54, Average loss: 3.449460865170867\n",
      "Epoch: 3106, Len of Training loss: 18, Average loss: 1.0246380865573883\n",
      "Len of Validation loss: 54, Average loss: 3.315467268228531\n",
      "Epoch: 3107, Len of Training loss: 18, Average loss: 1.0445058047771454\n",
      "Len of Validation loss: 54, Average loss: 3.4108150557235435\n",
      "Epoch: 3108, Len of Training loss: 18, Average loss: 1.1913992961247761\n",
      "Len of Validation loss: 54, Average loss: 3.542309366994434\n",
      "Epoch: 3109, Len of Training loss: 18, Average loss: 1.1558791200319927\n",
      "Len of Validation loss: 54, Average loss: 3.4496030785419323\n",
      "Epoch: 3110, Len of Training loss: 18, Average loss: 1.1134511828422546\n",
      "Len of Validation loss: 54, Average loss: 3.460258694710555\n",
      "Epoch: 3111, Len of Training loss: 18, Average loss: 1.0814916160371568\n",
      "Len of Validation loss: 54, Average loss: 3.3405484325355954\n",
      "Epoch: 3112, Len of Training loss: 18, Average loss: 1.0598080489370558\n",
      "Len of Validation loss: 54, Average loss: 3.4646210449713246\n",
      "Epoch: 3113, Len of Training loss: 18, Average loss: 1.1175050404336717\n",
      "Len of Validation loss: 54, Average loss: 3.4144479168785944\n",
      "Epoch: 3114, Len of Training loss: 18, Average loss: 1.0906822747654386\n",
      "Len of Validation loss: 54, Average loss: 3.565950533858052\n",
      "Epoch: 3115, Len of Training loss: 18, Average loss: 1.4543331861495972\n",
      "Len of Validation loss: 54, Average loss: 3.9303326385992543\n",
      "Epoch: 3116, Len of Training loss: 18, Average loss: 1.3417487343152363\n",
      "Len of Validation loss: 54, Average loss: 3.598079757557975\n",
      "Epoch: 3117, Len of Training loss: 18, Average loss: 1.232931097348531\n",
      "Len of Validation loss: 54, Average loss: 3.4652186146488897\n",
      "Epoch: 3118, Len of Training loss: 18, Average loss: 1.1941205196910434\n",
      "Len of Validation loss: 54, Average loss: 3.710604833232032\n",
      "Epoch: 3119, Len of Training loss: 18, Average loss: 1.3295106689135234\n",
      "Len of Validation loss: 54, Average loss: 3.5484048812477678\n",
      "Epoch: 3120, Len of Training loss: 18, Average loss: 1.151877138349745\n",
      "Len of Validation loss: 54, Average loss: 3.563099494686833\n",
      "Epoch: 3121, Len of Training loss: 18, Average loss: 1.1785307659043207\n",
      "Len of Validation loss: 54, Average loss: 3.4503584735923343\n",
      "Epoch: 3122, Len of Training loss: 18, Average loss: 1.0830888383918338\n",
      "Len of Validation loss: 54, Average loss: 3.528920306099786\n",
      "Epoch: 3123, Len of Training loss: 18, Average loss: 1.0380800863107045\n",
      "Len of Validation loss: 54, Average loss: 3.36609109905031\n",
      "Epoch: 3124, Len of Training loss: 18, Average loss: 1.274481068054835\n",
      "Len of Validation loss: 54, Average loss: 3.607049449726387\n",
      "Epoch: 3125, Len of Training loss: 18, Average loss: 1.3742994533644781\n",
      "Len of Validation loss: 54, Average loss: 3.6112242219624697\n",
      "Epoch: 3126, Len of Training loss: 18, Average loss: 1.414018644226922\n",
      "Len of Validation loss: 54, Average loss: 3.8817665157494723\n",
      "Epoch: 3127, Len of Training loss: 18, Average loss: 1.387206455071767\n",
      "Len of Validation loss: 54, Average loss: 3.6066784306808755\n",
      "Epoch: 3128, Len of Training loss: 18, Average loss: 1.2203670607672796\n",
      "Len of Validation loss: 54, Average loss: 3.586783124340905\n",
      "Epoch: 3129, Len of Training loss: 18, Average loss: 1.2167739669481914\n",
      "Len of Validation loss: 54, Average loss: 3.3925758291173866\n",
      "Epoch: 3130, Len of Training loss: 18, Average loss: 1.1509000195397272\n",
      "Len of Validation loss: 54, Average loss: 3.63215422851068\n",
      "Epoch: 3131, Len of Training loss: 18, Average loss: 1.3493840959337022\n",
      "Len of Validation loss: 54, Average loss: 3.8144493290671595\n",
      "Epoch: 3132, Len of Training loss: 18, Average loss: 1.3605867756737604\n",
      "Len of Validation loss: 54, Average loss: 3.5008157006016485\n",
      "Epoch: 3133, Len of Training loss: 18, Average loss: 1.2007490992546082\n",
      "Len of Validation loss: 54, Average loss: 3.6314091274031886\n",
      "Epoch: 3134, Len of Training loss: 18, Average loss: 1.3544410467147827\n",
      "Len of Validation loss: 54, Average loss: 3.629378824322312\n",
      "Epoch: 3135, Len of Training loss: 18, Average loss: 1.327145708931817\n",
      "Len of Validation loss: 54, Average loss: 3.4643598397572837\n",
      "Epoch: 3136, Len of Training loss: 18, Average loss: 1.1877888441085815\n",
      "Len of Validation loss: 54, Average loss: 3.5404804205452955\n",
      "Epoch: 3137, Len of Training loss: 18, Average loss: 1.1321399344338312\n",
      "Len of Validation loss: 54, Average loss: 3.376744537441819\n",
      "Epoch: 3138, Len of Training loss: 18, Average loss: 1.1227928267584906\n",
      "Len of Validation loss: 54, Average loss: 3.5755605145736977\n",
      "Epoch: 3139, Len of Training loss: 18, Average loss: 1.1931367450290256\n",
      "Len of Validation loss: 54, Average loss: 3.413183711193226\n",
      "Epoch: 3140, Len of Training loss: 18, Average loss: 1.2182522349887424\n",
      "Len of Validation loss: 54, Average loss: 3.594936599334081\n",
      "Epoch: 3141, Len of Training loss: 18, Average loss: 1.2184551821814642\n",
      "Len of Validation loss: 54, Average loss: 3.342509436386603\n",
      "Epoch: 3142, Len of Training loss: 18, Average loss: 1.169558174080319\n",
      "Len of Validation loss: 54, Average loss: 3.527208028016267\n",
      "Epoch: 3143, Len of Training loss: 18, Average loss: 1.2070140507486131\n",
      "Len of Validation loss: 54, Average loss: 3.5051017580208956\n",
      "Epoch: 3144, Len of Training loss: 18, Average loss: 1.145857122209337\n",
      "Len of Validation loss: 54, Average loss: 3.432841648658117\n",
      "Epoch: 3145, Len of Training loss: 18, Average loss: 1.1291664606995053\n",
      "Len of Validation loss: 54, Average loss: 3.4539146224657693\n",
      "Epoch: 3146, Len of Training loss: 18, Average loss: 1.1424807243876987\n",
      "Len of Validation loss: 54, Average loss: 3.7476581235726676\n",
      "Epoch: 3147, Len of Training loss: 18, Average loss: 1.123319834470749\n",
      "Len of Validation loss: 54, Average loss: 3.5034337639808655\n",
      "Epoch: 3148, Len of Training loss: 18, Average loss: 1.078163892030716\n",
      "Len of Validation loss: 54, Average loss: 3.4065957301192813\n",
      "Epoch: 3149, Len of Training loss: 18, Average loss: 1.1181179285049438\n",
      "Len of Validation loss: 54, Average loss: 3.509894730868163\n",
      "Epoch: 3150, Len of Training loss: 18, Average loss: 1.073305047220654\n",
      "Len of Validation loss: 54, Average loss: 3.5041420459747314\n",
      "Epoch: 3151, Len of Training loss: 18, Average loss: 1.2822269664870367\n",
      "Len of Validation loss: 54, Average loss: 4.07401383806158\n",
      "Epoch: 3152, Len of Training loss: 18, Average loss: 1.4261066516240437\n",
      "Len of Validation loss: 54, Average loss: 3.564742158960413\n",
      "Epoch: 3153, Len of Training loss: 18, Average loss: 1.2505611115031772\n",
      "Len of Validation loss: 54, Average loss: 3.639130660781154\n",
      "Epoch: 3154, Len of Training loss: 18, Average loss: 1.2901904582977295\n",
      "Len of Validation loss: 54, Average loss: 3.5186003744602203\n",
      "Epoch: 3155, Len of Training loss: 18, Average loss: 1.1632237666183047\n",
      "Len of Validation loss: 54, Average loss: 3.680556618505054\n",
      "Epoch: 3156, Len of Training loss: 18, Average loss: 1.072594208849801\n",
      "Len of Validation loss: 54, Average loss: 3.5825623980274908\n",
      "Epoch: 3157, Len of Training loss: 18, Average loss: 1.0406056377622817\n",
      "Len of Validation loss: 54, Average loss: 3.446223082365813\n",
      "Epoch: 3158, Len of Training loss: 18, Average loss: 1.0764812959565058\n",
      "Len of Validation loss: 54, Average loss: 3.4489296045568256\n",
      "Epoch: 3159, Len of Training loss: 18, Average loss: 1.10569167137146\n",
      "Len of Validation loss: 54, Average loss: 3.717718812050643\n",
      "Epoch: 3160, Len of Training loss: 18, Average loss: 1.2907796170976427\n",
      "Len of Validation loss: 54, Average loss: 3.5144573284520044\n",
      "Epoch: 3161, Len of Training loss: 18, Average loss: 1.361704455481635\n",
      "Len of Validation loss: 54, Average loss: 3.397059288289812\n",
      "Epoch: 3162, Len of Training loss: 18, Average loss: 1.1609876884354486\n",
      "Len of Validation loss: 54, Average loss: 3.6042660629307783\n",
      "Epoch: 3163, Len of Training loss: 18, Average loss: 1.079548590713077\n",
      "Len of Validation loss: 54, Average loss: 3.486169182591968\n",
      "Epoch: 3164, Len of Training loss: 18, Average loss: 1.0186060667037964\n",
      "Len of Validation loss: 54, Average loss: 3.488516753470456\n",
      "Epoch: 3165, Len of Training loss: 18, Average loss: 1.0473755531840854\n",
      "Len of Validation loss: 54, Average loss: 3.429682555022063\n",
      "Epoch: 3166, Len of Training loss: 18, Average loss: 1.0789390835497115\n",
      "Len of Validation loss: 54, Average loss: 3.4863771718961223\n",
      "Epoch: 3167, Len of Training loss: 18, Average loss: 1.0429256624645658\n",
      "Len of Validation loss: 54, Average loss: 3.601159272370515\n",
      "Epoch: 3168, Len of Training loss: 18, Average loss: 1.0074205597241719\n",
      "Len of Validation loss: 54, Average loss: 3.44164060332157\n",
      "Epoch: 3169, Len of Training loss: 18, Average loss: 0.9977023171053993\n",
      "Len of Validation loss: 54, Average loss: 3.52245424191157\n",
      "Epoch: 3170, Len of Training loss: 18, Average loss: 1.0107694533136156\n",
      "Len of Validation loss: 54, Average loss: 3.457453354641243\n",
      "Epoch: 3171, Len of Training loss: 18, Average loss: 1.210135132074356\n",
      "Len of Validation loss: 54, Average loss: 3.658834425387559\n",
      "Epoch: 3172, Len of Training loss: 18, Average loss: 1.3476613362630208\n",
      "Len of Validation loss: 54, Average loss: 3.680864292162436\n",
      "Epoch: 3173, Len of Training loss: 18, Average loss: 1.270054002602895\n",
      "Len of Validation loss: 54, Average loss: 3.7473999758561454\n",
      "Epoch: 3174, Len of Training loss: 18, Average loss: 1.1942253079679277\n",
      "Len of Validation loss: 54, Average loss: 3.4960435540587813\n",
      "Epoch: 3175, Len of Training loss: 18, Average loss: 1.2315647270944383\n",
      "Len of Validation loss: 54, Average loss: 3.5230364004770913\n",
      "Epoch: 3176, Len of Training loss: 18, Average loss: 1.28724119398329\n",
      "Len of Validation loss: 54, Average loss: 3.7936349312464395\n",
      "Epoch: 3177, Len of Training loss: 18, Average loss: 1.3656384150187175\n",
      "Len of Validation loss: 54, Average loss: 3.548842888187479\n",
      "Epoch: 3178, Len of Training loss: 18, Average loss: 1.2298296689987183\n",
      "Len of Validation loss: 54, Average loss: 3.3919116611833924\n",
      "Epoch: 3179, Len of Training loss: 18, Average loss: 1.1323414676719241\n",
      "Len of Validation loss: 54, Average loss: 3.4325409675085985\n",
      "Epoch: 3180, Len of Training loss: 18, Average loss: 1.093086239364412\n",
      "Len of Validation loss: 54, Average loss: 3.5307453870773315\n",
      "Epoch: 3181, Len of Training loss: 18, Average loss: 1.1751629445287917\n",
      "Len of Validation loss: 54, Average loss: 3.57575249892694\n",
      "Epoch: 3182, Len of Training loss: 18, Average loss: 1.1008224222395155\n",
      "Len of Validation loss: 54, Average loss: 3.3145911682535103\n",
      "Epoch: 3183, Len of Training loss: 18, Average loss: 1.119889262649748\n",
      "Len of Validation loss: 54, Average loss: 3.421870494330371\n",
      "Epoch: 3184, Len of Training loss: 18, Average loss: 1.1811856693691678\n",
      "Len of Validation loss: 54, Average loss: 3.4537088904115887\n",
      "Epoch: 3185, Len of Training loss: 18, Average loss: 1.050029844045639\n",
      "Len of Validation loss: 54, Average loss: 3.2223377492692737\n",
      "Epoch: 3186, Len of Training loss: 18, Average loss: 1.1003776027096643\n",
      "Len of Validation loss: 54, Average loss: 3.561349481344223\n",
      "Epoch: 3187, Len of Training loss: 18, Average loss: 1.0967041154702504\n",
      "Len of Validation loss: 54, Average loss: 3.3525369432237415\n",
      "Epoch: 3188, Len of Training loss: 18, Average loss: 1.0881384015083313\n",
      "Len of Validation loss: 54, Average loss: 3.347042328781552\n",
      "Epoch: 3189, Len of Training loss: 18, Average loss: 1.1722872853279114\n",
      "Len of Validation loss: 54, Average loss: 3.4971005022525787\n",
      "Epoch: 3190, Len of Training loss: 18, Average loss: 1.1423185070355732\n",
      "Len of Validation loss: 54, Average loss: 3.591071359537266\n",
      "Epoch: 3191, Len of Training loss: 18, Average loss: 1.1558990677197774\n",
      "Len of Validation loss: 54, Average loss: 3.5418719108457917\n",
      "Epoch: 3192, Len of Training loss: 18, Average loss: 1.1551000542110867\n",
      "Len of Validation loss: 54, Average loss: 3.6257760800697185\n",
      "Epoch: 3193, Len of Training loss: 18, Average loss: 1.1773253679275513\n",
      "Len of Validation loss: 54, Average loss: 3.4634558437047183\n",
      "Epoch: 3194, Len of Training loss: 18, Average loss: 1.2032703161239624\n",
      "Len of Validation loss: 54, Average loss: 3.507579972346624\n",
      "Epoch: 3195, Len of Training loss: 18, Average loss: 1.1163129806518555\n",
      "Len of Validation loss: 54, Average loss: 3.588530577995159\n",
      "Epoch: 3196, Len of Training loss: 18, Average loss: 1.127130130926768\n",
      "Len of Validation loss: 54, Average loss: 3.6231967784740307\n",
      "Epoch: 3197, Len of Training loss: 18, Average loss: 1.101550151904424\n",
      "Len of Validation loss: 54, Average loss: 3.470476899985914\n",
      "Epoch: 3198, Len of Training loss: 18, Average loss: 1.1165480183230505\n",
      "Len of Validation loss: 54, Average loss: 3.509360642344863\n",
      "Epoch: 3199, Len of Training loss: 18, Average loss: 1.2550562289026048\n",
      "Len of Validation loss: 54, Average loss: 3.511377956028338\n",
      "Epoch: 3200, Len of Training loss: 18, Average loss: 1.2074467870924208\n",
      "Len of Validation loss: 54, Average loss: 3.335544537614893\n",
      "Epoch: 3201, Len of Training loss: 18, Average loss: 1.0816576182842255\n",
      "Len of Validation loss: 54, Average loss: 3.4388105151829897\n",
      "Epoch: 3202, Len of Training loss: 18, Average loss: 0.9968363642692566\n",
      "Len of Validation loss: 54, Average loss: 3.2573733186280287\n",
      "Epoch: 3203, Len of Training loss: 18, Average loss: 1.079785231086943\n",
      "Len of Validation loss: 54, Average loss: 3.430446556320897\n",
      "Epoch: 3204, Len of Training loss: 18, Average loss: 1.1336397793557909\n",
      "Len of Validation loss: 54, Average loss: 3.569993492629793\n",
      "Epoch: 3205, Len of Training loss: 18, Average loss: 1.1713352534506056\n",
      "Len of Validation loss: 54, Average loss: 3.7994954188664756\n",
      "Epoch: 3206, Len of Training loss: 18, Average loss: 1.153205566936069\n",
      "Len of Validation loss: 54, Average loss: 3.3974710923654063\n",
      "Epoch: 3207, Len of Training loss: 18, Average loss: 1.1206510696146224\n",
      "Len of Validation loss: 54, Average loss: 3.4139180779457092\n",
      "Epoch: 3208, Len of Training loss: 18, Average loss: 1.1179810298813715\n",
      "Len of Validation loss: 54, Average loss: 3.4097324742211237\n",
      "Epoch: 3209, Len of Training loss: 18, Average loss: 1.138827959696452\n",
      "Len of Validation loss: 54, Average loss: 3.5532157564604723\n",
      "Epoch: 3210, Len of Training loss: 18, Average loss: 1.1550058159563277\n",
      "Len of Validation loss: 54, Average loss: 3.4263083084865853\n",
      "Epoch: 3211, Len of Training loss: 18, Average loss: 1.1649520132276747\n",
      "Len of Validation loss: 54, Average loss: 3.540893914522948\n",
      "Epoch: 3212, Len of Training loss: 18, Average loss: 1.1362333363956876\n",
      "Len of Validation loss: 54, Average loss: 3.7214721386079437\n",
      "Epoch: 3213, Len of Training loss: 18, Average loss: 1.104514976342519\n",
      "Len of Validation loss: 54, Average loss: 3.2702207234170704\n",
      "Epoch: 3214, Len of Training loss: 18, Average loss: 1.1098639931943681\n",
      "Len of Validation loss: 54, Average loss: 3.484312378697925\n",
      "Epoch: 3215, Len of Training loss: 18, Average loss: 1.1338076922628615\n",
      "Len of Validation loss: 54, Average loss: 3.497459316695178\n",
      "Epoch: 3216, Len of Training loss: 18, Average loss: 1.1541790498627558\n",
      "Len of Validation loss: 54, Average loss: 3.5357584169617406\n",
      "Epoch: 3217, Len of Training loss: 18, Average loss: 1.1191959381103516\n",
      "Len of Validation loss: 54, Average loss: 3.676011734538608\n",
      "Epoch: 3218, Len of Training loss: 18, Average loss: 1.0984720057911344\n",
      "Len of Validation loss: 54, Average loss: 3.584539869317302\n",
      "Epoch: 3219, Len of Training loss: 18, Average loss: 1.0788199967808194\n",
      "Len of Validation loss: 54, Average loss: 3.381996998080501\n",
      "Epoch: 3220, Len of Training loss: 18, Average loss: 1.1054994894398584\n",
      "Len of Validation loss: 54, Average loss: 3.5319626640390465\n",
      "Epoch: 3221, Len of Training loss: 18, Average loss: 1.1754253970252142\n",
      "Len of Validation loss: 54, Average loss: 3.520855137595424\n",
      "Epoch: 3222, Len of Training loss: 18, Average loss: 1.2186106443405151\n",
      "Len of Validation loss: 54, Average loss: 3.8247036282662994\n",
      "Epoch: 3223, Len of Training loss: 18, Average loss: 1.2945017549726698\n",
      "Len of Validation loss: 54, Average loss: 3.7672928980103246\n",
      "Epoch: 3224, Len of Training loss: 18, Average loss: 1.234607504473792\n",
      "Len of Validation loss: 54, Average loss: 3.532436379679927\n",
      "Epoch: 3225, Len of Training loss: 18, Average loss: 1.1316410534911685\n",
      "Len of Validation loss: 54, Average loss: 3.4779428861759327\n",
      "Epoch: 3226, Len of Training loss: 18, Average loss: 1.1056346827083163\n",
      "Len of Validation loss: 54, Average loss: 3.621362963208446\n",
      "Epoch: 3227, Len of Training loss: 18, Average loss: 1.1118399037255182\n",
      "Len of Validation loss: 54, Average loss: 3.7034715756222054\n",
      "Epoch: 3228, Len of Training loss: 18, Average loss: 1.1547784838411543\n",
      "Len of Validation loss: 54, Average loss: 3.3659171340642153\n",
      "Epoch: 3229, Len of Training loss: 18, Average loss: 1.1646746065881517\n",
      "Len of Validation loss: 54, Average loss: 3.5558707460209176\n",
      "Epoch: 3230, Len of Training loss: 18, Average loss: 1.3556135892868042\n",
      "Len of Validation loss: 54, Average loss: 3.8313136674739696\n",
      "Epoch: 3231, Len of Training loss: 18, Average loss: 1.2704010009765625\n",
      "Len of Validation loss: 54, Average loss: 3.4944410390324063\n",
      "Epoch: 3232, Len of Training loss: 18, Average loss: 1.295680132177141\n",
      "Len of Validation loss: 54, Average loss: 3.578025412780267\n",
      "Epoch: 3233, Len of Training loss: 18, Average loss: 1.3203233149316576\n",
      "Len of Validation loss: 54, Average loss: 3.6693330429218434\n",
      "Epoch: 3234, Len of Training loss: 18, Average loss: 1.2592929734124079\n",
      "Len of Validation loss: 54, Average loss: 3.7788220942020416\n",
      "Epoch: 3235, Len of Training loss: 18, Average loss: 1.1348547736803691\n",
      "Len of Validation loss: 54, Average loss: 3.5201534452261747\n",
      "Epoch: 3236, Len of Training loss: 18, Average loss: 1.1170223587089114\n",
      "Len of Validation loss: 54, Average loss: 3.31332145024229\n",
      "Epoch: 3237, Len of Training loss: 18, Average loss: 1.1929756138059828\n",
      "Len of Validation loss: 54, Average loss: 3.511165863937802\n",
      "Epoch: 3238, Len of Training loss: 18, Average loss: 1.1159448855453067\n",
      "Len of Validation loss: 54, Average loss: 3.578913361937911\n",
      "Epoch: 3239, Len of Training loss: 18, Average loss: 1.1510514385170407\n",
      "Len of Validation loss: 54, Average loss: 3.395488687135555\n",
      "Epoch: 3240, Len of Training loss: 18, Average loss: 1.1439525220129225\n",
      "Len of Validation loss: 54, Average loss: 3.6859146058559418\n",
      "Epoch: 3241, Len of Training loss: 18, Average loss: 1.1056905719969008\n",
      "Len of Validation loss: 54, Average loss: 3.7272377720585577\n",
      "Epoch: 3242, Len of Training loss: 18, Average loss: 1.1730502049128215\n",
      "Len of Validation loss: 54, Average loss: 3.6235752745910927\n",
      "Epoch: 3243, Len of Training loss: 18, Average loss: 1.0957228110896216\n",
      "Len of Validation loss: 54, Average loss: 3.5174201484079712\n",
      "Epoch: 3244, Len of Training loss: 18, Average loss: 1.0507143702771928\n",
      "Len of Validation loss: 54, Average loss: 3.535605242958775\n",
      "Epoch: 3245, Len of Training loss: 18, Average loss: 1.0388085842132568\n",
      "Len of Validation loss: 54, Average loss: 3.339748274396967\n",
      "Epoch: 3246, Len of Training loss: 18, Average loss: 1.0424530373679266\n",
      "Len of Validation loss: 54, Average loss: 3.4800061550405292\n",
      "Epoch: 3247, Len of Training loss: 18, Average loss: 1.0376775761445363\n",
      "Len of Validation loss: 54, Average loss: 3.328634571146082\n",
      "Epoch: 3248, Len of Training loss: 18, Average loss: 1.0168849262926314\n",
      "Len of Validation loss: 54, Average loss: 3.5683705828807972\n",
      "Epoch: 3249, Len of Training loss: 18, Average loss: 1.0749915805127885\n",
      "Len of Validation loss: 54, Average loss: 3.639756034921717\n",
      "Epoch: 3250, Len of Training loss: 18, Average loss: 1.2524786061710782\n",
      "Len of Validation loss: 54, Average loss: 3.5472327205869885\n",
      "Epoch: 3251, Len of Training loss: 18, Average loss: 1.1138874822192721\n",
      "Len of Validation loss: 54, Average loss: 3.6690359512964883\n",
      "Epoch: 3252, Len of Training loss: 18, Average loss: 1.111739416917165\n",
      "Len of Validation loss: 54, Average loss: 3.6200342697125896\n",
      "Epoch: 3253, Len of Training loss: 18, Average loss: 1.101198633511861\n",
      "Len of Validation loss: 54, Average loss: 3.6012057032850056\n",
      "Epoch: 3254, Len of Training loss: 18, Average loss: 1.0498730738957722\n",
      "Len of Validation loss: 54, Average loss: 3.4751887630533287\n",
      "Epoch: 3255, Len of Training loss: 18, Average loss: 1.0608624286121793\n",
      "Len of Validation loss: 54, Average loss: 3.5917201152554266\n",
      "Epoch: 3256, Len of Training loss: 18, Average loss: 1.0545116894774966\n",
      "Len of Validation loss: 54, Average loss: 3.376262614020595\n",
      "Epoch: 3257, Len of Training loss: 18, Average loss: 1.015032132466634\n",
      "Len of Validation loss: 54, Average loss: 3.333745132993769\n",
      "Epoch: 3258, Len of Training loss: 18, Average loss: 1.0703209241231282\n",
      "Len of Validation loss: 54, Average loss: 3.536796030071047\n",
      "Epoch: 3259, Len of Training loss: 18, Average loss: 1.1059210234218173\n",
      "Len of Validation loss: 54, Average loss: 3.6139875010207847\n",
      "Epoch: 3260, Len of Training loss: 18, Average loss: 1.0726590421464708\n",
      "Len of Validation loss: 54, Average loss: 3.4300198974432767\n",
      "Epoch: 3261, Len of Training loss: 18, Average loss: 1.093561718861262\n",
      "Len of Validation loss: 54, Average loss: 3.4678900716481387\n",
      "Epoch: 3262, Len of Training loss: 18, Average loss: 1.0407644112904866\n",
      "Len of Validation loss: 54, Average loss: 3.423728515704473\n",
      "Epoch: 3263, Len of Training loss: 18, Average loss: 1.0914946231577132\n",
      "Len of Validation loss: 54, Average loss: 3.519818490302121\n",
      "Epoch: 3264, Len of Training loss: 18, Average loss: 1.0662448902924855\n",
      "Len of Validation loss: 54, Average loss: 3.4368432649859675\n",
      "Epoch: 3265, Len of Training loss: 18, Average loss: 1.0923850569460127\n",
      "Len of Validation loss: 54, Average loss: 3.610397150119146\n",
      "Epoch: 3266, Len of Training loss: 18, Average loss: 1.084147595696979\n",
      "Len of Validation loss: 54, Average loss: 3.531422703354447\n",
      "Epoch: 3267, Len of Training loss: 18, Average loss: 1.122137149175008\n",
      "Len of Validation loss: 54, Average loss: 3.5689159786259688\n",
      "Epoch: 3268, Len of Training loss: 18, Average loss: 1.1371232006284926\n",
      "Len of Validation loss: 54, Average loss: 3.471435339362533\n",
      "Epoch: 3269, Len of Training loss: 18, Average loss: 1.1323624054590862\n",
      "Len of Validation loss: 54, Average loss: 3.2671360230004347\n",
      "Epoch: 3270, Len of Training loss: 18, Average loss: 1.1660534971290164\n",
      "Len of Validation loss: 54, Average loss: 3.5506676269902124\n",
      "Epoch: 3271, Len of Training loss: 18, Average loss: 1.2112388809521992\n",
      "Len of Validation loss: 54, Average loss: 3.6121756401326923\n",
      "Epoch: 3272, Len of Training loss: 18, Average loss: 1.1504209571414523\n",
      "Len of Validation loss: 54, Average loss: 3.6703837547037335\n",
      "Epoch: 3273, Len of Training loss: 18, Average loss: 1.1313329703278012\n",
      "Len of Validation loss: 54, Average loss: 3.354534075216011\n",
      "Epoch: 3274, Len of Training loss: 18, Average loss: 1.1450230446126726\n",
      "Len of Validation loss: 54, Average loss: 3.548422517599883\n",
      "Epoch: 3275, Len of Training loss: 18, Average loss: 1.1285973058806524\n",
      "Len of Validation loss: 54, Average loss: 3.5136739567474082\n",
      "Epoch: 3276, Len of Training loss: 18, Average loss: 1.2959454655647278\n",
      "Len of Validation loss: 54, Average loss: 3.6584506785428084\n",
      "Epoch: 3277, Len of Training loss: 18, Average loss: 1.3204233513938055\n",
      "Len of Validation loss: 54, Average loss: 3.4483422769440546\n",
      "Epoch: 3278, Len of Training loss: 18, Average loss: 1.3414135641521878\n",
      "Len of Validation loss: 54, Average loss: 3.739065842496024\n",
      "Epoch: 3279, Len of Training loss: 18, Average loss: 1.251419034269121\n",
      "Len of Validation loss: 54, Average loss: 3.4100384767408722\n",
      "Epoch: 3280, Len of Training loss: 18, Average loss: 1.2512787580490112\n",
      "Len of Validation loss: 54, Average loss: 3.8599931045814797\n",
      "Epoch: 3281, Len of Training loss: 18, Average loss: 1.3232959045304193\n",
      "Len of Validation loss: 54, Average loss: 3.431293379377436\n",
      "Epoch: 3282, Len of Training loss: 18, Average loss: 1.3802747461530898\n",
      "Len of Validation loss: 54, Average loss: 3.626311495348259\n",
      "Epoch: 3283, Len of Training loss: 18, Average loss: 1.2095519966549344\n",
      "Len of Validation loss: 54, Average loss: 3.401536859847881\n",
      "Epoch: 3284, Len of Training loss: 18, Average loss: 1.0830361445744832\n",
      "Len of Validation loss: 54, Average loss: 3.65260425541136\n",
      "Epoch: 3285, Len of Training loss: 18, Average loss: 1.1186728212568495\n",
      "Len of Validation loss: 54, Average loss: 3.4325805968708463\n",
      "Epoch: 3286, Len of Training loss: 18, Average loss: 1.0956845879554749\n",
      "Len of Validation loss: 54, Average loss: 3.432933956384659\n",
      "Epoch: 3287, Len of Training loss: 18, Average loss: 1.1267772548728519\n",
      "Len of Validation loss: 54, Average loss: 3.407413747575548\n",
      "Epoch: 3288, Len of Training loss: 18, Average loss: 1.02752681573232\n",
      "Len of Validation loss: 54, Average loss: 3.496156930923462\n",
      "Epoch: 3289, Len of Training loss: 18, Average loss: 1.0555908580621083\n",
      "Len of Validation loss: 54, Average loss: 3.4455510245429144\n",
      "Epoch: 3290, Len of Training loss: 18, Average loss: 1.2275475594732497\n",
      "Len of Validation loss: 54, Average loss: 3.542692990214736\n",
      "Epoch: 3291, Len of Training loss: 18, Average loss: 1.2179254227214389\n",
      "Len of Validation loss: 54, Average loss: 3.3144882266168243\n",
      "Epoch: 3292, Len of Training loss: 18, Average loss: 1.1836742626296148\n",
      "Len of Validation loss: 54, Average loss: 3.4371512752992137\n",
      "Epoch: 3293, Len of Training loss: 18, Average loss: 1.2915354304843478\n",
      "Len of Validation loss: 54, Average loss: 3.5809980542571456\n",
      "Epoch: 3294, Len of Training loss: 18, Average loss: 1.4899893403053284\n",
      "Len of Validation loss: 54, Average loss: 3.754157103874065\n",
      "Epoch: 3295, Len of Training loss: 18, Average loss: 1.4267542362213135\n",
      "Len of Validation loss: 54, Average loss: 3.6311288475990295\n",
      "Epoch: 3296, Len of Training loss: 18, Average loss: 1.214469141430325\n",
      "Len of Validation loss: 54, Average loss: 3.5267931951416864\n",
      "Epoch: 3297, Len of Training loss: 18, Average loss: 1.1276318662696414\n",
      "Len of Validation loss: 54, Average loss: 3.5797519827330553\n",
      "Epoch: 3298, Len of Training loss: 18, Average loss: 1.0992529193560283\n",
      "Len of Validation loss: 54, Average loss: 3.4857608874638877\n",
      "Epoch: 3299, Len of Training loss: 18, Average loss: 1.2252374622556899\n",
      "Len of Validation loss: 54, Average loss: 3.5374984674983554\n",
      "Epoch: 3300, Len of Training loss: 18, Average loss: 1.1104984283447266\n",
      "Len of Validation loss: 54, Average loss: 3.4575739160731986\n",
      "Epoch: 3301, Len of Training loss: 18, Average loss: 1.1329579916265275\n",
      "Len of Validation loss: 54, Average loss: 3.4830960211930453\n",
      "Epoch: 3302, Len of Training loss: 18, Average loss: 1.4745766090022192\n",
      "Len of Validation loss: 54, Average loss: 3.6758759529502303\n",
      "Epoch: 3303, Len of Training loss: 18, Average loss: 1.4398642910851374\n",
      "Len of Validation loss: 54, Average loss: 3.6263736663041293\n",
      "Epoch: 3304, Len of Training loss: 18, Average loss: 1.2015002369880676\n",
      "Len of Validation loss: 54, Average loss: 3.472701832100197\n",
      "Epoch: 3305, Len of Training loss: 18, Average loss: 1.0466617445151012\n",
      "Len of Validation loss: 54, Average loss: 3.525248220673314\n",
      "Epoch: 3306, Len of Training loss: 18, Average loss: 1.1147269242339664\n",
      "Len of Validation loss: 54, Average loss: 3.411401331424713\n",
      "Epoch: 3307, Len of Training loss: 18, Average loss: 1.0514459212621052\n",
      "Len of Validation loss: 54, Average loss: 3.3590278095669217\n",
      "Epoch: 3308, Len of Training loss: 18, Average loss: 1.045240021414227\n",
      "Len of Validation loss: 54, Average loss: 3.4790541971171343\n",
      "Epoch: 3309, Len of Training loss: 18, Average loss: 1.153758962949117\n",
      "Len of Validation loss: 54, Average loss: 3.6604840645083674\n",
      "Epoch: 3310, Len of Training loss: 18, Average loss: 1.3330722053845723\n",
      "Len of Validation loss: 54, Average loss: 3.7359723272147\n",
      "Epoch: 3311, Len of Training loss: 18, Average loss: 1.3686970472335815\n",
      "Len of Validation loss: 54, Average loss: 3.4929490232909166\n",
      "Epoch: 3312, Len of Training loss: 18, Average loss: 1.215014785528183\n",
      "Len of Validation loss: 54, Average loss: 3.5356843372186026\n",
      "Epoch: 3313, Len of Training loss: 18, Average loss: 1.190782265530692\n",
      "Len of Validation loss: 54, Average loss: 3.4710302827534854\n",
      "Epoch: 3314, Len of Training loss: 18, Average loss: 1.1211681365966797\n",
      "Len of Validation loss: 54, Average loss: 3.3571352075647423\n",
      "Epoch: 3315, Len of Training loss: 18, Average loss: 1.140163176589542\n",
      "Len of Validation loss: 54, Average loss: 3.452197018596861\n",
      "Epoch: 3316, Len of Training loss: 18, Average loss: 1.044997622569402\n",
      "Len of Validation loss: 54, Average loss: 3.53487941733113\n",
      "Epoch: 3317, Len of Training loss: 18, Average loss: 1.0131676230165694\n",
      "Len of Validation loss: 54, Average loss: 3.491361720694436\n",
      "Epoch: 3318, Len of Training loss: 18, Average loss: 1.0005099342928991\n",
      "Len of Validation loss: 54, Average loss: 3.395117508040534\n",
      "Epoch: 3319, Len of Training loss: 18, Average loss: 0.9973294138908386\n",
      "Len of Validation loss: 54, Average loss: 3.6893405417601266\n",
      "Epoch: 3320, Len of Training loss: 18, Average loss: 1.0946045087443457\n",
      "Len of Validation loss: 54, Average loss: 3.514748682578405\n",
      "Epoch: 3321, Len of Training loss: 18, Average loss: 1.1241151657369401\n",
      "Len of Validation loss: 54, Average loss: 3.5302102720295943\n",
      "Epoch: 3322, Len of Training loss: 18, Average loss: 1.0312803188959758\n",
      "Len of Validation loss: 54, Average loss: 3.4495244511851557\n",
      "Epoch: 3323, Len of Training loss: 18, Average loss: 1.04344661699401\n",
      "Len of Validation loss: 54, Average loss: 3.473930545427181\n",
      "Epoch: 3324, Len of Training loss: 18, Average loss: 1.0446562932597265\n",
      "Len of Validation loss: 54, Average loss: 3.3262833467236272\n",
      "Epoch: 3325, Len of Training loss: 18, Average loss: 1.046112921502855\n",
      "Len of Validation loss: 54, Average loss: 3.175270724075812\n",
      "Epoch: 3326, Len of Training loss: 18, Average loss: 1.0192335579130385\n",
      "Len of Validation loss: 54, Average loss: 3.6508360394725092\n",
      "Epoch: 3327, Len of Training loss: 18, Average loss: 1.0421902073754206\n",
      "Len of Validation loss: 54, Average loss: 3.3084051896024635\n",
      "Epoch: 3328, Len of Training loss: 18, Average loss: 0.9658974077966478\n",
      "Len of Validation loss: 54, Average loss: 3.3284406584722026\n",
      "Epoch: 3329, Len of Training loss: 18, Average loss: 1.008274604876836\n",
      "Len of Validation loss: 54, Average loss: 3.439433098943145\n",
      "Epoch: 3330, Len of Training loss: 18, Average loss: 1.0219226976235707\n",
      "Len of Validation loss: 54, Average loss: 3.5047784955413253\n",
      "Epoch: 3331, Len of Training loss: 18, Average loss: 1.0241812003983393\n",
      "Len of Validation loss: 54, Average loss: 3.3996198961028345\n",
      "Epoch: 3332, Len of Training loss: 18, Average loss: 1.158529023329417\n",
      "Len of Validation loss: 54, Average loss: 3.517512363416177\n",
      "Epoch: 3333, Len of Training loss: 18, Average loss: 1.3145777318212721\n",
      "Len of Validation loss: 54, Average loss: 3.7917682109055697\n",
      "Epoch: 3334, Len of Training loss: 18, Average loss: 1.5250319772296481\n",
      "Len of Validation loss: 54, Average loss: 3.5963609682189093\n",
      "Epoch: 3335, Len of Training loss: 18, Average loss: 1.4360266791449652\n",
      "Len of Validation loss: 54, Average loss: 3.7097122448462025\n",
      "Epoch: 3336, Len of Training loss: 18, Average loss: 1.2353219985961914\n",
      "Len of Validation loss: 54, Average loss: 3.6504992467385753\n",
      "Epoch: 3337, Len of Training loss: 18, Average loss: 1.1861071851518419\n",
      "Len of Validation loss: 54, Average loss: 3.481689347161187\n",
      "Epoch: 3338, Len of Training loss: 18, Average loss: 1.0964554515149858\n",
      "Len of Validation loss: 54, Average loss: 3.509984996583727\n",
      "Epoch: 3339, Len of Training loss: 18, Average loss: 1.1446787317593892\n",
      "Len of Validation loss: 54, Average loss: 3.3782282482694694\n",
      "Epoch: 3340, Len of Training loss: 18, Average loss: 1.128678537077374\n",
      "Len of Validation loss: 54, Average loss: 3.711647676097022\n",
      "Epoch: 3341, Len of Training loss: 18, Average loss: 1.2118025289641485\n",
      "Len of Validation loss: 54, Average loss: 3.6434614139574544\n",
      "Epoch: 3342, Len of Training loss: 18, Average loss: 1.320619973871443\n",
      "Len of Validation loss: 54, Average loss: 3.5234904653496213\n",
      "Epoch: 3343, Len of Training loss: 18, Average loss: 1.255213244093789\n",
      "Len of Validation loss: 54, Average loss: 3.447249617841509\n",
      "Epoch: 3344, Len of Training loss: 18, Average loss: 1.1677915222114987\n",
      "Len of Validation loss: 54, Average loss: 3.282331105735567\n",
      "Epoch: 3345, Len of Training loss: 18, Average loss: 1.1784298287497625\n",
      "Len of Validation loss: 54, Average loss: 3.5215742146527327\n",
      "Epoch: 3346, Len of Training loss: 18, Average loss: 1.1330469051996868\n",
      "Len of Validation loss: 54, Average loss: 3.3712298152623354\n",
      "Epoch: 3347, Len of Training loss: 18, Average loss: 1.0823326971795824\n",
      "Len of Validation loss: 54, Average loss: 3.474955785053748\n",
      "Epoch: 3348, Len of Training loss: 18, Average loss: 1.1331020361847348\n",
      "Len of Validation loss: 54, Average loss: 3.604249718012633\n",
      "Epoch: 3349, Len of Training loss: 18, Average loss: 1.1060524649090238\n",
      "Len of Validation loss: 54, Average loss: 3.5438797584286443\n",
      "Epoch: 3350, Len of Training loss: 18, Average loss: 1.0789999432033963\n",
      "Len of Validation loss: 54, Average loss: 3.36363743963065\n",
      "Epoch: 3351, Len of Training loss: 18, Average loss: 1.0657224357128143\n",
      "Len of Validation loss: 54, Average loss: 3.464291484267623\n",
      "Epoch: 3352, Len of Training loss: 18, Average loss: 1.060787793662813\n",
      "Len of Validation loss: 54, Average loss: 3.4590884524363057\n",
      "Epoch: 3353, Len of Training loss: 18, Average loss: 1.0295364459355671\n",
      "Len of Validation loss: 54, Average loss: 3.479391098022461\n",
      "Epoch: 3354, Len of Training loss: 18, Average loss: 1.001657356818517\n",
      "Len of Validation loss: 54, Average loss: 3.5340758562088013\n",
      "Epoch: 3355, Len of Training loss: 18, Average loss: 1.155061940352122\n",
      "Len of Validation loss: 54, Average loss: 3.480495336982939\n",
      "Epoch: 3356, Len of Training loss: 18, Average loss: 1.0465005801783667\n",
      "Len of Validation loss: 54, Average loss: 3.3804718454678855\n",
      "Epoch: 3357, Len of Training loss: 18, Average loss: 1.0866851773526933\n",
      "Len of Validation loss: 54, Average loss: 3.2902839205883168\n",
      "Epoch: 3358, Len of Training loss: 18, Average loss: 1.2502839763959248\n",
      "Len of Validation loss: 54, Average loss: 3.4442826995143183\n",
      "Epoch: 3359, Len of Training loss: 18, Average loss: 1.2792504562271967\n",
      "Len of Validation loss: 54, Average loss: 3.597592916753557\n",
      "Epoch: 3360, Len of Training loss: 18, Average loss: 1.2101971705754597\n",
      "Len of Validation loss: 54, Average loss: 3.4391372943366014\n",
      "Epoch: 3361, Len of Training loss: 18, Average loss: 1.2331699199146695\n",
      "Len of Validation loss: 54, Average loss: 3.7029133560480894\n",
      "Epoch: 3362, Len of Training loss: 18, Average loss: 1.236197120613522\n",
      "Len of Validation loss: 54, Average loss: 3.624196703787203\n",
      "Epoch: 3363, Len of Training loss: 18, Average loss: 1.5559216803974576\n",
      "Len of Validation loss: 54, Average loss: 3.786901491659659\n",
      "Epoch: 3364, Len of Training loss: 18, Average loss: 1.5478434496455722\n",
      "Len of Validation loss: 54, Average loss: 3.6598437362247043\n",
      "Epoch: 3365, Len of Training loss: 18, Average loss: 1.4394386543167963\n",
      "Len of Validation loss: 54, Average loss: 3.6942431374832436\n",
      "Epoch: 3366, Len of Training loss: 18, Average loss: 1.402479390303294\n",
      "Len of Validation loss: 54, Average loss: 3.520998888545566\n",
      "Epoch: 3367, Len of Training loss: 18, Average loss: 1.22564031680425\n",
      "Len of Validation loss: 54, Average loss: 3.6245497277489416\n",
      "Epoch: 3368, Len of Training loss: 18, Average loss: 1.1364974909358554\n",
      "Len of Validation loss: 54, Average loss: 3.56537832374926\n",
      "Epoch: 3369, Len of Training loss: 18, Average loss: 1.141709910498725\n",
      "Len of Validation loss: 54, Average loss: 3.694445069189425\n",
      "Epoch: 3370, Len of Training loss: 18, Average loss: 1.2003469963868458\n",
      "Len of Validation loss: 54, Average loss: 3.4674499366018505\n",
      "Epoch: 3371, Len of Training loss: 18, Average loss: 1.1631774306297302\n",
      "Len of Validation loss: 54, Average loss: 3.5824473721009715\n",
      "Epoch: 3372, Len of Training loss: 18, Average loss: 1.3867425322532654\n",
      "Len of Validation loss: 54, Average loss: 3.480282229405862\n",
      "Epoch: 3373, Len of Training loss: 18, Average loss: 1.1626505487494998\n",
      "Len of Validation loss: 54, Average loss: 3.479758871926202\n",
      "Epoch: 3374, Len of Training loss: 18, Average loss: 1.0721266402138605\n",
      "Len of Validation loss: 54, Average loss: 3.847203113414623\n",
      "Epoch: 3375, Len of Training loss: 18, Average loss: 1.1200548609097798\n",
      "Len of Validation loss: 54, Average loss: 3.5176996533517486\n",
      "Epoch: 3376, Len of Training loss: 18, Average loss: 1.231803274816937\n",
      "Len of Validation loss: 54, Average loss: 3.6772321926222906\n",
      "Epoch: 3377, Len of Training loss: 18, Average loss: 1.7823221418592665\n",
      "Len of Validation loss: 54, Average loss: 4.540306905905406\n",
      "Epoch: 3378, Len of Training loss: 18, Average loss: 3.4335073232650757\n",
      "Len of Validation loss: 54, Average loss: 4.43546868915911\n",
      "Epoch: 3379, Len of Training loss: 18, Average loss: 2.7672806448406644\n",
      "Len of Validation loss: 54, Average loss: 4.647393069885395\n",
      "Epoch: 3380, Len of Training loss: 18, Average loss: 2.2421047819985285\n",
      "Len of Validation loss: 54, Average loss: 3.9120828288572804\n",
      "Epoch: 3381, Len of Training loss: 18, Average loss: 1.8842079838116963\n",
      "Len of Validation loss: 54, Average loss: 3.9526091339411558\n",
      "Epoch: 3382, Len of Training loss: 18, Average loss: 1.5074915422333612\n",
      "Len of Validation loss: 54, Average loss: 3.528156816959381\n",
      "Epoch: 3383, Len of Training loss: 18, Average loss: 1.2711843053499858\n",
      "Len of Validation loss: 54, Average loss: 3.604815242467103\n",
      "Epoch: 3384, Len of Training loss: 18, Average loss: 1.286679784456889\n",
      "Len of Validation loss: 54, Average loss: 3.6055767701731787\n",
      "Epoch: 3385, Len of Training loss: 18, Average loss: 1.5364215042856004\n",
      "Len of Validation loss: 54, Average loss: 3.3725017298150948\n",
      "Epoch: 3386, Len of Training loss: 18, Average loss: 2.0444286002053156\n",
      "Len of Validation loss: 54, Average loss: 3.7805644560743263\n",
      "Epoch: 3387, Len of Training loss: 18, Average loss: 1.5257516900698345\n",
      "Len of Validation loss: 54, Average loss: 3.397603956637559\n",
      "Epoch: 3388, Len of Training loss: 18, Average loss: 1.2105325129297044\n",
      "Len of Validation loss: 54, Average loss: 3.661566464989274\n",
      "Epoch: 3389, Len of Training loss: 18, Average loss: 1.1826817790667217\n",
      "Len of Validation loss: 54, Average loss: 3.430780920717451\n",
      "Epoch: 3390, Len of Training loss: 18, Average loss: 1.0876189039813147\n",
      "Len of Validation loss: 54, Average loss: 3.130283116190522\n",
      "Epoch: 3391, Len of Training loss: 18, Average loss: 1.0811354981528387\n",
      "Len of Validation loss: 54, Average loss: 3.5486159170115434\n",
      "Epoch: 3392, Len of Training loss: 18, Average loss: 1.038977368010415\n",
      "Len of Validation loss: 54, Average loss: 3.5011309153503842\n",
      "Epoch: 3393, Len of Training loss: 18, Average loss: 1.020340273777644\n",
      "Len of Validation loss: 54, Average loss: 3.671997817578139\n",
      "Epoch: 3394, Len of Training loss: 18, Average loss: 1.1625959773858388\n",
      "Len of Validation loss: 54, Average loss: 3.3917519218391843\n",
      "Epoch: 3395, Len of Training loss: 18, Average loss: 1.0395161045922174\n",
      "Len of Validation loss: 54, Average loss: 3.5309607044414237\n",
      "Epoch: 3396, Len of Training loss: 18, Average loss: 0.9951130979590945\n",
      "Len of Validation loss: 54, Average loss: 3.365269273519516\n",
      "Epoch: 3397, Len of Training loss: 18, Average loss: 0.9979889889558157\n",
      "Len of Validation loss: 54, Average loss: 3.5454179832228907\n",
      "Epoch: 3398, Len of Training loss: 18, Average loss: 0.9846463203430176\n",
      "Len of Validation loss: 54, Average loss: 3.5116946597894034\n",
      "Epoch: 3399, Len of Training loss: 18, Average loss: 1.0129456950558557\n",
      "Len of Validation loss: 54, Average loss: 3.414976128825435\n",
      "Epoch: 3400, Len of Training loss: 18, Average loss: 1.0682102408674028\n",
      "Len of Validation loss: 54, Average loss: 3.354807440881376\n",
      "Epoch: 3401, Len of Training loss: 18, Average loss: 1.0725805693202548\n",
      "Len of Validation loss: 54, Average loss: 3.3583746993983232\n",
      "Epoch: 3402, Len of Training loss: 18, Average loss: 0.9860325025187598\n",
      "Len of Validation loss: 54, Average loss: 3.313546398171672\n",
      "Epoch: 3403, Len of Training loss: 18, Average loss: 0.9288694560527802\n",
      "Len of Validation loss: 54, Average loss: 3.3734547893206277\n",
      "Epoch: 3404, Len of Training loss: 18, Average loss: 0.9250913792186313\n",
      "Len of Validation loss: 54, Average loss: 3.3991770170353077\n",
      "Epoch: 3405, Len of Training loss: 18, Average loss: 0.9678710665967729\n",
      "Len of Validation loss: 54, Average loss: 3.526876343621148\n",
      "Epoch: 3406, Len of Training loss: 18, Average loss: 1.007608261373308\n",
      "Len of Validation loss: 54, Average loss: 3.490384731027815\n",
      "Epoch: 3407, Len of Training loss: 18, Average loss: 1.053185049030516\n",
      "Len of Validation loss: 54, Average loss: 3.5997008195629827\n",
      "Epoch: 3408, Len of Training loss: 18, Average loss: 1.083439803785748\n",
      "Len of Validation loss: 54, Average loss: 3.5012408704669387\n",
      "Epoch: 3409, Len of Training loss: 18, Average loss: 1.0665330058998532\n",
      "Len of Validation loss: 54, Average loss: 3.3996499787878105\n",
      "Epoch: 3410, Len of Training loss: 18, Average loss: 1.0381277799606323\n",
      "Len of Validation loss: 54, Average loss: 3.4943045907550387\n",
      "Epoch: 3411, Len of Training loss: 18, Average loss: 1.0683892998430464\n",
      "Len of Validation loss: 54, Average loss: 3.5746322119677507\n",
      "Epoch: 3412, Len of Training loss: 18, Average loss: 1.0667789346641965\n",
      "Len of Validation loss: 54, Average loss: 3.3602739142047033\n",
      "Epoch: 3413, Len of Training loss: 18, Average loss: 1.0226334631443024\n",
      "Len of Validation loss: 54, Average loss: 3.364184856414795\n",
      "Epoch: 3414, Len of Training loss: 18, Average loss: 1.0024158822165594\n",
      "Len of Validation loss: 54, Average loss: 3.3772102925512524\n",
      "Epoch: 3415, Len of Training loss: 18, Average loss: 1.0013680987887912\n",
      "Len of Validation loss: 54, Average loss: 3.533080587784449\n",
      "Epoch: 3416, Len of Training loss: 18, Average loss: 1.0232599278291066\n",
      "Len of Validation loss: 54, Average loss: 3.5097526300836495\n",
      "Epoch: 3417, Len of Training loss: 18, Average loss: 1.0473783910274506\n",
      "Len of Validation loss: 54, Average loss: 3.354864368836085\n",
      "Epoch: 3418, Len of Training loss: 18, Average loss: 1.0158974925676982\n",
      "Len of Validation loss: 54, Average loss: 3.4013770443421825\n",
      "Epoch: 3419, Len of Training loss: 18, Average loss: 0.9746902485688528\n",
      "Len of Validation loss: 54, Average loss: 3.133044859877339\n",
      "Epoch: 3420, Len of Training loss: 18, Average loss: 1.072399506966273\n",
      "Len of Validation loss: 54, Average loss: 3.3564373122321234\n",
      "Epoch: 3421, Len of Training loss: 18, Average loss: 1.1351513498359256\n",
      "Len of Validation loss: 54, Average loss: 3.8520642618338266\n",
      "Epoch: 3422, Len of Training loss: 18, Average loss: 1.4382204678323534\n",
      "Len of Validation loss: 54, Average loss: 3.676848902746483\n",
      "Epoch: 3423, Len of Training loss: 18, Average loss: 1.2469609048631456\n",
      "Len of Validation loss: 54, Average loss: 3.6328349080350666\n",
      "Epoch: 3424, Len of Training loss: 18, Average loss: 1.111916595035129\n",
      "Len of Validation loss: 54, Average loss: 3.372024458867532\n",
      "Epoch: 3425, Len of Training loss: 18, Average loss: 1.0969196458657582\n",
      "Len of Validation loss: 54, Average loss: 3.642770341149083\n",
      "Epoch: 3426, Len of Training loss: 18, Average loss: 1.077589876121945\n",
      "Len of Validation loss: 54, Average loss: 3.552684802699972\n",
      "Epoch: 3427, Len of Training loss: 18, Average loss: 1.0463943050967321\n",
      "Len of Validation loss: 54, Average loss: 3.6757622206652605\n",
      "Epoch: 3428, Len of Training loss: 18, Average loss: 1.0469609730773501\n",
      "Len of Validation loss: 54, Average loss: 3.236916803651386\n",
      "Epoch: 3429, Len of Training loss: 18, Average loss: 1.0011387169361115\n",
      "Len of Validation loss: 54, Average loss: 3.3739654543223203\n",
      "Epoch: 3430, Len of Training loss: 18, Average loss: 1.0602742830912273\n",
      "Len of Validation loss: 54, Average loss: 3.3924906286928387\n",
      "Epoch: 3431, Len of Training loss: 18, Average loss: 1.0372476047939725\n",
      "Len of Validation loss: 54, Average loss: 3.4511173257121333\n",
      "Epoch: 3432, Len of Training loss: 18, Average loss: 1.2515119579103258\n",
      "Len of Validation loss: 54, Average loss: 3.7938829263051352\n",
      "Epoch: 3433, Len of Training loss: 18, Average loss: 1.472334537241194\n",
      "Len of Validation loss: 54, Average loss: 3.3740532961156635\n",
      "Epoch: 3434, Len of Training loss: 18, Average loss: 1.2234905693266127\n",
      "Len of Validation loss: 54, Average loss: 3.5154534898422383\n",
      "Epoch: 3435, Len of Training loss: 18, Average loss: 1.146664771768782\n",
      "Len of Validation loss: 54, Average loss: 3.3650791545708976\n",
      "Epoch: 3436, Len of Training loss: 18, Average loss: 1.1304872201548681\n",
      "Len of Validation loss: 54, Average loss: 3.404035061597824\n",
      "Epoch: 3437, Len of Training loss: 18, Average loss: 1.0960095822811127\n",
      "Len of Validation loss: 54, Average loss: 3.3299212599242174\n",
      "Epoch: 3438, Len of Training loss: 18, Average loss: 1.0648936463726892\n",
      "Len of Validation loss: 54, Average loss: 3.3576591224582106\n",
      "Epoch: 3439, Len of Training loss: 18, Average loss: 1.0286866923173268\n",
      "Len of Validation loss: 54, Average loss: 3.5093389374238475\n",
      "Epoch: 3440, Len of Training loss: 18, Average loss: 1.183180358674791\n",
      "Len of Validation loss: 54, Average loss: 3.6509367205478527\n",
      "Epoch: 3441, Len of Training loss: 18, Average loss: 1.0997608502705891\n",
      "Len of Validation loss: 54, Average loss: 3.382234719064501\n",
      "Epoch: 3442, Len of Training loss: 18, Average loss: 1.023300482167138\n",
      "Len of Validation loss: 54, Average loss: 3.5753493871953754\n",
      "Epoch: 3443, Len of Training loss: 18, Average loss: 1.067261152797275\n",
      "Len of Validation loss: 54, Average loss: 3.587912345374072\n",
      "Epoch: 3444, Len of Training loss: 18, Average loss: 1.1338721215724945\n",
      "Len of Validation loss: 54, Average loss: 3.414780490928226\n",
      "Epoch: 3445, Len of Training loss: 18, Average loss: 1.162000788582696\n",
      "Len of Validation loss: 54, Average loss: 3.4817378554079266\n",
      "Epoch: 3446, Len of Training loss: 18, Average loss: 1.125248180495368\n",
      "Len of Validation loss: 54, Average loss: 3.6233222870915025\n",
      "Epoch: 3447, Len of Training loss: 18, Average loss: 1.1282493140962389\n",
      "Len of Validation loss: 54, Average loss: 3.5160890331974737\n",
      "Epoch: 3448, Len of Training loss: 18, Average loss: 1.0824998219807942\n",
      "Len of Validation loss: 54, Average loss: 3.5054452231636755\n",
      "Epoch: 3449, Len of Training loss: 18, Average loss: 1.1935062011082966\n",
      "Len of Validation loss: 54, Average loss: 3.8665361558949507\n",
      "Epoch: 3450, Len of Training loss: 18, Average loss: 1.5583409269650776\n",
      "Len of Validation loss: 54, Average loss: 3.902571439743042\n",
      "Epoch: 3451, Len of Training loss: 18, Average loss: 1.6791799399587843\n",
      "Len of Validation loss: 54, Average loss: 4.020374931671001\n",
      "Epoch: 3452, Len of Training loss: 18, Average loss: 1.4001080261336432\n",
      "Len of Validation loss: 54, Average loss: 3.662015876284352\n",
      "Epoch: 3453, Len of Training loss: 18, Average loss: 1.2991993427276611\n",
      "Len of Validation loss: 54, Average loss: 3.488733661395532\n",
      "Epoch: 3454, Len of Training loss: 18, Average loss: 1.206400387816959\n",
      "Len of Validation loss: 54, Average loss: 3.6135333942042456\n",
      "Epoch: 3455, Len of Training loss: 18, Average loss: 1.1502018438445196\n",
      "Len of Validation loss: 54, Average loss: 3.4817447110458657\n",
      "Epoch: 3456, Len of Training loss: 18, Average loss: 1.0868619978427887\n",
      "Len of Validation loss: 54, Average loss: 3.517600386231034\n",
      "Epoch: 3457, Len of Training loss: 18, Average loss: 1.0094038877222273\n",
      "Len of Validation loss: 54, Average loss: 3.5324008210941598\n",
      "Epoch: 3458, Len of Training loss: 18, Average loss: 0.9897363318337334\n",
      "Len of Validation loss: 54, Average loss: 3.6198702962310225\n",
      "Epoch: 3459, Len of Training loss: 18, Average loss: 1.257632123099433\n",
      "Len of Validation loss: 54, Average loss: 3.670765468367824\n",
      "Epoch: 3460, Len of Training loss: 18, Average loss: 1.3336237404081557\n",
      "Len of Validation loss: 54, Average loss: 3.5766340869444386\n",
      "Epoch: 3461, Len of Training loss: 18, Average loss: 1.2557885646820068\n",
      "Len of Validation loss: 54, Average loss: 3.5022482684365026\n",
      "Epoch: 3462, Len of Training loss: 18, Average loss: 1.156229664882024\n",
      "Len of Validation loss: 54, Average loss: 3.407281075362806\n",
      "Epoch: 3463, Len of Training loss: 18, Average loss: 1.381704760922326\n",
      "Len of Validation loss: 54, Average loss: 3.792108859176989\n",
      "Epoch: 3464, Len of Training loss: 18, Average loss: 1.2114932669533625\n",
      "Len of Validation loss: 54, Average loss: 3.50764532883962\n",
      "Epoch: 3465, Len of Training loss: 18, Average loss: 1.140633053249783\n",
      "Len of Validation loss: 54, Average loss: 3.4574777804039143\n",
      "Epoch: 3466, Len of Training loss: 18, Average loss: 1.073315140273836\n",
      "Len of Validation loss: 54, Average loss: 3.3100583807185844\n",
      "Epoch: 3467, Len of Training loss: 18, Average loss: 1.0165162119600508\n",
      "Len of Validation loss: 54, Average loss: 3.39766455690066\n",
      "Epoch: 3468, Len of Training loss: 18, Average loss: 0.9622271756331126\n",
      "Len of Validation loss: 54, Average loss: 3.5320616684578083\n",
      "Epoch: 3469, Len of Training loss: 18, Average loss: 1.0028056369887457\n",
      "Len of Validation loss: 54, Average loss: 3.5334036030151226\n",
      "Epoch: 3470, Len of Training loss: 18, Average loss: 1.0028873085975647\n",
      "Len of Validation loss: 54, Average loss: 3.354471340223595\n",
      "Epoch: 3471, Len of Training loss: 18, Average loss: 1.0113337371084425\n",
      "Len of Validation loss: 54, Average loss: 3.540336948853952\n",
      "Epoch: 3472, Len of Training loss: 18, Average loss: 1.3033346401320562\n",
      "Len of Validation loss: 54, Average loss: 3.4878102154643447\n",
      "Epoch: 3473, Len of Training loss: 18, Average loss: 1.2379294898774889\n",
      "Len of Validation loss: 54, Average loss: 3.439076653233281\n",
      "Epoch: 3474, Len of Training loss: 18, Average loss: 1.1968951026598613\n",
      "Len of Validation loss: 54, Average loss: 3.474943910483961\n",
      "Epoch: 3475, Len of Training loss: 18, Average loss: 1.1625473035706415\n",
      "Len of Validation loss: 54, Average loss: 3.37820698817571\n",
      "Epoch: 3476, Len of Training loss: 18, Average loss: 1.0743689503934648\n",
      "Len of Validation loss: 54, Average loss: 3.4002268855218536\n",
      "Epoch: 3477, Len of Training loss: 18, Average loss: 1.0274755424923367\n",
      "Len of Validation loss: 54, Average loss: 3.373926766492702\n",
      "Epoch: 3478, Len of Training loss: 18, Average loss: 1.1295711199442546\n",
      "Len of Validation loss: 54, Average loss: 3.8803929587205253\n",
      "Epoch: 3479, Len of Training loss: 18, Average loss: 1.5796179705195956\n",
      "Len of Validation loss: 54, Average loss: 4.166174336715981\n",
      "Epoch: 3480, Len of Training loss: 18, Average loss: 1.4459429184595745\n",
      "Len of Validation loss: 54, Average loss: 3.563018755780326\n",
      "Epoch: 3481, Len of Training loss: 18, Average loss: 1.260642409324646\n",
      "Len of Validation loss: 54, Average loss: 3.4993776656963207\n",
      "Epoch: 3482, Len of Training loss: 18, Average loss: 1.1997577945391338\n",
      "Len of Validation loss: 54, Average loss: 3.5711823834313288\n",
      "Epoch: 3483, Len of Training loss: 18, Average loss: 1.0893595880932279\n",
      "Len of Validation loss: 54, Average loss: 3.520884641894588\n",
      "Epoch: 3484, Len of Training loss: 18, Average loss: 1.0981412033240001\n",
      "Len of Validation loss: 54, Average loss: 3.5692066737899073\n",
      "Epoch: 3485, Len of Training loss: 18, Average loss: 1.1216877533329859\n",
      "Len of Validation loss: 54, Average loss: 3.383595628870858\n",
      "Epoch: 3486, Len of Training loss: 18, Average loss: 1.0469881527953677\n",
      "Len of Validation loss: 54, Average loss: 3.453845336481377\n",
      "Epoch: 3487, Len of Training loss: 18, Average loss: 1.093474758995904\n",
      "Len of Validation loss: 54, Average loss: 3.492530459607089\n",
      "Epoch: 3488, Len of Training loss: 18, Average loss: 1.0702024334006839\n",
      "Len of Validation loss: 54, Average loss: 3.467102121423792\n",
      "Epoch: 3489, Len of Training loss: 18, Average loss: 1.0650503403610654\n",
      "Len of Validation loss: 54, Average loss: 3.6847743833506548\n",
      "Epoch: 3490, Len of Training loss: 18, Average loss: 1.1762509478463068\n",
      "Len of Validation loss: 54, Average loss: 3.558890386863991\n",
      "Epoch: 3491, Len of Training loss: 18, Average loss: 1.0596971942318811\n",
      "Len of Validation loss: 54, Average loss: 3.433226752060431\n",
      "Epoch: 3492, Len of Training loss: 18, Average loss: 1.0377107991112604\n",
      "Len of Validation loss: 54, Average loss: 3.719719933138953\n",
      "Epoch: 3493, Len of Training loss: 18, Average loss: 1.0854869551128812\n",
      "Len of Validation loss: 54, Average loss: 3.4179185319829872\n",
      "Epoch: 3494, Len of Training loss: 18, Average loss: 1.0753939814037747\n",
      "Len of Validation loss: 54, Average loss: 3.476782645340319\n",
      "Epoch: 3495, Len of Training loss: 18, Average loss: 1.0765491591559515\n",
      "Len of Validation loss: 54, Average loss: 3.4972181772744215\n",
      "Epoch: 3496, Len of Training loss: 18, Average loss: 0.9920219911469353\n",
      "Len of Validation loss: 54, Average loss: 3.414885636832979\n",
      "Epoch: 3497, Len of Training loss: 18, Average loss: 1.0572025378545125\n",
      "Len of Validation loss: 54, Average loss: 3.61587134886671\n",
      "Epoch: 3498, Len of Training loss: 18, Average loss: 1.0782971216572657\n",
      "Len of Validation loss: 54, Average loss: 3.6123997094454587\n",
      "Epoch: 3499, Len of Training loss: 18, Average loss: 1.0328480634424422\n",
      "Len of Validation loss: 54, Average loss: 3.4052426936449827\n",
      "Epoch: 3500, Len of Training loss: 18, Average loss: 1.044883131980896\n",
      "Len of Validation loss: 54, Average loss: 3.4923284351825714\n",
      "Epoch: 3501, Len of Training loss: 18, Average loss: 1.1765910685062408\n",
      "Len of Validation loss: 54, Average loss: 3.6901234686374664\n",
      "Epoch: 3502, Len of Training loss: 18, Average loss: 1.1075790888733335\n",
      "Len of Validation loss: 54, Average loss: 3.7818286650710635\n",
      "Epoch: 3503, Len of Training loss: 18, Average loss: 1.1593074401219685\n",
      "Len of Validation loss: 54, Average loss: 3.636791710500364\n",
      "Epoch: 3504, Len of Training loss: 18, Average loss: 1.0655165910720825\n",
      "Len of Validation loss: 54, Average loss: 3.482476250992881\n",
      "Epoch: 3505, Len of Training loss: 18, Average loss: 1.0183206796646118\n",
      "Len of Validation loss: 54, Average loss: 3.3504112230406866\n",
      "Epoch: 3506, Len of Training loss: 18, Average loss: 1.0169746147261725\n",
      "Len of Validation loss: 54, Average loss: 3.44082612130377\n",
      "Epoch: 3507, Len of Training loss: 18, Average loss: 1.055090543296602\n",
      "Len of Validation loss: 54, Average loss: 3.3151968044263347\n",
      "Epoch: 3508, Len of Training loss: 18, Average loss: 1.0912508633401659\n",
      "Len of Validation loss: 54, Average loss: 3.560197722028803\n",
      "Epoch: 3509, Len of Training loss: 18, Average loss: 1.199159754647149\n",
      "Len of Validation loss: 54, Average loss: 3.518472376796934\n",
      "Epoch: 3510, Len of Training loss: 18, Average loss: 1.22291499376297\n",
      "Len of Validation loss: 54, Average loss: 3.549030812802138\n",
      "Epoch: 3511, Len of Training loss: 18, Average loss: 1.1093180610073938\n",
      "Len of Validation loss: 54, Average loss: 3.4865910653714782\n",
      "Epoch: 3512, Len of Training loss: 18, Average loss: 1.0522023174497817\n",
      "Len of Validation loss: 54, Average loss: 3.4242315237168914\n",
      "Epoch: 3513, Len of Training loss: 18, Average loss: 1.0381187233659956\n",
      "Len of Validation loss: 54, Average loss: 3.5565815137492285\n",
      "Epoch: 3514, Len of Training loss: 18, Average loss: 1.0038866930537753\n",
      "Len of Validation loss: 54, Average loss: 3.5466626860477306\n",
      "Epoch: 3515, Len of Training loss: 18, Average loss: 1.0652894311481051\n",
      "Len of Validation loss: 54, Average loss: 3.4839807185861797\n",
      "Epoch: 3516, Len of Training loss: 18, Average loss: 1.0217280454105802\n",
      "Len of Validation loss: 54, Average loss: 3.49322153462304\n",
      "Epoch: 3517, Len of Training loss: 18, Average loss: 1.051334861252043\n",
      "Len of Validation loss: 54, Average loss: 3.343073350411874\n",
      "Epoch: 3518, Len of Training loss: 18, Average loss: 1.049910651312934\n",
      "Len of Validation loss: 54, Average loss: 3.5505931189766637\n",
      "Epoch: 3519, Len of Training loss: 18, Average loss: 1.076939155658086\n",
      "Len of Validation loss: 54, Average loss: 3.617259411900132\n",
      "Epoch: 3520, Len of Training loss: 18, Average loss: 1.012714723745982\n",
      "Len of Validation loss: 54, Average loss: 3.415604340809363\n",
      "Epoch: 3521, Len of Training loss: 18, Average loss: 0.988913787735833\n",
      "Len of Validation loss: 54, Average loss: 3.4779856580275075\n",
      "Epoch: 3522, Len of Training loss: 18, Average loss: 0.9545557167794969\n",
      "Len of Validation loss: 54, Average loss: 3.4661631021234722\n",
      "Epoch: 3523, Len of Training loss: 18, Average loss: 0.9308507674270206\n",
      "Len of Validation loss: 54, Average loss: 3.4449892518696963\n",
      "Epoch: 3524, Len of Training loss: 18, Average loss: 1.1074086129665375\n",
      "Len of Validation loss: 54, Average loss: 3.425818055868149\n",
      "Epoch: 3525, Len of Training loss: 18, Average loss: 1.0836167534192402\n",
      "Len of Validation loss: 54, Average loss: 3.549135054703112\n",
      "Epoch: 3526, Len of Training loss: 18, Average loss: 1.2394102182653215\n",
      "Len of Validation loss: 54, Average loss: 3.5482978335133306\n",
      "Epoch: 3527, Len of Training loss: 18, Average loss: 1.4096233116255865\n",
      "Len of Validation loss: 54, Average loss: 3.4715088881828167\n",
      "Epoch: 3528, Len of Training loss: 18, Average loss: 1.2772252294752333\n",
      "Len of Validation loss: 54, Average loss: 3.4465346292213157\n",
      "Epoch: 3529, Len of Training loss: 18, Average loss: 1.4124386310577393\n",
      "Len of Validation loss: 54, Average loss: 3.6443785936744124\n",
      "Epoch: 3530, Len of Training loss: 18, Average loss: 1.2382972637812297\n",
      "Len of Validation loss: 54, Average loss: 3.5782575927398823\n",
      "Epoch: 3531, Len of Training loss: 18, Average loss: 1.114452560742696\n",
      "Len of Validation loss: 54, Average loss: 3.560471174893556\n",
      "Epoch: 3532, Len of Training loss: 18, Average loss: 1.1007868515120611\n",
      "Len of Validation loss: 54, Average loss: 3.4382284725153887\n",
      "Epoch: 3533, Len of Training loss: 18, Average loss: 1.0347077747186024\n",
      "Len of Validation loss: 54, Average loss: 3.4458654820919037\n",
      "Epoch: 3534, Len of Training loss: 18, Average loss: 1.1165392498175304\n",
      "Len of Validation loss: 54, Average loss: 3.62733554398572\n",
      "Epoch: 3535, Len of Training loss: 18, Average loss: 1.1137371162573497\n",
      "Len of Validation loss: 54, Average loss: 3.5448080919407032\n",
      "Epoch: 3536, Len of Training loss: 18, Average loss: 1.2615223659409418\n",
      "Len of Validation loss: 54, Average loss: 3.467205907459612\n",
      "Epoch: 3537, Len of Training loss: 18, Average loss: 1.2564889457490709\n",
      "Len of Validation loss: 54, Average loss: 3.4459366853590363\n",
      "Epoch: 3538, Len of Training loss: 18, Average loss: 1.2468338807423909\n",
      "Len of Validation loss: 54, Average loss: 3.541052105250182\n",
      "Epoch: 3539, Len of Training loss: 18, Average loss: 1.1243036323123508\n",
      "Len of Validation loss: 54, Average loss: 3.4366082791928894\n",
      "Epoch: 3540, Len of Training loss: 18, Average loss: 1.1803886161910162\n",
      "Len of Validation loss: 54, Average loss: 3.5143134284902504\n",
      "Epoch: 3541, Len of Training loss: 18, Average loss: 1.1278417342238956\n",
      "Len of Validation loss: 54, Average loss: 3.3742065540066473\n",
      "Epoch: 3542, Len of Training loss: 18, Average loss: 1.0841222604115803\n",
      "Len of Validation loss: 54, Average loss: 3.534944820183295\n",
      "Epoch: 3543, Len of Training loss: 18, Average loss: 1.0749519964059193\n",
      "Len of Validation loss: 54, Average loss: 3.4288222988446555\n",
      "Epoch: 3544, Len of Training loss: 18, Average loss: 1.1244188712702856\n",
      "Len of Validation loss: 54, Average loss: 3.882482847681752\n",
      "Epoch: 3545, Len of Training loss: 18, Average loss: 1.389185607433319\n",
      "Len of Validation loss: 54, Average loss: 3.5369769632816315\n",
      "Epoch: 3546, Len of Training loss: 18, Average loss: 1.3487152457237244\n",
      "Len of Validation loss: 54, Average loss: 3.966014471318987\n",
      "Epoch: 3547, Len of Training loss: 18, Average loss: 1.3839664922820196\n",
      "Len of Validation loss: 54, Average loss: 3.4609442662309715\n",
      "Epoch: 3548, Len of Training loss: 18, Average loss: 1.1865919364823236\n",
      "Len of Validation loss: 54, Average loss: 3.491661731843595\n",
      "Epoch: 3549, Len of Training loss: 18, Average loss: 1.0477408601178064\n",
      "Len of Validation loss: 54, Average loss: 3.3485342495971255\n",
      "Epoch: 3550, Len of Training loss: 18, Average loss: 1.0578956206639607\n",
      "Len of Validation loss: 54, Average loss: 3.374225879157031\n",
      "Epoch: 3551, Len of Training loss: 18, Average loss: 1.098183638519711\n",
      "Len of Validation loss: 54, Average loss: 3.5236286377465285\n",
      "Epoch: 3552, Len of Training loss: 18, Average loss: 1.284202469719781\n",
      "Len of Validation loss: 54, Average loss: 3.601037440476594\n",
      "Epoch: 3553, Len of Training loss: 18, Average loss: 1.2501855625046625\n",
      "Len of Validation loss: 54, Average loss: 3.4861869900314897\n",
      "Epoch: 3554, Len of Training loss: 18, Average loss: 1.191959175798628\n",
      "Len of Validation loss: 54, Average loss: 3.6216525203651853\n",
      "Epoch: 3555, Len of Training loss: 18, Average loss: 1.135736922423045\n",
      "Len of Validation loss: 54, Average loss: 3.6164546686190144\n",
      "Epoch: 3556, Len of Training loss: 18, Average loss: 1.019351965851254\n",
      "Len of Validation loss: 54, Average loss: 3.4525307748052807\n",
      "Epoch: 3557, Len of Training loss: 18, Average loss: 1.002140399482515\n",
      "Len of Validation loss: 54, Average loss: 3.518533150355021\n",
      "Epoch: 3558, Len of Training loss: 18, Average loss: 0.9997372859054141\n",
      "Len of Validation loss: 54, Average loss: 3.506995076382602\n",
      "Epoch: 3559, Len of Training loss: 18, Average loss: 1.0714727309015062\n",
      "Len of Validation loss: 54, Average loss: 3.699490142089349\n",
      "Epoch: 3560, Len of Training loss: 18, Average loss: 1.1016769972112443\n",
      "Len of Validation loss: 54, Average loss: 3.352025358765214\n",
      "Epoch: 3561, Len of Training loss: 18, Average loss: 1.0244865748617384\n",
      "Len of Validation loss: 54, Average loss: 3.443203646827627\n",
      "Epoch: 3562, Len of Training loss: 18, Average loss: 1.0011067158646054\n",
      "Len of Validation loss: 54, Average loss: 3.4478825396961637\n",
      "Epoch: 3563, Len of Training loss: 18, Average loss: 1.115398171875212\n",
      "Len of Validation loss: 54, Average loss: 3.631458635683413\n",
      "Epoch: 3564, Len of Training loss: 18, Average loss: 1.0800560216108959\n",
      "Len of Validation loss: 54, Average loss: 3.56425283131776\n",
      "Epoch: 3565, Len of Training loss: 18, Average loss: 1.1853246589501698\n",
      "Len of Validation loss: 54, Average loss: 3.5920152542767703\n",
      "Epoch: 3566, Len of Training loss: 18, Average loss: 1.163208583990733\n",
      "Len of Validation loss: 54, Average loss: 3.4113083614243402\n",
      "Epoch: 3567, Len of Training loss: 18, Average loss: 1.0529330439037747\n",
      "Len of Validation loss: 54, Average loss: 3.5624254919864513\n",
      "Epoch: 3568, Len of Training loss: 18, Average loss: 1.0390818781322904\n",
      "Len of Validation loss: 54, Average loss: 3.4092941758809268\n",
      "Epoch: 3569, Len of Training loss: 18, Average loss: 1.0032743414243062\n",
      "Len of Validation loss: 54, Average loss: 3.514204263687134\n",
      "Epoch: 3570, Len of Training loss: 18, Average loss: 1.0127274890740712\n",
      "Len of Validation loss: 54, Average loss: 3.582886109749476\n",
      "Epoch: 3571, Len of Training loss: 18, Average loss: 1.0017278293768566\n",
      "Len of Validation loss: 54, Average loss: 3.4339025914669037\n",
      "Epoch: 3572, Len of Training loss: 18, Average loss: 0.950392570760515\n",
      "Len of Validation loss: 54, Average loss: 3.3387441999382443\n",
      "Epoch: 3573, Len of Training loss: 18, Average loss: 1.02958235806889\n",
      "Len of Validation loss: 54, Average loss: 3.5365708845633046\n",
      "Epoch: 3574, Len of Training loss: 18, Average loss: 1.1220591697427962\n",
      "Len of Validation loss: 54, Average loss: 3.602677631157416\n",
      "Epoch: 3575, Len of Training loss: 18, Average loss: 1.0022414326667786\n",
      "Len of Validation loss: 54, Average loss: 3.3920793500211506\n",
      "Epoch: 3576, Len of Training loss: 18, Average loss: 0.9578046136432223\n",
      "Len of Validation loss: 54, Average loss: 3.3486501276493073\n",
      "Epoch: 3577, Len of Training loss: 18, Average loss: 1.0446790622340307\n",
      "Len of Validation loss: 54, Average loss: 3.4105943077140384\n",
      "Epoch: 3578, Len of Training loss: 18, Average loss: 1.0731082260608673\n",
      "Len of Validation loss: 54, Average loss: 3.3302353057596417\n",
      "Epoch: 3579, Len of Training loss: 18, Average loss: 1.1048756274912093\n",
      "Len of Validation loss: 54, Average loss: 3.4364753321365074\n",
      "Epoch: 3580, Len of Training loss: 18, Average loss: 1.1898320648405287\n",
      "Len of Validation loss: 54, Average loss: 3.5850220642707966\n",
      "Epoch: 3581, Len of Training loss: 18, Average loss: 1.1701202690601349\n",
      "Len of Validation loss: 54, Average loss: 3.5441158292470156\n",
      "Epoch: 3582, Len of Training loss: 18, Average loss: 1.1404345002439287\n",
      "Len of Validation loss: 54, Average loss: 3.5521501545552856\n",
      "Epoch: 3583, Len of Training loss: 18, Average loss: 1.1470968392160203\n",
      "Len of Validation loss: 54, Average loss: 3.4589180482758417\n",
      "Epoch: 3584, Len of Training loss: 18, Average loss: 1.1995469397968717\n",
      "Len of Validation loss: 54, Average loss: 3.756733340245706\n",
      "Epoch: 3585, Len of Training loss: 18, Average loss: 1.1893269916375477\n",
      "Len of Validation loss: 54, Average loss: 3.4383398537282592\n",
      "Epoch: 3586, Len of Training loss: 18, Average loss: 1.091899077097575\n",
      "Len of Validation loss: 54, Average loss: 3.4299508498774633\n",
      "Epoch: 3587, Len of Training loss: 18, Average loss: 1.0515170627170138\n",
      "Len of Validation loss: 54, Average loss: 3.3997682145348302\n",
      "Epoch: 3588, Len of Training loss: 18, Average loss: 1.056749102142122\n",
      "Len of Validation loss: 54, Average loss: 3.2356416196734816\n",
      "Epoch: 3589, Len of Training loss: 18, Average loss: 1.0282140771547954\n",
      "Len of Validation loss: 54, Average loss: 3.3843518749431327\n",
      "Epoch: 3590, Len of Training loss: 18, Average loss: 0.9934603141413795\n",
      "Len of Validation loss: 54, Average loss: 3.2981481817033558\n",
      "Epoch: 3591, Len of Training loss: 18, Average loss: 0.9541620347234938\n",
      "Len of Validation loss: 54, Average loss: 3.5346395483723394\n",
      "Epoch: 3592, Len of Training loss: 18, Average loss: 1.0403332610925038\n",
      "Len of Validation loss: 54, Average loss: 3.4736405699341386\n",
      "Epoch: 3593, Len of Training loss: 18, Average loss: 1.0230554342269897\n",
      "Len of Validation loss: 54, Average loss: 3.399936883537858\n",
      "Epoch: 3594, Len of Training loss: 18, Average loss: 1.0688919126987457\n",
      "Len of Validation loss: 54, Average loss: 3.3891967369450464\n",
      "Epoch: 3595, Len of Training loss: 18, Average loss: 1.059557514058219\n",
      "Len of Validation loss: 54, Average loss: 3.5351686599078\n",
      "Epoch: 3596, Len of Training loss: 18, Average loss: 1.0863809684912364\n",
      "Len of Validation loss: 54, Average loss: 3.4880013973624617\n",
      "Epoch: 3597, Len of Training loss: 18, Average loss: 0.9823639161056943\n",
      "Len of Validation loss: 54, Average loss: 3.477273381418652\n",
      "Epoch: 3598, Len of Training loss: 18, Average loss: 1.0633688469727833\n",
      "Len of Validation loss: 54, Average loss: 3.264709570893535\n",
      "Epoch: 3599, Len of Training loss: 18, Average loss: 1.0072019729349349\n",
      "Len of Validation loss: 54, Average loss: 3.629553940561083\n",
      "Epoch: 3600, Len of Training loss: 18, Average loss: 1.1396857963667975\n",
      "Len of Validation loss: 54, Average loss: 3.5755890486416995\n",
      "Epoch: 3601, Len of Training loss: 18, Average loss: 1.0946930183304682\n",
      "Len of Validation loss: 54, Average loss: 3.6100284225410886\n",
      "Epoch: 3602, Len of Training loss: 18, Average loss: 1.1589194966687097\n",
      "Len of Validation loss: 54, Average loss: 3.5430279771486917\n",
      "Epoch: 3603, Len of Training loss: 18, Average loss: 1.646102203263177\n",
      "Len of Validation loss: 54, Average loss: 3.945148794739335\n",
      "Epoch: 3604, Len of Training loss: 18, Average loss: 1.470923191971249\n",
      "Len of Validation loss: 54, Average loss: 3.5565349238890187\n",
      "Epoch: 3605, Len of Training loss: 18, Average loss: 1.358202616373698\n",
      "Len of Validation loss: 54, Average loss: 3.627080625957913\n",
      "Epoch: 3606, Len of Training loss: 18, Average loss: 1.1738065514299605\n",
      "Len of Validation loss: 54, Average loss: 3.4692005426795394\n",
      "Epoch: 3607, Len of Training loss: 18, Average loss: 1.0442538029617734\n",
      "Len of Validation loss: 54, Average loss: 3.468846180924663\n",
      "Epoch: 3608, Len of Training loss: 18, Average loss: 1.0871708790461223\n",
      "Len of Validation loss: 54, Average loss: 3.3665573983280748\n",
      "Epoch: 3609, Len of Training loss: 18, Average loss: 1.1053847206963434\n",
      "Len of Validation loss: 54, Average loss: 3.4792316555976868\n",
      "Epoch: 3610, Len of Training loss: 18, Average loss: 1.091303630007638\n",
      "Len of Validation loss: 54, Average loss: 3.24838419313784\n",
      "Epoch: 3611, Len of Training loss: 18, Average loss: 1.1003236042128668\n",
      "Len of Validation loss: 54, Average loss: 3.5111583968003592\n",
      "Epoch: 3612, Len of Training loss: 18, Average loss: 1.0530033674505022\n",
      "Len of Validation loss: 54, Average loss: 3.4954155109546803\n",
      "Epoch: 3613, Len of Training loss: 18, Average loss: 1.0291465885109372\n",
      "Len of Validation loss: 54, Average loss: 3.3012882559387773\n",
      "Epoch: 3614, Len of Training loss: 18, Average loss: 0.971038152774175\n",
      "Len of Validation loss: 54, Average loss: 3.4012629169004933\n",
      "Epoch: 3615, Len of Training loss: 18, Average loss: 1.0269305374887254\n",
      "Len of Validation loss: 54, Average loss: 3.468800147374471\n",
      "Epoch: 3616, Len of Training loss: 18, Average loss: 0.9823201828532748\n",
      "Len of Validation loss: 54, Average loss: 3.47947637019334\n",
      "Epoch: 3617, Len of Training loss: 18, Average loss: 1.1289704905615912\n",
      "Len of Validation loss: 54, Average loss: 3.3245569898022547\n",
      "Epoch: 3618, Len of Training loss: 18, Average loss: 1.1475219991472032\n",
      "Len of Validation loss: 54, Average loss: 3.5753636867911727\n",
      "Epoch: 3619, Len of Training loss: 18, Average loss: 1.2245887849065993\n",
      "Len of Validation loss: 54, Average loss: 3.325932330555386\n",
      "Epoch: 3620, Len of Training loss: 18, Average loss: 1.1628748840755887\n",
      "Len of Validation loss: 54, Average loss: 3.4592205164609133\n",
      "Epoch: 3621, Len of Training loss: 18, Average loss: 1.1802517871061962\n",
      "Len of Validation loss: 54, Average loss: 3.618911742060273\n",
      "Epoch: 3622, Len of Training loss: 18, Average loss: 1.143575714694129\n",
      "Len of Validation loss: 54, Average loss: 3.482773024726797\n",
      "Epoch: 3623, Len of Training loss: 18, Average loss: 1.100839889711804\n",
      "Len of Validation loss: 54, Average loss: 3.6842865954946586\n",
      "Epoch: 3624, Len of Training loss: 18, Average loss: 1.0518380668428209\n",
      "Len of Validation loss: 54, Average loss: 3.337900592221154\n",
      "Epoch: 3625, Len of Training loss: 18, Average loss: 0.9747072292698754\n",
      "Len of Validation loss: 54, Average loss: 3.466430607769224\n",
      "Epoch: 3626, Len of Training loss: 18, Average loss: 0.9962300492657555\n",
      "Len of Validation loss: 54, Average loss: 3.5830781205936715\n",
      "Epoch: 3627, Len of Training loss: 18, Average loss: 0.9883704218599532\n",
      "Len of Validation loss: 54, Average loss: 3.406901607910792\n",
      "Epoch: 3628, Len of Training loss: 18, Average loss: 1.055456406540341\n",
      "Len of Validation loss: 54, Average loss: 3.4199863219702684\n",
      "Epoch: 3629, Len of Training loss: 18, Average loss: 1.0525790088706546\n",
      "Len of Validation loss: 54, Average loss: 3.413625876108805\n",
      "Epoch: 3630, Len of Training loss: 18, Average loss: 1.039076543516583\n",
      "Len of Validation loss: 54, Average loss: 3.449092506258576\n",
      "Epoch: 3631, Len of Training loss: 18, Average loss: 1.0145526395903692\n",
      "Len of Validation loss: 54, Average loss: 3.602022527544587\n",
      "Epoch: 3632, Len of Training loss: 18, Average loss: 1.0411006841394637\n",
      "Len of Validation loss: 54, Average loss: 3.4211339773955167\n",
      "Epoch: 3633, Len of Training loss: 18, Average loss: 1.0504620439476438\n",
      "Len of Validation loss: 54, Average loss: 3.357767038875156\n",
      "Epoch: 3634, Len of Training loss: 18, Average loss: 1.0499892234802246\n",
      "Len of Validation loss: 54, Average loss: 3.4318162235948773\n",
      "Epoch: 3635, Len of Training loss: 18, Average loss: 1.1117393275101979\n",
      "Len of Validation loss: 54, Average loss: 3.4878108667002783\n",
      "Epoch: 3636, Len of Training loss: 18, Average loss: 1.132644126812617\n",
      "Len of Validation loss: 54, Average loss: 3.6007423522295774\n",
      "Epoch: 3637, Len of Training loss: 18, Average loss: 1.2857758965757158\n",
      "Len of Validation loss: 54, Average loss: 3.4225940152450844\n",
      "Epoch: 3638, Len of Training loss: 18, Average loss: 1.1304828458362155\n",
      "Len of Validation loss: 54, Average loss: 3.3814792964193554\n",
      "Epoch: 3639, Len of Training loss: 18, Average loss: 1.1195474200778537\n",
      "Len of Validation loss: 54, Average loss: 3.5412546186535447\n",
      "Epoch: 3640, Len of Training loss: 18, Average loss: 1.0830096436871424\n",
      "Len of Validation loss: 54, Average loss: 3.561823206919211\n",
      "Epoch: 3641, Len of Training loss: 18, Average loss: 1.047932979133394\n",
      "Len of Validation loss: 54, Average loss: 3.4866842837245375\n",
      "Epoch: 3642, Len of Training loss: 18, Average loss: 1.1736864414479997\n",
      "Len of Validation loss: 54, Average loss: 3.536430686712265\n",
      "Epoch: 3643, Len of Training loss: 18, Average loss: 1.0985961225297716\n",
      "Len of Validation loss: 54, Average loss: 3.3935491619286715\n",
      "Epoch: 3644, Len of Training loss: 18, Average loss: 1.099599728981654\n",
      "Len of Validation loss: 54, Average loss: 3.8661220371723175\n",
      "Epoch: 3645, Len of Training loss: 18, Average loss: 1.1167185538344913\n",
      "Len of Validation loss: 54, Average loss: 3.4891231369089195\n",
      "Epoch: 3646, Len of Training loss: 18, Average loss: 1.1896421644422743\n",
      "Len of Validation loss: 54, Average loss: 3.510809960188689\n",
      "Epoch: 3647, Len of Training loss: 18, Average loss: 1.1033109260929956\n",
      "Len of Validation loss: 54, Average loss: 3.4124066675150835\n",
      "Epoch: 3648, Len of Training loss: 18, Average loss: 1.0323972271548376\n",
      "Len of Validation loss: 54, Average loss: 3.529032065912529\n",
      "Epoch: 3649, Len of Training loss: 18, Average loss: 1.0034553938441806\n",
      "Len of Validation loss: 54, Average loss: 3.4150707843127073\n",
      "Epoch: 3650, Len of Training loss: 18, Average loss: 0.9680400788784027\n",
      "Len of Validation loss: 54, Average loss: 3.5333341128296323\n",
      "Epoch: 3651, Len of Training loss: 18, Average loss: 0.9444451100296445\n",
      "Len of Validation loss: 54, Average loss: 3.500195720681438\n",
      "Epoch: 3652, Len of Training loss: 18, Average loss: 1.0321347382333543\n",
      "Len of Validation loss: 54, Average loss: 3.4992240799797907\n",
      "Epoch: 3653, Len of Training loss: 18, Average loss: 1.1621622112062242\n",
      "Len of Validation loss: 54, Average loss: 3.375159287894214\n",
      "Epoch: 3654, Len of Training loss: 18, Average loss: 1.0384310980637868\n",
      "Len of Validation loss: 54, Average loss: 3.492721124931618\n",
      "Epoch: 3655, Len of Training loss: 18, Average loss: 1.0049649278322856\n",
      "Len of Validation loss: 54, Average loss: 3.4331933315153473\n",
      "Epoch: 3656, Len of Training loss: 18, Average loss: 1.0573224524656932\n",
      "Len of Validation loss: 54, Average loss: 3.582308559506028\n",
      "Epoch: 3657, Len of Training loss: 18, Average loss: 1.2633671561876934\n",
      "Len of Validation loss: 54, Average loss: 3.612958994176653\n",
      "Epoch: 3658, Len of Training loss: 18, Average loss: 1.2125813961029053\n",
      "Len of Validation loss: 54, Average loss: 3.516392291695983\n",
      "Epoch: 3659, Len of Training loss: 18, Average loss: 1.1821816232469347\n",
      "Len of Validation loss: 54, Average loss: 3.4904963153379933\n",
      "Epoch: 3660, Len of Training loss: 18, Average loss: 1.139172825548384\n",
      "Len of Validation loss: 54, Average loss: 3.4609042737219067\n",
      "Epoch: 3661, Len of Training loss: 18, Average loss: 1.093594600756963\n",
      "Len of Validation loss: 54, Average loss: 3.4772640179704735\n",
      "Epoch: 3662, Len of Training loss: 18, Average loss: 1.068734112713072\n",
      "Len of Validation loss: 54, Average loss: 3.3996869634698936\n",
      "Epoch: 3663, Len of Training loss: 18, Average loss: 1.0282352202468448\n",
      "Len of Validation loss: 54, Average loss: 3.4687144833582417\n",
      "Epoch: 3664, Len of Training loss: 18, Average loss: 1.0145817697048187\n",
      "Len of Validation loss: 54, Average loss: 3.428605290474715\n",
      "Epoch: 3665, Len of Training loss: 18, Average loss: 0.9846361014578078\n",
      "Len of Validation loss: 54, Average loss: 3.490239884014483\n",
      "Epoch: 3666, Len of Training loss: 18, Average loss: 1.1199344661500719\n",
      "Len of Validation loss: 54, Average loss: 3.572508697156553\n",
      "Epoch: 3667, Len of Training loss: 18, Average loss: 1.065506829155816\n",
      "Len of Validation loss: 54, Average loss: 3.368727015124427\n",
      "Epoch: 3668, Len of Training loss: 18, Average loss: 0.9913261466556125\n",
      "Len of Validation loss: 54, Average loss: 3.364984883202447\n",
      "Epoch: 3669, Len of Training loss: 18, Average loss: 1.0606228046947055\n",
      "Len of Validation loss: 54, Average loss: 3.6232078627303794\n",
      "Epoch: 3670, Len of Training loss: 18, Average loss: 1.209724399778578\n",
      "Len of Validation loss: 54, Average loss: 3.4468181188459748\n",
      "Epoch: 3671, Len of Training loss: 18, Average loss: 1.436094151602851\n",
      "Len of Validation loss: 54, Average loss: 3.6629318826728396\n",
      "Epoch: 3672, Len of Training loss: 18, Average loss: 1.2110820942454867\n",
      "Len of Validation loss: 54, Average loss: 3.68343123793602\n",
      "Epoch: 3673, Len of Training loss: 18, Average loss: 1.0950246188375685\n",
      "Len of Validation loss: 54, Average loss: 3.3073812127113342\n",
      "Epoch: 3674, Len of Training loss: 18, Average loss: 1.07406766878234\n",
      "Len of Validation loss: 54, Average loss: 3.4508104379530304\n",
      "Epoch: 3675, Len of Training loss: 18, Average loss: 1.0300210648112826\n",
      "Len of Validation loss: 54, Average loss: 3.4354794577316\n",
      "Epoch: 3676, Len of Training loss: 18, Average loss: 1.0111678606934018\n",
      "Len of Validation loss: 54, Average loss: 3.3491865076400615\n",
      "Epoch: 3677, Len of Training loss: 18, Average loss: 1.0313369101948209\n",
      "Len of Validation loss: 54, Average loss: 3.4073833657635584\n",
      "Epoch: 3678, Len of Training loss: 18, Average loss: 1.0158155494266086\n",
      "Len of Validation loss: 54, Average loss: 3.531881789366404\n",
      "Epoch: 3679, Len of Training loss: 18, Average loss: 0.9847645958264669\n",
      "Len of Validation loss: 54, Average loss: 3.2930859702604787\n",
      "Epoch: 3680, Len of Training loss: 18, Average loss: 0.921181344323688\n",
      "Len of Validation loss: 54, Average loss: 3.4977548519770303\n",
      "Epoch: 3681, Len of Training loss: 18, Average loss: 0.9365084204408858\n",
      "Len of Validation loss: 54, Average loss: 3.5621446117206856\n",
      "Epoch: 3682, Len of Training loss: 18, Average loss: 1.0031692816151514\n",
      "Len of Validation loss: 54, Average loss: 3.288352725682435\n",
      "Epoch: 3683, Len of Training loss: 18, Average loss: 0.9948398437764909\n",
      "Len of Validation loss: 54, Average loss: 3.4125832286145954\n",
      "Epoch: 3684, Len of Training loss: 18, Average loss: 1.063750124639935\n",
      "Len of Validation loss: 54, Average loss: 3.433847556511561\n",
      "Epoch: 3685, Len of Training loss: 18, Average loss: 1.1063571373621623\n",
      "Len of Validation loss: 54, Average loss: 3.594896230432722\n",
      "Epoch: 3686, Len of Training loss: 18, Average loss: 1.2574609186914232\n",
      "Len of Validation loss: 54, Average loss: 3.3881898566528603\n",
      "Epoch: 3687, Len of Training loss: 18, Average loss: 1.1193629999955494\n",
      "Len of Validation loss: 54, Average loss: 3.6929380893707275\n",
      "Epoch: 3688, Len of Training loss: 18, Average loss: 1.1745862861474354\n",
      "Len of Validation loss: 54, Average loss: 3.6125335218729795\n",
      "Epoch: 3689, Len of Training loss: 18, Average loss: 1.0436133742332458\n",
      "Len of Validation loss: 54, Average loss: 3.5552812903015703\n",
      "Epoch: 3690, Len of Training loss: 18, Average loss: 1.0196351640754275\n",
      "Len of Validation loss: 54, Average loss: 3.3077074686686196\n",
      "Epoch: 3691, Len of Training loss: 18, Average loss: 0.9771949185265435\n",
      "Len of Validation loss: 54, Average loss: 3.5006370235372475\n",
      "Epoch: 3692, Len of Training loss: 18, Average loss: 0.9712794025739034\n",
      "Len of Validation loss: 54, Average loss: 3.500169544308274\n",
      "Epoch: 3693, Len of Training loss: 18, Average loss: 0.9936014711856842\n",
      "Len of Validation loss: 54, Average loss: 3.594005609000171\n",
      "Epoch: 3694, Len of Training loss: 18, Average loss: 0.980634911192788\n",
      "Len of Validation loss: 54, Average loss: 3.345045510265562\n",
      "Epoch: 3695, Len of Training loss: 18, Average loss: 0.9602330724398295\n",
      "Len of Validation loss: 54, Average loss: 3.5445207832036196\n",
      "Epoch: 3696, Len of Training loss: 18, Average loss: 0.9790979690021939\n",
      "Len of Validation loss: 54, Average loss: 3.359303066024074\n",
      "Epoch: 3697, Len of Training loss: 18, Average loss: 1.0217926767137315\n",
      "Len of Validation loss: 54, Average loss: 3.4888321613823927\n",
      "Epoch: 3698, Len of Training loss: 18, Average loss: 0.9729321897029877\n",
      "Len of Validation loss: 54, Average loss: 3.5732129017512\n",
      "Epoch: 3699, Len of Training loss: 18, Average loss: 1.0602202547921076\n",
      "Len of Validation loss: 54, Average loss: 3.632469277690958\n",
      "Epoch: 3700, Len of Training loss: 18, Average loss: 1.0020132395956252\n",
      "Len of Validation loss: 54, Average loss: 3.410398492106685\n",
      "Epoch: 3701, Len of Training loss: 18, Average loss: 0.9769363668229845\n",
      "Len of Validation loss: 54, Average loss: 3.4774916337596045\n",
      "Epoch: 3702, Len of Training loss: 18, Average loss: 1.0372315578990512\n",
      "Len of Validation loss: 54, Average loss: 3.5049021784906036\n",
      "Epoch: 3703, Len of Training loss: 18, Average loss: 1.020029518339369\n",
      "Len of Validation loss: 54, Average loss: 3.5218028050881847\n",
      "Epoch: 3704, Len of Training loss: 18, Average loss: 1.0148307416174147\n",
      "Len of Validation loss: 54, Average loss: 3.495341411343327\n",
      "Epoch: 3705, Len of Training loss: 18, Average loss: 1.1313451561662886\n",
      "Len of Validation loss: 54, Average loss: 3.4179402159319983\n",
      "Epoch: 3706, Len of Training loss: 18, Average loss: 1.1482757329940796\n",
      "Len of Validation loss: 54, Average loss: 3.562022183780317\n",
      "Epoch: 3707, Len of Training loss: 18, Average loss: 1.1779952711529202\n",
      "Len of Validation loss: 54, Average loss: 3.3944611847400665\n",
      "Epoch: 3708, Len of Training loss: 18, Average loss: 1.129365815056695\n",
      "Len of Validation loss: 54, Average loss: 3.4001168332718037\n",
      "Epoch: 3709, Len of Training loss: 18, Average loss: 1.1461485491858587\n",
      "Len of Validation loss: 54, Average loss: 3.3140577375888824\n",
      "Epoch: 3710, Len of Training loss: 18, Average loss: 1.1555244227250416\n",
      "Len of Validation loss: 54, Average loss: 3.570866706194701\n",
      "Epoch: 3711, Len of Training loss: 18, Average loss: 1.045550028483073\n",
      "Len of Validation loss: 54, Average loss: 3.3562177176828736\n",
      "Epoch: 3712, Len of Training loss: 18, Average loss: 1.0293421712186601\n",
      "Len of Validation loss: 54, Average loss: 3.5796664081237934\n",
      "Epoch: 3713, Len of Training loss: 18, Average loss: 1.0580825573868222\n",
      "Len of Validation loss: 54, Average loss: 3.646065581727911\n",
      "Epoch: 3714, Len of Training loss: 18, Average loss: 1.1074133945835962\n",
      "Len of Validation loss: 54, Average loss: 3.5325389162257865\n",
      "Epoch: 3715, Len of Training loss: 18, Average loss: 1.0954969194200304\n",
      "Len of Validation loss: 54, Average loss: 3.4511493941148124\n",
      "Epoch: 3716, Len of Training loss: 18, Average loss: 1.0439200202624004\n",
      "Len of Validation loss: 54, Average loss: 3.4905782606866627\n",
      "Epoch: 3717, Len of Training loss: 18, Average loss: 1.0348817507425945\n",
      "Len of Validation loss: 54, Average loss: 3.4351083404488034\n",
      "Epoch: 3718, Len of Training loss: 18, Average loss: 1.0420029560724895\n",
      "Len of Validation loss: 54, Average loss: 3.4115732025217125\n",
      "Epoch: 3719, Len of Training loss: 18, Average loss: 1.0821271075142755\n",
      "Len of Validation loss: 54, Average loss: 3.56120432416598\n",
      "Epoch: 3720, Len of Training loss: 18, Average loss: 1.0954775280422635\n",
      "Len of Validation loss: 54, Average loss: 3.2408584190739527\n",
      "Epoch: 3721, Len of Training loss: 18, Average loss: 1.0994162559509277\n",
      "Len of Validation loss: 54, Average loss: 3.390601681338416\n",
      "Epoch: 3722, Len of Training loss: 18, Average loss: 1.0627181695567236\n",
      "Len of Validation loss: 54, Average loss: 3.4057999187045627\n",
      "Epoch: 3723, Len of Training loss: 18, Average loss: 1.0379338363806407\n",
      "Len of Validation loss: 54, Average loss: 3.55803339348899\n",
      "Epoch: 3724, Len of Training loss: 18, Average loss: 1.19816964202457\n",
      "Len of Validation loss: 54, Average loss: 3.652036846787841\n",
      "Epoch: 3725, Len of Training loss: 18, Average loss: 1.5995643536249797\n",
      "Len of Validation loss: 54, Average loss: 3.566370917691125\n",
      "Epoch: 3726, Len of Training loss: 18, Average loss: 1.4183233380317688\n",
      "Len of Validation loss: 54, Average loss: 3.851140378801911\n",
      "Epoch: 3727, Len of Training loss: 18, Average loss: 1.301424013243781\n",
      "Len of Validation loss: 54, Average loss: 3.5657265429143554\n",
      "Epoch: 3728, Len of Training loss: 18, Average loss: 1.1420865621831682\n",
      "Len of Validation loss: 54, Average loss: 3.2694563744244753\n",
      "Epoch: 3729, Len of Training loss: 18, Average loss: 1.0449388921260834\n",
      "Len of Validation loss: 54, Average loss: 3.3762856445930622\n",
      "Epoch: 3730, Len of Training loss: 18, Average loss: 1.0068313744333055\n",
      "Len of Validation loss: 54, Average loss: 3.3819497845791004\n",
      "Epoch: 3731, Len of Training loss: 18, Average loss: 1.0362124807304807\n",
      "Len of Validation loss: 54, Average loss: 3.3726061483224234\n",
      "Epoch: 3732, Len of Training loss: 18, Average loss: 1.0917586651113298\n",
      "Len of Validation loss: 54, Average loss: 3.5780450801054635\n",
      "Epoch: 3733, Len of Training loss: 18, Average loss: 1.0152566300498114\n",
      "Len of Validation loss: 54, Average loss: 3.4719575312402515\n",
      "Epoch: 3734, Len of Training loss: 18, Average loss: 1.044556114408705\n",
      "Len of Validation loss: 54, Average loss: 3.3639196201607033\n",
      "Epoch: 3735, Len of Training loss: 18, Average loss: 1.067236191696591\n",
      "Len of Validation loss: 54, Average loss: 3.461539172463947\n",
      "Epoch: 3736, Len of Training loss: 18, Average loss: 1.0283236735396915\n",
      "Len of Validation loss: 54, Average loss: 3.365290683728677\n",
      "Epoch: 3737, Len of Training loss: 18, Average loss: 1.2188834216859605\n",
      "Len of Validation loss: 54, Average loss: 3.6931579576598272\n",
      "Epoch: 3738, Len of Training loss: 18, Average loss: 1.3341242803467646\n",
      "Len of Validation loss: 54, Average loss: 3.5717177523507013\n",
      "Epoch: 3739, Len of Training loss: 18, Average loss: 1.3546101649602253\n",
      "Len of Validation loss: 54, Average loss: 3.503505638352147\n",
      "Epoch: 3740, Len of Training loss: 18, Average loss: 1.2039854791429307\n",
      "Len of Validation loss: 54, Average loss: 3.3294732493382915\n",
      "Epoch: 3741, Len of Training loss: 18, Average loss: 1.0668673813343048\n",
      "Len of Validation loss: 54, Average loss: 3.5383398996459112\n",
      "Epoch: 3742, Len of Training loss: 18, Average loss: 1.056843678156535\n",
      "Len of Validation loss: 54, Average loss: 3.5488727501145116\n",
      "Epoch: 3743, Len of Training loss: 18, Average loss: 1.162355124950409\n",
      "Len of Validation loss: 54, Average loss: 3.564443677663803\n",
      "Epoch: 3744, Len of Training loss: 18, Average loss: 1.1478220687972174\n",
      "Len of Validation loss: 54, Average loss: 3.476183326156051\n",
      "Epoch: 3745, Len of Training loss: 18, Average loss: 1.0527779426839616\n",
      "Len of Validation loss: 54, Average loss: 3.4903444802319563\n",
      "Epoch: 3746, Len of Training loss: 18, Average loss: 1.0310606757799785\n",
      "Len of Validation loss: 54, Average loss: 3.5449547094327434\n",
      "Epoch: 3747, Len of Training loss: 18, Average loss: 1.0793095926443736\n",
      "Len of Validation loss: 54, Average loss: 3.216170272341481\n",
      "Epoch: 3748, Len of Training loss: 18, Average loss: 1.0118242965804205\n",
      "Len of Validation loss: 54, Average loss: 3.568915432250058\n",
      "Epoch: 3749, Len of Training loss: 18, Average loss: 1.0436804460154638\n",
      "Len of Validation loss: 54, Average loss: 3.3837985120437764\n",
      "Epoch: 3750, Len of Training loss: 18, Average loss: 1.0207395652929943\n",
      "Len of Validation loss: 54, Average loss: 3.5454930552729853\n",
      "Epoch: 3751, Len of Training loss: 18, Average loss: 1.019891278611289\n",
      "Len of Validation loss: 54, Average loss: 3.3696184290779962\n",
      "Epoch: 3752, Len of Training loss: 18, Average loss: 1.0547868841224246\n",
      "Len of Validation loss: 54, Average loss: 3.4896170209955284\n",
      "Epoch: 3753, Len of Training loss: 18, Average loss: 1.1074426174163818\n",
      "Len of Validation loss: 54, Average loss: 3.553808984933076\n",
      "Epoch: 3754, Len of Training loss: 18, Average loss: 1.101231078306834\n",
      "Len of Validation loss: 54, Average loss: 3.7440785898102655\n",
      "Epoch: 3755, Len of Training loss: 18, Average loss: 1.2012291351954143\n",
      "Len of Validation loss: 54, Average loss: 3.409214499923918\n",
      "Epoch: 3756, Len of Training loss: 18, Average loss: 1.1806445287333593\n",
      "Len of Validation loss: 54, Average loss: 3.5320014313415244\n",
      "Epoch: 3757, Len of Training loss: 18, Average loss: 1.3361509376102023\n",
      "Len of Validation loss: 54, Average loss: 3.757448642342179\n",
      "Epoch: 3758, Len of Training loss: 18, Average loss: 1.1708898213174608\n",
      "Len of Validation loss: 54, Average loss: 3.4732284523822643\n",
      "Epoch: 3759, Len of Training loss: 18, Average loss: 1.0679580238130357\n",
      "Len of Validation loss: 54, Average loss: 3.2804818782541485\n",
      "Epoch: 3760, Len of Training loss: 18, Average loss: 1.102078033818139\n",
      "Len of Validation loss: 54, Average loss: 3.479524285705001\n",
      "Epoch: 3761, Len of Training loss: 18, Average loss: 1.0746108492215474\n",
      "Len of Validation loss: 54, Average loss: 3.339409860195937\n",
      "Epoch: 3762, Len of Training loss: 18, Average loss: 1.1203706529405382\n",
      "Len of Validation loss: 54, Average loss: 3.554141225638213\n",
      "Epoch: 3763, Len of Training loss: 18, Average loss: 1.1021864546669855\n",
      "Len of Validation loss: 54, Average loss: 3.614357074101766\n",
      "Epoch: 3764, Len of Training loss: 18, Average loss: 1.03749993774626\n",
      "Len of Validation loss: 54, Average loss: 3.5477664316142046\n",
      "Epoch: 3765, Len of Training loss: 18, Average loss: 1.0768207278516557\n",
      "Len of Validation loss: 54, Average loss: 3.5553537905216217\n",
      "Epoch: 3766, Len of Training loss: 18, Average loss: 1.081795871257782\n",
      "Len of Validation loss: 54, Average loss: 3.557780936912254\n",
      "Epoch: 3767, Len of Training loss: 18, Average loss: 1.082229087750117\n",
      "Len of Validation loss: 54, Average loss: 3.557600256469515\n",
      "Epoch: 3768, Len of Training loss: 18, Average loss: 1.0555624465147655\n",
      "Len of Validation loss: 54, Average loss: 3.6518575758845717\n",
      "Epoch: 3769, Len of Training loss: 18, Average loss: 1.093545397122701\n",
      "Len of Validation loss: 54, Average loss: 3.4999321312816054\n",
      "Epoch: 3770, Len of Training loss: 18, Average loss: 1.0721874038378398\n",
      "Len of Validation loss: 54, Average loss: 3.6874507058549812\n",
      "Epoch: 3771, Len of Training loss: 18, Average loss: 1.0662322574191623\n",
      "Len of Validation loss: 54, Average loss: 3.510325378841824\n",
      "Epoch: 3772, Len of Training loss: 18, Average loss: 1.0561758677164714\n",
      "Len of Validation loss: 54, Average loss: 3.6491650100107544\n",
      "Epoch: 3773, Len of Training loss: 18, Average loss: 1.0912929640875921\n",
      "Len of Validation loss: 54, Average loss: 3.4702940384546914\n",
      "Epoch: 3774, Len of Training loss: 18, Average loss: 1.0927821960714128\n",
      "Len of Validation loss: 54, Average loss: 3.5846419036388397\n",
      "Epoch: 3775, Len of Training loss: 18, Average loss: 1.2385320862134297\n",
      "Len of Validation loss: 54, Average loss: 3.7814863280013755\n",
      "Epoch: 3776, Len of Training loss: 18, Average loss: 1.1572292579544916\n",
      "Len of Validation loss: 54, Average loss: 3.5109539561801486\n",
      "Epoch: 3777, Len of Training loss: 18, Average loss: 1.1416432989968195\n",
      "Len of Validation loss: 54, Average loss: 3.4332360923290253\n",
      "Epoch: 3778, Len of Training loss: 18, Average loss: 1.0802633431222703\n",
      "Len of Validation loss: 54, Average loss: 3.833656665351656\n",
      "Epoch: 3779, Len of Training loss: 18, Average loss: 1.03466045194202\n",
      "Len of Validation loss: 54, Average loss: 3.7446282583254353\n",
      "Epoch: 3780, Len of Training loss: 18, Average loss: 0.9908110201358795\n",
      "Len of Validation loss: 54, Average loss: 3.389490195998439\n",
      "Epoch: 3781, Len of Training loss: 18, Average loss: 0.9440140790409512\n",
      "Len of Validation loss: 54, Average loss: 3.3979371852344937\n",
      "Epoch: 3782, Len of Training loss: 18, Average loss: 1.1807681553893619\n",
      "Len of Validation loss: 54, Average loss: 3.6078345047103033\n",
      "Epoch: 3783, Len of Training loss: 18, Average loss: 1.53142731057273\n",
      "Len of Validation loss: 54, Average loss: 3.728119829186687\n",
      "Epoch: 3784, Len of Training loss: 18, Average loss: 1.4077818128797743\n",
      "Len of Validation loss: 54, Average loss: 3.566554679914757\n",
      "Epoch: 3785, Len of Training loss: 18, Average loss: 1.220316423310174\n",
      "Len of Validation loss: 54, Average loss: 3.557249395935624\n",
      "Epoch: 3786, Len of Training loss: 18, Average loss: 1.1453828778531816\n",
      "Len of Validation loss: 54, Average loss: 3.4756118147461503\n",
      "Epoch: 3787, Len of Training loss: 18, Average loss: 1.194065531094869\n",
      "Len of Validation loss: 54, Average loss: 3.5154220230049558\n",
      "Epoch: 3788, Len of Training loss: 18, Average loss: 1.076506644487381\n",
      "Len of Validation loss: 54, Average loss: 3.744252351699052\n",
      "Epoch: 3789, Len of Training loss: 18, Average loss: 1.047066652112537\n",
      "Len of Validation loss: 54, Average loss: 3.4544489714834423\n",
      "Epoch: 3790, Len of Training loss: 18, Average loss: 0.9600615898768107\n",
      "Len of Validation loss: 54, Average loss: 3.7403081280213817\n",
      "Epoch: 3791, Len of Training loss: 18, Average loss: 1.0040245552857716\n",
      "Len of Validation loss: 54, Average loss: 3.4860106894263514\n",
      "Epoch: 3792, Len of Training loss: 18, Average loss: 1.0375533998012543\n",
      "Len of Validation loss: 54, Average loss: 3.5164008372359805\n",
      "Epoch: 3793, Len of Training loss: 18, Average loss: 1.0720356570349798\n",
      "Len of Validation loss: 54, Average loss: 3.29285141715297\n",
      "Epoch: 3794, Len of Training loss: 18, Average loss: 0.9862018558714125\n",
      "Len of Validation loss: 54, Average loss: 3.538645984949889\n",
      "Epoch: 3795, Len of Training loss: 18, Average loss: 1.0639917651812236\n",
      "Len of Validation loss: 54, Average loss: 3.2895439443764865\n",
      "Epoch: 3796, Len of Training loss: 18, Average loss: 1.0884261826674144\n",
      "Len of Validation loss: 54, Average loss: 3.389461218206971\n",
      "Epoch: 3797, Len of Training loss: 18, Average loss: 1.0486434433195326\n",
      "Len of Validation loss: 54, Average loss: 3.379496337087066\n",
      "Epoch: 3798, Len of Training loss: 18, Average loss: 1.0427298148473103\n",
      "Len of Validation loss: 54, Average loss: 3.340504265493817\n",
      "Epoch: 3799, Len of Training loss: 18, Average loss: 1.2512843741310968\n",
      "Len of Validation loss: 54, Average loss: 3.751989680307883\n",
      "Epoch: 3800, Len of Training loss: 18, Average loss: 1.19816388686498\n",
      "Len of Validation loss: 54, Average loss: 3.5708328739360526\n",
      "Epoch: 3801, Len of Training loss: 18, Average loss: 1.1113072170151606\n",
      "Len of Validation loss: 54, Average loss: 3.5408951540788016\n",
      "Epoch: 3802, Len of Training loss: 18, Average loss: 1.056246002515157\n",
      "Len of Validation loss: 54, Average loss: 3.3901665287989156\n",
      "Epoch: 3803, Len of Training loss: 18, Average loss: 0.9718843996524811\n",
      "Len of Validation loss: 54, Average loss: 3.2779088539105876\n",
      "Epoch: 3804, Len of Training loss: 18, Average loss: 0.9890202416314019\n",
      "Len of Validation loss: 54, Average loss: 3.1816487897325447\n",
      "Epoch: 3805, Len of Training loss: 18, Average loss: 0.9963536196284823\n",
      "Len of Validation loss: 54, Average loss: 3.450427187813653\n",
      "Epoch: 3806, Len of Training loss: 18, Average loss: 1.0331774983141158\n",
      "Len of Validation loss: 54, Average loss: 3.7859194212489657\n",
      "Epoch: 3807, Len of Training loss: 18, Average loss: 1.0089016291830275\n",
      "Len of Validation loss: 54, Average loss: 3.489819996886783\n",
      "Epoch: 3808, Len of Training loss: 18, Average loss: 0.9501503904660543\n",
      "Len of Validation loss: 54, Average loss: 3.3878565265072718\n",
      "Epoch: 3809, Len of Training loss: 18, Average loss: 0.9567692014906142\n",
      "Len of Validation loss: 54, Average loss: 3.443712881317845\n",
      "Epoch: 3810, Len of Training loss: 18, Average loss: 1.0181548529201083\n",
      "Len of Validation loss: 54, Average loss: 3.5005309592794487\n",
      "Epoch: 3811, Len of Training loss: 18, Average loss: 1.1126444538434346\n",
      "Len of Validation loss: 54, Average loss: 3.627431145420781\n",
      "Epoch: 3812, Len of Training loss: 18, Average loss: 1.143401089641783\n",
      "Len of Validation loss: 54, Average loss: 3.5388973697468087\n",
      "Epoch: 3813, Len of Training loss: 18, Average loss: 1.0577175385422177\n",
      "Len of Validation loss: 54, Average loss: 3.433845874336031\n",
      "Epoch: 3814, Len of Training loss: 18, Average loss: 0.9778032104174296\n",
      "Len of Validation loss: 54, Average loss: 3.5793055218678935\n",
      "Epoch: 3815, Len of Training loss: 18, Average loss: 0.9574047293927934\n",
      "Len of Validation loss: 54, Average loss: 3.494135512246026\n",
      "Epoch: 3816, Len of Training loss: 18, Average loss: 0.978780253065957\n",
      "Len of Validation loss: 54, Average loss: 3.4636039866341486\n",
      "Epoch: 3817, Len of Training loss: 18, Average loss: 1.1214430729548137\n",
      "Len of Validation loss: 54, Average loss: 4.0313832384568675\n",
      "Epoch: 3818, Len of Training loss: 18, Average loss: 1.6644362343682184\n",
      "Len of Validation loss: 54, Average loss: 3.8333337395279496\n",
      "Epoch: 3819, Len of Training loss: 18, Average loss: 1.4175363050566778\n",
      "Len of Validation loss: 54, Average loss: 3.187683231300778\n",
      "Epoch: 3820, Len of Training loss: 18, Average loss: 1.5057051844067044\n",
      "Len of Validation loss: 54, Average loss: 3.4743260708120136\n",
      "Epoch: 3821, Len of Training loss: 18, Average loss: 1.2755770749515958\n",
      "Len of Validation loss: 54, Average loss: 3.405293878581789\n",
      "Epoch: 3822, Len of Training loss: 18, Average loss: 1.0614485177728865\n",
      "Len of Validation loss: 54, Average loss: 3.379327835860076\n",
      "Epoch: 3823, Len of Training loss: 18, Average loss: 1.0903887814945645\n",
      "Len of Validation loss: 54, Average loss: 3.390710771083832\n",
      "Epoch: 3824, Len of Training loss: 18, Average loss: 1.3015171156989203\n",
      "Len of Validation loss: 54, Average loss: 3.822546524030191\n",
      "Epoch: 3825, Len of Training loss: 18, Average loss: 1.4006798730956183\n",
      "Len of Validation loss: 54, Average loss: 3.5253180298540325\n",
      "Epoch: 3826, Len of Training loss: 18, Average loss: 1.1570304532845814\n",
      "Len of Validation loss: 54, Average loss: 3.5676321674276283\n",
      "Epoch: 3827, Len of Training loss: 18, Average loss: 1.035748518175549\n",
      "Len of Validation loss: 54, Average loss: 3.396711004001123\n",
      "Epoch: 3828, Len of Training loss: 18, Average loss: 1.093359003464381\n",
      "Len of Validation loss: 54, Average loss: 3.727821679026992\n",
      "Epoch: 3829, Len of Training loss: 18, Average loss: 1.238344543510013\n",
      "Len of Validation loss: 54, Average loss: 3.6417854891883\n",
      "Epoch: 3830, Len of Training loss: 18, Average loss: 1.365176306830512\n",
      "Len of Validation loss: 54, Average loss: 3.3602594942958266\n",
      "Epoch: 3831, Len of Training loss: 18, Average loss: 1.1215669049157038\n",
      "Len of Validation loss: 54, Average loss: 3.3622937025847257\n",
      "Epoch: 3832, Len of Training loss: 18, Average loss: 1.0611415969000921\n",
      "Len of Validation loss: 54, Average loss: 3.5460720779719175\n",
      "Epoch: 3833, Len of Training loss: 18, Average loss: 1.0040122734175787\n",
      "Len of Validation loss: 54, Average loss: 3.3800439050904028\n",
      "Epoch: 3834, Len of Training loss: 18, Average loss: 0.9857615133126577\n",
      "Len of Validation loss: 54, Average loss: 3.4447182924659163\n",
      "Epoch: 3835, Len of Training loss: 18, Average loss: 0.9603851934274038\n",
      "Len of Validation loss: 54, Average loss: 3.416383954109969\n",
      "Epoch: 3836, Len of Training loss: 18, Average loss: 0.9468755357795291\n",
      "Len of Validation loss: 54, Average loss: 3.317309276925193\n",
      "Epoch: 3837, Len of Training loss: 18, Average loss: 0.9402481151951684\n",
      "Len of Validation loss: 54, Average loss: 3.3332531761240074\n",
      "Epoch: 3838, Len of Training loss: 18, Average loss: 1.0535613662666745\n",
      "Len of Validation loss: 54, Average loss: 3.3759479776576713\n",
      "Epoch: 3839, Len of Training loss: 18, Average loss: 1.0262481371561687\n",
      "Len of Validation loss: 54, Average loss: 3.481085463806435\n",
      "Epoch: 3840, Len of Training loss: 18, Average loss: 1.0007953113979764\n",
      "Len of Validation loss: 54, Average loss: 3.4861676317674144\n",
      "Epoch: 3841, Len of Training loss: 18, Average loss: 1.0570748183462355\n",
      "Len of Validation loss: 54, Average loss: 3.3254720358936876\n",
      "Epoch: 3842, Len of Training loss: 18, Average loss: 0.9921366771062216\n",
      "Len of Validation loss: 54, Average loss: 3.3992783537617437\n",
      "Epoch: 3843, Len of Training loss: 18, Average loss: 0.9956347147623698\n",
      "Len of Validation loss: 54, Average loss: 3.454309562842051\n",
      "Epoch: 3844, Len of Training loss: 18, Average loss: 1.077103316783905\n",
      "Len of Validation loss: 54, Average loss: 3.419595409322668\n",
      "Epoch: 3845, Len of Training loss: 18, Average loss: 1.1447731852531433\n",
      "Len of Validation loss: 54, Average loss: 3.545417027340995\n",
      "Epoch: 3846, Len of Training loss: 18, Average loss: 1.1285791397094727\n",
      "Len of Validation loss: 54, Average loss: 3.504751980304718\n",
      "Epoch: 3847, Len of Training loss: 18, Average loss: 1.0567436085806952\n",
      "Len of Validation loss: 54, Average loss: 3.2620326666920274\n",
      "Epoch: 3848, Len of Training loss: 18, Average loss: 0.9822128646903567\n",
      "Len of Validation loss: 54, Average loss: 3.3529985039322465\n",
      "Epoch: 3849, Len of Training loss: 18, Average loss: 1.0050141778257158\n",
      "Len of Validation loss: 54, Average loss: 3.316399694592864\n",
      "Epoch: 3850, Len of Training loss: 18, Average loss: 1.0291136735015445\n",
      "Len of Validation loss: 54, Average loss: 3.304864494888871\n",
      "Epoch: 3851, Len of Training loss: 18, Average loss: 1.0350386632813349\n",
      "Len of Validation loss: 54, Average loss: 3.5246863906030304\n",
      "Epoch: 3852, Len of Training loss: 18, Average loss: 1.1033912367290921\n",
      "Len of Validation loss: 54, Average loss: 3.520398469986739\n",
      "Epoch: 3853, Len of Training loss: 18, Average loss: 1.1818803482585483\n",
      "Len of Validation loss: 54, Average loss: 3.441950769336135\n",
      "Epoch: 3854, Len of Training loss: 18, Average loss: 1.0427416463692982\n",
      "Len of Validation loss: 54, Average loss: 3.346865653991699\n",
      "Epoch: 3855, Len of Training loss: 18, Average loss: 1.0004812479019165\n",
      "Len of Validation loss: 54, Average loss: 3.471562388870451\n",
      "Epoch: 3856, Len of Training loss: 18, Average loss: 1.0016384853257074\n",
      "Len of Validation loss: 54, Average loss: 3.345001909467909\n",
      "Epoch: 3857, Len of Training loss: 18, Average loss: 1.0902061925994024\n",
      "Len of Validation loss: 54, Average loss: 3.359461107739696\n",
      "Epoch: 3858, Len of Training loss: 18, Average loss: 0.9942179951402876\n",
      "Len of Validation loss: 54, Average loss: 3.4457678971467196\n",
      "Epoch: 3859, Len of Training loss: 18, Average loss: 0.987035397026274\n",
      "Len of Validation loss: 54, Average loss: 3.3280493043087147\n",
      "Epoch: 3860, Len of Training loss: 18, Average loss: 0.987669888469908\n",
      "Len of Validation loss: 54, Average loss: 3.2399596439467535\n",
      "Epoch: 3861, Len of Training loss: 18, Average loss: 1.109721279806561\n",
      "Len of Validation loss: 54, Average loss: 3.518894332426566\n",
      "Epoch: 3862, Len of Training loss: 18, Average loss: 1.0861774616771274\n",
      "Len of Validation loss: 54, Average loss: 3.3865278451531022\n",
      "Epoch: 3863, Len of Training loss: 18, Average loss: 1.0045163962576125\n",
      "Len of Validation loss: 54, Average loss: 3.4616212712393866\n",
      "Epoch: 3864, Len of Training loss: 18, Average loss: 0.9746752182642618\n",
      "Len of Validation loss: 54, Average loss: 3.347559752287688\n",
      "Epoch: 3865, Len of Training loss: 18, Average loss: 0.9292603267563714\n",
      "Len of Validation loss: 54, Average loss: 3.4517509087368294\n",
      "Epoch: 3866, Len of Training loss: 18, Average loss: 1.0169407162401412\n",
      "Len of Validation loss: 54, Average loss: 3.6105227768421173\n",
      "Epoch: 3867, Len of Training loss: 18, Average loss: 1.0668927431106567\n",
      "Len of Validation loss: 54, Average loss: 3.2865830483259977\n",
      "Epoch: 3868, Len of Training loss: 18, Average loss: 1.0645782550175984\n",
      "Len of Validation loss: 54, Average loss: 3.474438996226699\n",
      "Epoch: 3869, Len of Training loss: 18, Average loss: 1.0381518105665843\n",
      "Len of Validation loss: 54, Average loss: 3.424534797668457\n",
      "Epoch: 3870, Len of Training loss: 18, Average loss: 0.9892023271984525\n",
      "Len of Validation loss: 54, Average loss: 3.577363661041966\n",
      "Epoch: 3871, Len of Training loss: 18, Average loss: 1.0519658260875278\n",
      "Len of Validation loss: 54, Average loss: 3.528274063710813\n",
      "Epoch: 3872, Len of Training loss: 18, Average loss: 1.0299936566087935\n",
      "Len of Validation loss: 54, Average loss: 3.302980308179502\n",
      "Epoch: 3873, Len of Training loss: 18, Average loss: 0.969203081395891\n",
      "Len of Validation loss: 54, Average loss: 3.4191007879045276\n",
      "Epoch: 3874, Len of Training loss: 18, Average loss: 0.9651731451352438\n",
      "Len of Validation loss: 54, Average loss: 3.6472324629624686\n",
      "Epoch: 3875, Len of Training loss: 18, Average loss: 1.004241708252165\n",
      "Len of Validation loss: 54, Average loss: 3.345124793273431\n",
      "Epoch: 3876, Len of Training loss: 18, Average loss: 1.0245787567562528\n",
      "Len of Validation loss: 54, Average loss: 3.688349863997212\n",
      "Epoch: 3877, Len of Training loss: 18, Average loss: 0.9896981649928622\n",
      "Len of Validation loss: 54, Average loss: 3.321174788254279\n",
      "Epoch: 3878, Len of Training loss: 18, Average loss: 0.9383479621675279\n",
      "Len of Validation loss: 54, Average loss: 3.416597325492788\n",
      "Epoch: 3879, Len of Training loss: 18, Average loss: 0.9620342585775588\n",
      "Len of Validation loss: 54, Average loss: 3.5071916017267437\n",
      "Epoch: 3880, Len of Training loss: 18, Average loss: 0.9546392957369486\n",
      "Len of Validation loss: 54, Average loss: 3.5201987900115825\n",
      "Epoch: 3881, Len of Training loss: 18, Average loss: 0.9754084216223823\n",
      "Len of Validation loss: 54, Average loss: 3.273082384356746\n",
      "Epoch: 3882, Len of Training loss: 18, Average loss: 0.968304865890079\n",
      "Len of Validation loss: 54, Average loss: 3.444752225169429\n",
      "Epoch: 3883, Len of Training loss: 18, Average loss: 1.3789411452081468\n",
      "Len of Validation loss: 54, Average loss: 3.6588364088976824\n",
      "Epoch: 3884, Len of Training loss: 18, Average loss: 1.2436448203192816\n",
      "Len of Validation loss: 54, Average loss: 3.3424557171486042\n",
      "Epoch: 3885, Len of Training loss: 18, Average loss: 1.119545277622011\n",
      "Len of Validation loss: 54, Average loss: 3.504074501770514\n",
      "Epoch: 3886, Len of Training loss: 18, Average loss: 1.1804080340597365\n",
      "Len of Validation loss: 54, Average loss: 3.4850997505364596\n",
      "Epoch: 3887, Len of Training loss: 18, Average loss: 1.0927192999256983\n",
      "Len of Validation loss: 54, Average loss: 3.6022329970642373\n",
      "Epoch: 3888, Len of Training loss: 18, Average loss: 1.094883571068446\n",
      "Len of Validation loss: 54, Average loss: 3.458928093866066\n",
      "Epoch: 3889, Len of Training loss: 18, Average loss: 1.0390920142332714\n",
      "Len of Validation loss: 54, Average loss: 3.4006911151938968\n",
      "Epoch: 3890, Len of Training loss: 18, Average loss: 1.0385533803039126\n",
      "Len of Validation loss: 54, Average loss: 3.4700009480670646\n",
      "Epoch: 3891, Len of Training loss: 18, Average loss: 1.084899776511722\n",
      "Len of Validation loss: 54, Average loss: 3.5056625571515827\n",
      "Epoch: 3892, Len of Training loss: 18, Average loss: 1.0933609472380743\n",
      "Len of Validation loss: 54, Average loss: 3.54488264741721\n",
      "Epoch: 3893, Len of Training loss: 18, Average loss: 1.0315734644730885\n",
      "Len of Validation loss: 54, Average loss: 3.525977768279888\n",
      "Epoch: 3894, Len of Training loss: 18, Average loss: 1.373024241791831\n",
      "Len of Validation loss: 54, Average loss: 3.892587650705267\n",
      "Epoch: 3895, Len of Training loss: 18, Average loss: 1.7168755994902716\n",
      "Len of Validation loss: 54, Average loss: 3.7018078477294356\n",
      "Epoch: 3896, Len of Training loss: 18, Average loss: 1.405513670709398\n",
      "Len of Validation loss: 54, Average loss: 3.6023813751008777\n",
      "Epoch: 3897, Len of Training loss: 18, Average loss: 1.2906617654694452\n",
      "Len of Validation loss: 54, Average loss: 3.8368036868395627\n",
      "Epoch: 3898, Len of Training loss: 18, Average loss: 1.0903956360287137\n",
      "Len of Validation loss: 54, Average loss: 3.415151717486205\n",
      "Epoch: 3899, Len of Training loss: 18, Average loss: 1.0542280309730105\n",
      "Len of Validation loss: 54, Average loss: 3.5973755993224956\n",
      "Epoch: 3900, Len of Training loss: 18, Average loss: 1.1981235245863597\n",
      "Len of Validation loss: 54, Average loss: 3.6027019917964935\n",
      "Epoch: 3901, Len of Training loss: 18, Average loss: 1.1064472960101233\n",
      "Len of Validation loss: 54, Average loss: 3.7372922941490456\n",
      "Epoch: 3902, Len of Training loss: 18, Average loss: 1.102898551358117\n",
      "Len of Validation loss: 54, Average loss: 3.4778960943222046\n",
      "Epoch: 3903, Len of Training loss: 18, Average loss: 1.148472272687488\n",
      "Len of Validation loss: 54, Average loss: 3.358434475130505\n",
      "Epoch: 3904, Len of Training loss: 18, Average loss: 1.0945845478110843\n",
      "Len of Validation loss: 54, Average loss: 3.4853839995684446\n",
      "Epoch: 3905, Len of Training loss: 18, Average loss: 1.1590356330076854\n",
      "Len of Validation loss: 54, Average loss: 3.554708350587774\n",
      "Epoch: 3906, Len of Training loss: 18, Average loss: 1.071038285891215\n",
      "Len of Validation loss: 54, Average loss: 3.5961141917440624\n",
      "Epoch: 3907, Len of Training loss: 18, Average loss: 1.0620254808002048\n",
      "Len of Validation loss: 54, Average loss: 3.491487056016922\n",
      "Epoch: 3908, Len of Training loss: 18, Average loss: 1.4552190038892958\n",
      "Len of Validation loss: 54, Average loss: 3.689678931677783\n",
      "Epoch: 3909, Len of Training loss: 18, Average loss: 1.2463348375426397\n",
      "Len of Validation loss: 54, Average loss: 3.5608760118484497\n",
      "Epoch: 3910, Len of Training loss: 18, Average loss: 1.3342504103978474\n",
      "Len of Validation loss: 54, Average loss: 3.710763049346429\n",
      "Epoch: 3911, Len of Training loss: 18, Average loss: 1.2235542866918776\n",
      "Len of Validation loss: 54, Average loss: 3.6872523526350656\n",
      "Epoch: 3912, Len of Training loss: 18, Average loss: 1.1421937147776287\n",
      "Len of Validation loss: 54, Average loss: 3.6491352330755302\n",
      "Epoch: 3913, Len of Training loss: 18, Average loss: 1.1326928701665666\n",
      "Len of Validation loss: 54, Average loss: 3.4754497949723846\n",
      "Epoch: 3914, Len of Training loss: 18, Average loss: 1.1460025442971125\n",
      "Len of Validation loss: 54, Average loss: 3.4256279446460582\n",
      "Epoch: 3915, Len of Training loss: 18, Average loss: 1.0761098232534196\n",
      "Len of Validation loss: 54, Average loss: 3.5237918385752924\n",
      "Epoch: 3916, Len of Training loss: 18, Average loss: 1.048980047305425\n",
      "Len of Validation loss: 54, Average loss: 3.3407412182401726\n",
      "Epoch: 3917, Len of Training loss: 18, Average loss: 1.1375492413838704\n",
      "Len of Validation loss: 54, Average loss: 3.4637597545429513\n",
      "Epoch: 3918, Len of Training loss: 18, Average loss: 1.143314864900377\n",
      "Len of Validation loss: 54, Average loss: 3.324026910243211\n",
      "Epoch: 3919, Len of Training loss: 18, Average loss: 1.058750718832016\n",
      "Len of Validation loss: 54, Average loss: 3.475798220546157\n",
      "Epoch: 3920, Len of Training loss: 18, Average loss: 1.0300273497899373\n",
      "Len of Validation loss: 54, Average loss: 3.3626026102790125\n",
      "Epoch: 3921, Len of Training loss: 18, Average loss: 0.959266205628713\n",
      "Len of Validation loss: 54, Average loss: 3.507397100881294\n",
      "Epoch: 3922, Len of Training loss: 18, Average loss: 1.0054637392361958\n",
      "Len of Validation loss: 54, Average loss: 3.4937077407483703\n",
      "Epoch: 3923, Len of Training loss: 18, Average loss: 0.9995653331279755\n",
      "Len of Validation loss: 54, Average loss: 3.3495062865592815\n",
      "Epoch: 3924, Len of Training loss: 18, Average loss: 0.9149568908744388\n",
      "Len of Validation loss: 54, Average loss: 3.440623448954688\n",
      "Epoch: 3925, Len of Training loss: 18, Average loss: 0.8979960083961487\n",
      "Len of Validation loss: 54, Average loss: 3.2332824534840054\n",
      "Epoch: 3926, Len of Training loss: 18, Average loss: 0.8923772209220462\n",
      "Len of Validation loss: 54, Average loss: 3.437816031553127\n",
      "Epoch: 3927, Len of Training loss: 18, Average loss: 0.9333638913101621\n",
      "Len of Validation loss: 54, Average loss: 3.5522679443712586\n",
      "Epoch: 3928, Len of Training loss: 18, Average loss: 0.9315686457686954\n",
      "Len of Validation loss: 54, Average loss: 3.3449317857071206\n",
      "Epoch: 3929, Len of Training loss: 18, Average loss: 0.9791577723291185\n",
      "Len of Validation loss: 54, Average loss: 3.3908688481207245\n",
      "Epoch: 3930, Len of Training loss: 18, Average loss: 0.9667940802044339\n",
      "Len of Validation loss: 54, Average loss: 3.4145608411894903\n",
      "Epoch: 3931, Len of Training loss: 18, Average loss: 1.1177011132240295\n",
      "Len of Validation loss: 54, Average loss: 3.6487958519547075\n",
      "Epoch: 3932, Len of Training loss: 18, Average loss: 1.2295605275366042\n",
      "Len of Validation loss: 54, Average loss: 3.704800727190795\n",
      "Epoch: 3933, Len of Training loss: 18, Average loss: 1.1156664258903928\n",
      "Len of Validation loss: 54, Average loss: 3.4375882336386927\n",
      "Epoch: 3934, Len of Training loss: 18, Average loss: 1.0110650128788419\n",
      "Len of Validation loss: 54, Average loss: 3.54817059746495\n",
      "Epoch: 3935, Len of Training loss: 18, Average loss: 1.0250131024254694\n",
      "Len of Validation loss: 54, Average loss: 3.4817074482087738\n",
      "Epoch: 3936, Len of Training loss: 18, Average loss: 1.0911604993873172\n",
      "Len of Validation loss: 54, Average loss: 3.563134543321751\n",
      "Epoch: 3937, Len of Training loss: 18, Average loss: 1.083004030916426\n",
      "Len of Validation loss: 54, Average loss: 3.499515774073424\n",
      "Epoch: 3938, Len of Training loss: 18, Average loss: 0.9940560393863254\n",
      "Len of Validation loss: 54, Average loss: 3.243854205917429\n",
      "Epoch: 3939, Len of Training loss: 18, Average loss: 0.970084547996521\n",
      "Len of Validation loss: 54, Average loss: 3.5452688689585083\n",
      "Epoch: 3940, Len of Training loss: 18, Average loss: 0.9672262933519151\n",
      "Len of Validation loss: 54, Average loss: 3.469666025152913\n",
      "Epoch: 3941, Len of Training loss: 18, Average loss: 1.024765716658698\n",
      "Len of Validation loss: 54, Average loss: 3.4857009572011455\n",
      "Epoch: 3942, Len of Training loss: 18, Average loss: 1.0303510162565443\n",
      "Len of Validation loss: 54, Average loss: 3.3936289372267545\n",
      "Epoch: 3943, Len of Training loss: 18, Average loss: 1.1424603925810919\n",
      "Len of Validation loss: 54, Average loss: 3.423490207504343\n",
      "Epoch: 3944, Len of Training loss: 18, Average loss: 1.0666960345374212\n",
      "Len of Validation loss: 54, Average loss: 3.383910959517514\n",
      "Epoch: 3945, Len of Training loss: 18, Average loss: 1.0709869696034326\n",
      "Len of Validation loss: 54, Average loss: 3.403460372377325\n",
      "Epoch: 3946, Len of Training loss: 18, Average loss: 1.2312305569648743\n",
      "Len of Validation loss: 54, Average loss: 3.4575447616753756\n",
      "Epoch: 3947, Len of Training loss: 18, Average loss: 1.2752741045422025\n",
      "Len of Validation loss: 54, Average loss: 3.538699334418332\n",
      "Epoch: 3948, Len of Training loss: 18, Average loss: 1.1080429123507605\n",
      "Len of Validation loss: 54, Average loss: 3.516057499029018\n",
      "Epoch: 3949, Len of Training loss: 18, Average loss: 1.0056449605358972\n",
      "Len of Validation loss: 54, Average loss: 3.4085471530755362\n",
      "Epoch: 3950, Len of Training loss: 18, Average loss: 0.9829949571026696\n",
      "Len of Validation loss: 54, Average loss: 3.354454895964375\n",
      "Epoch: 3951, Len of Training loss: 18, Average loss: 1.078585512108273\n",
      "Len of Validation loss: 54, Average loss: 3.7166437396296748\n",
      "Epoch: 3952, Len of Training loss: 18, Average loss: 1.1475321451822917\n",
      "Len of Validation loss: 54, Average loss: 3.5904002255863614\n",
      "Epoch: 3953, Len of Training loss: 18, Average loss: 1.0679122176435258\n",
      "Len of Validation loss: 54, Average loss: 3.395072518675416\n",
      "Epoch: 3954, Len of Training loss: 18, Average loss: 1.0101724233892229\n",
      "Len of Validation loss: 54, Average loss: 3.4994093543953366\n",
      "Epoch: 3955, Len of Training loss: 18, Average loss: 1.0451226499345567\n",
      "Len of Validation loss: 54, Average loss: 3.495163810473901\n",
      "Epoch: 3956, Len of Training loss: 18, Average loss: 1.1281992064581976\n",
      "Len of Validation loss: 54, Average loss: 3.4773306956997625\n",
      "Epoch: 3957, Len of Training loss: 18, Average loss: 1.0392978489398956\n",
      "Len of Validation loss: 54, Average loss: 3.719836086034775\n",
      "Epoch: 3958, Len of Training loss: 18, Average loss: 1.1222862369484372\n",
      "Len of Validation loss: 54, Average loss: 3.462182061539756\n",
      "Epoch: 3959, Len of Training loss: 18, Average loss: 1.2569952938291762\n",
      "Len of Validation loss: 54, Average loss: 3.6196011349006936\n",
      "Epoch: 3960, Len of Training loss: 18, Average loss: 1.1254329548941717\n",
      "Len of Validation loss: 54, Average loss: 3.326303626652117\n",
      "Epoch: 3961, Len of Training loss: 18, Average loss: 1.0266792211267683\n",
      "Len of Validation loss: 54, Average loss: 3.7184139181066445\n",
      "Epoch: 3962, Len of Training loss: 18, Average loss: 0.9606224960751004\n",
      "Len of Validation loss: 54, Average loss: 3.4860068140206515\n",
      "Epoch: 3963, Len of Training loss: 18, Average loss: 1.0301422940360174\n",
      "Len of Validation loss: 54, Average loss: 3.3875577129699566\n",
      "Epoch: 3964, Len of Training loss: 18, Average loss: 1.3842067685392168\n",
      "Len of Validation loss: 54, Average loss: 4.522085205272392\n",
      "Epoch: 3965, Len of Training loss: 18, Average loss: 1.7093175450960796\n",
      "Len of Validation loss: 54, Average loss: 3.7165020649079925\n",
      "Epoch: 3966, Len of Training loss: 18, Average loss: 1.255051228735182\n",
      "Len of Validation loss: 54, Average loss: 3.559466786958553\n",
      "Epoch: 3967, Len of Training loss: 18, Average loss: 1.169987267918057\n",
      "Len of Validation loss: 54, Average loss: 3.589583456516266\n",
      "Epoch: 3968, Len of Training loss: 18, Average loss: 1.1169043944941626\n",
      "Len of Validation loss: 54, Average loss: 3.3911017367133387\n",
      "Epoch: 3969, Len of Training loss: 18, Average loss: 1.0388200084368389\n",
      "Len of Validation loss: 54, Average loss: 3.5001360255259053\n",
      "Epoch: 3970, Len of Training loss: 18, Average loss: 0.9811084469159445\n",
      "Len of Validation loss: 54, Average loss: 3.4856066196053117\n",
      "Epoch: 3971, Len of Training loss: 18, Average loss: 1.0951539145575628\n",
      "Len of Validation loss: 54, Average loss: 3.501175574682377\n",
      "Epoch: 3972, Len of Training loss: 18, Average loss: 1.335459013779958\n",
      "Len of Validation loss: 54, Average loss: 3.7581332094139523\n",
      "Epoch: 3973, Len of Training loss: 18, Average loss: 1.1333567831251357\n",
      "Len of Validation loss: 54, Average loss: 3.6289661018936723\n",
      "Epoch: 3974, Len of Training loss: 18, Average loss: 1.0696743428707123\n",
      "Len of Validation loss: 54, Average loss: 3.4368136249206684\n",
      "Epoch: 3975, Len of Training loss: 18, Average loss: 0.9751148091422187\n",
      "Len of Validation loss: 54, Average loss: 3.3452745179335275\n",
      "Epoch: 3976, Len of Training loss: 18, Average loss: 1.024899936384625\n",
      "Len of Validation loss: 54, Average loss: 3.4833812514940896\n",
      "Epoch: 3977, Len of Training loss: 18, Average loss: 0.9466633233759139\n",
      "Len of Validation loss: 54, Average loss: 3.5357298332232014\n",
      "Epoch: 3978, Len of Training loss: 18, Average loss: 0.9696606894334158\n",
      "Len of Validation loss: 54, Average loss: 3.4542286131117077\n",
      "Epoch: 3979, Len of Training loss: 18, Average loss: 0.9836496346526675\n",
      "Len of Validation loss: 54, Average loss: 3.436883056605304\n",
      "Epoch: 3980, Len of Training loss: 18, Average loss: 1.0944907367229462\n",
      "Len of Validation loss: 54, Average loss: 3.6167768604225583\n",
      "Epoch: 3981, Len of Training loss: 18, Average loss: 1.2254977126916249\n",
      "Len of Validation loss: 54, Average loss: 3.872920494388651\n",
      "Epoch: 3982, Len of Training loss: 18, Average loss: 1.2403036157290142\n",
      "Len of Validation loss: 54, Average loss: 3.6548273916597718\n",
      "Epoch: 3983, Len of Training loss: 18, Average loss: 1.0819804966449738\n",
      "Len of Validation loss: 54, Average loss: 3.547393873885826\n",
      "Epoch: 3984, Len of Training loss: 18, Average loss: 1.0217981702751584\n",
      "Len of Validation loss: 54, Average loss: 3.370848118155091\n",
      "Epoch: 3985, Len of Training loss: 18, Average loss: 0.9555238518449995\n",
      "Len of Validation loss: 54, Average loss: 3.5106056608535625\n",
      "Epoch: 3986, Len of Training loss: 18, Average loss: 0.9415967928038703\n",
      "Len of Validation loss: 54, Average loss: 3.326260111950062\n",
      "Epoch: 3987, Len of Training loss: 18, Average loss: 0.966994023985333\n",
      "Len of Validation loss: 54, Average loss: 3.810374751135155\n",
      "Epoch: 3988, Len of Training loss: 18, Average loss: 1.0859936773777008\n",
      "Len of Validation loss: 54, Average loss: 3.346514231628842\n",
      "Epoch: 3989, Len of Training loss: 18, Average loss: 1.0298308663898044\n",
      "Len of Validation loss: 54, Average loss: 3.3333596774825343\n",
      "Epoch: 3990, Len of Training loss: 18, Average loss: 1.0159243179692163\n",
      "Len of Validation loss: 54, Average loss: 3.419966767231623\n",
      "Epoch: 3991, Len of Training loss: 18, Average loss: 1.020609090725581\n",
      "Len of Validation loss: 54, Average loss: 3.4953632145016282\n",
      "Epoch: 3992, Len of Training loss: 18, Average loss: 0.909063071012497\n",
      "Len of Validation loss: 54, Average loss: 3.3709839317533703\n",
      "Epoch: 3993, Len of Training loss: 18, Average loss: 0.9389436708556281\n",
      "Len of Validation loss: 54, Average loss: 3.5774644215901694\n",
      "Epoch: 3994, Len of Training loss: 18, Average loss: 1.0723479754394956\n",
      "Len of Validation loss: 54, Average loss: 3.7571763539755785\n",
      "Epoch: 3995, Len of Training loss: 18, Average loss: 1.2673491769366794\n",
      "Len of Validation loss: 54, Average loss: 3.303539020043832\n",
      "Epoch: 3996, Len of Training loss: 18, Average loss: 1.0692866146564484\n",
      "Len of Validation loss: 54, Average loss: 3.4694608129836895\n",
      "Epoch: 3997, Len of Training loss: 18, Average loss: 1.0128687189684973\n",
      "Len of Validation loss: 54, Average loss: 3.43872904115253\n",
      "Epoch: 3998, Len of Training loss: 18, Average loss: 1.0507174796528287\n",
      "Len of Validation loss: 54, Average loss: 3.4600587244387024\n",
      "Epoch: 3999, Len of Training loss: 18, Average loss: 1.019256095091502\n",
      "Len of Validation loss: 54, Average loss: 3.307572019320947\n",
      "Epoch: 4000, Len of Training loss: 18, Average loss: 0.9766597118642595\n",
      "Len of Validation loss: 54, Average loss: 3.3853734566105738\n",
      "Epoch: 4001, Len of Training loss: 18, Average loss: 0.993391795290841\n",
      "Len of Validation loss: 54, Average loss: 3.461537759613108\n",
      "Epoch: 4002, Len of Training loss: 18, Average loss: 0.9838755064540439\n",
      "Len of Validation loss: 54, Average loss: 3.3662971280239247\n",
      "Epoch: 4003, Len of Training loss: 18, Average loss: 0.9680081274774339\n",
      "Len of Validation loss: 54, Average loss: 3.398846444156435\n",
      "Epoch: 4004, Len of Training loss: 18, Average loss: 1.0081039004855685\n",
      "Len of Validation loss: 54, Average loss: 3.508880012565189\n",
      "Epoch: 4005, Len of Training loss: 18, Average loss: 1.1747467550966475\n",
      "Len of Validation loss: 54, Average loss: 3.6600456127413996\n",
      "Epoch: 4006, Len of Training loss: 18, Average loss: 1.1652097205320995\n",
      "Len of Validation loss: 54, Average loss: 3.4828601799629353\n",
      "Epoch: 4007, Len of Training loss: 18, Average loss: 1.0284255709913042\n",
      "Len of Validation loss: 54, Average loss: 3.387802899987609\n",
      "Epoch: 4008, Len of Training loss: 18, Average loss: 0.9529173605971866\n",
      "Len of Validation loss: 54, Average loss: 3.375942087835736\n",
      "Epoch: 4009, Len of Training loss: 18, Average loss: 1.0184504157967038\n",
      "Len of Validation loss: 54, Average loss: 3.4907124285344726\n",
      "Epoch: 4010, Len of Training loss: 18, Average loss: 0.9813385605812073\n",
      "Len of Validation loss: 54, Average loss: 3.38061112386209\n",
      "Epoch: 4011, Len of Training loss: 18, Average loss: 0.9734664327568479\n",
      "Len of Validation loss: 54, Average loss: 3.4558478417219938\n",
      "Epoch: 4012, Len of Training loss: 18, Average loss: 1.0653270052538977\n",
      "Len of Validation loss: 54, Average loss: 3.5094696250226765\n",
      "Epoch: 4013, Len of Training loss: 18, Average loss: 1.074436022175683\n",
      "Len of Validation loss: 54, Average loss: 3.4295831565503723\n",
      "Epoch: 4014, Len of Training loss: 18, Average loss: 0.9866744346088834\n",
      "Len of Validation loss: 54, Average loss: 3.3239530821641288\n",
      "Epoch: 4015, Len of Training loss: 18, Average loss: 0.9435728556580014\n",
      "Len of Validation loss: 54, Average loss: 3.3748989314944655\n",
      "Epoch: 4016, Len of Training loss: 18, Average loss: 0.9923422071668837\n",
      "Len of Validation loss: 54, Average loss: 3.571049550065288\n",
      "Epoch: 4017, Len of Training loss: 18, Average loss: 0.9671469761265649\n",
      "Len of Validation loss: 54, Average loss: 3.4094885576654366\n",
      "Epoch: 4018, Len of Training loss: 18, Average loss: 0.9826638764805264\n",
      "Len of Validation loss: 54, Average loss: 3.388504934531671\n",
      "Epoch: 4019, Len of Training loss: 18, Average loss: 0.9490927888287438\n",
      "Len of Validation loss: 54, Average loss: 3.354818195104599\n",
      "Epoch: 4020, Len of Training loss: 18, Average loss: 0.9088078108098772\n",
      "Len of Validation loss: 54, Average loss: 3.4478334221574993\n",
      "Epoch: 4021, Len of Training loss: 18, Average loss: 0.9529464973343743\n",
      "Len of Validation loss: 54, Average loss: 3.4886629195125014\n",
      "Epoch: 4022, Len of Training loss: 18, Average loss: 0.9482345084349314\n",
      "Len of Validation loss: 54, Average loss: 3.6052249438232846\n",
      "Epoch: 4023, Len of Training loss: 18, Average loss: 0.9712932507197062\n",
      "Len of Validation loss: 54, Average loss: 3.424502009594882\n",
      "Epoch: 4024, Len of Training loss: 18, Average loss: 0.9946354528268179\n",
      "Len of Validation loss: 54, Average loss: 3.4771373592041157\n",
      "Epoch: 4025, Len of Training loss: 18, Average loss: 0.9822073611948225\n",
      "Len of Validation loss: 54, Average loss: 3.4405588606993356\n",
      "Epoch: 4026, Len of Training loss: 18, Average loss: 0.937080916431215\n",
      "Len of Validation loss: 54, Average loss: 3.483672777811686\n",
      "Epoch: 4027, Len of Training loss: 18, Average loss: 1.1318272683355544\n",
      "Len of Validation loss: 54, Average loss: 3.624503586027357\n",
      "Epoch: 4028, Len of Training loss: 18, Average loss: 1.0820920368035634\n",
      "Len of Validation loss: 54, Average loss: 3.38940711595394\n",
      "Epoch: 4029, Len of Training loss: 18, Average loss: 1.0900168087747362\n",
      "Len of Validation loss: 54, Average loss: 3.4820264721358263\n",
      "Epoch: 4030, Len of Training loss: 18, Average loss: 1.2186943623754714\n",
      "Len of Validation loss: 54, Average loss: 3.504468580087026\n",
      "Epoch: 4031, Len of Training loss: 18, Average loss: 1.0830348233381908\n",
      "Len of Validation loss: 54, Average loss: 3.349266169247804\n",
      "Epoch: 4032, Len of Training loss: 18, Average loss: 0.9581477642059326\n",
      "Len of Validation loss: 54, Average loss: 3.499876615073946\n",
      "Epoch: 4033, Len of Training loss: 18, Average loss: 0.9161438842614492\n",
      "Len of Validation loss: 54, Average loss: 3.4174490493756755\n",
      "Epoch: 4034, Len of Training loss: 18, Average loss: 1.025077634387546\n",
      "Len of Validation loss: 54, Average loss: 3.5157366549527205\n",
      "Epoch: 4035, Len of Training loss: 18, Average loss: 0.9953401452965207\n",
      "Len of Validation loss: 54, Average loss: 3.7979962671244585\n",
      "Epoch: 4036, Len of Training loss: 18, Average loss: 0.9935697184668647\n",
      "Len of Validation loss: 54, Average loss: 3.3516901742528984\n",
      "Epoch: 4037, Len of Training loss: 18, Average loss: 0.9784342646598816\n",
      "Len of Validation loss: 54, Average loss: 3.5943748432177083\n",
      "Epoch: 4038, Len of Training loss: 18, Average loss: 0.9971266488234202\n",
      "Len of Validation loss: 54, Average loss: 3.524178880232352\n",
      "Epoch: 4039, Len of Training loss: 18, Average loss: 1.1213284697797563\n",
      "Len of Validation loss: 54, Average loss: 3.393345591094759\n",
      "Epoch: 4040, Len of Training loss: 18, Average loss: 1.0824745264318254\n",
      "Len of Validation loss: 54, Average loss: 3.323901617968524\n",
      "Epoch: 4041, Len of Training loss: 18, Average loss: 0.9521363013320499\n",
      "Len of Validation loss: 54, Average loss: 3.4323257318249456\n",
      "Epoch: 4042, Len of Training loss: 18, Average loss: 1.0167941749095917\n",
      "Len of Validation loss: 54, Average loss: 3.6332302446718567\n",
      "Epoch: 4043, Len of Training loss: 18, Average loss: 1.1274777981970046\n",
      "Len of Validation loss: 54, Average loss: 3.636286050081253\n",
      "Epoch: 4044, Len of Training loss: 18, Average loss: 1.0909383760558233\n",
      "Len of Validation loss: 54, Average loss: 3.69880653752221\n",
      "Epoch: 4045, Len of Training loss: 18, Average loss: 1.0552706254853144\n",
      "Len of Validation loss: 54, Average loss: 3.492043524980545\n",
      "Epoch: 4046, Len of Training loss: 18, Average loss: 1.0141009522808924\n",
      "Len of Validation loss: 54, Average loss: 3.3519410678633936\n",
      "Epoch: 4047, Len of Training loss: 18, Average loss: 0.9756521317693923\n",
      "Len of Validation loss: 54, Average loss: 3.3106178221879183\n",
      "Epoch: 4048, Len of Training loss: 18, Average loss: 0.9878536032305824\n",
      "Len of Validation loss: 54, Average loss: 3.497665140363905\n",
      "Epoch: 4049, Len of Training loss: 18, Average loss: 1.05407828423712\n",
      "Len of Validation loss: 54, Average loss: 3.79390956847756\n",
      "Epoch: 4050, Len of Training loss: 18, Average loss: 1.3055627279811435\n",
      "Len of Validation loss: 54, Average loss: 3.7092805085358798\n",
      "Epoch: 4051, Len of Training loss: 18, Average loss: 1.084535300731659\n",
      "Len of Validation loss: 54, Average loss: 3.3928912348217435\n",
      "Epoch: 4052, Len of Training loss: 18, Average loss: 1.020122508207957\n",
      "Len of Validation loss: 54, Average loss: 3.2925542846873954\n",
      "Epoch: 4053, Len of Training loss: 18, Average loss: 1.0104544758796692\n",
      "Len of Validation loss: 54, Average loss: 3.4623375369442835\n",
      "Epoch: 4054, Len of Training loss: 18, Average loss: 0.9973773426479764\n",
      "Len of Validation loss: 54, Average loss: 3.3670003469343537\n",
      "Epoch: 4055, Len of Training loss: 18, Average loss: 0.9598830143610636\n",
      "Len of Validation loss: 54, Average loss: 3.514179038780707\n",
      "Epoch: 4056, Len of Training loss: 18, Average loss: 1.0071906083159976\n",
      "Len of Validation loss: 54, Average loss: 3.4131078962926513\n",
      "Epoch: 4057, Len of Training loss: 18, Average loss: 0.9712855749660068\n",
      "Len of Validation loss: 54, Average loss: 3.2713538849795305\n",
      "Epoch: 4058, Len of Training loss: 18, Average loss: 1.0318942831622229\n",
      "Len of Validation loss: 54, Average loss: 3.3456205836048833\n",
      "Epoch: 4059, Len of Training loss: 18, Average loss: 0.9970537887679206\n",
      "Len of Validation loss: 54, Average loss: 3.5176733643920333\n",
      "Epoch: 4060, Len of Training loss: 18, Average loss: 1.2761594255765278\n",
      "Len of Validation loss: 54, Average loss: 3.539199279414283\n",
      "Epoch: 4061, Len of Training loss: 18, Average loss: 1.077522532807456\n",
      "Len of Validation loss: 54, Average loss: 3.4635662270916834\n",
      "Epoch: 4062, Len of Training loss: 18, Average loss: 0.9956408275498284\n",
      "Len of Validation loss: 54, Average loss: 3.2841782945173756\n",
      "Epoch: 4063, Len of Training loss: 18, Average loss: 1.0675306187735663\n",
      "Len of Validation loss: 54, Average loss: 3.4431626774646618\n",
      "Epoch: 4064, Len of Training loss: 18, Average loss: 1.058904293510649\n",
      "Len of Validation loss: 54, Average loss: 3.568086807374601\n",
      "Epoch: 4065, Len of Training loss: 18, Average loss: 0.9877064459853702\n",
      "Len of Validation loss: 54, Average loss: 3.423370390026658\n",
      "Epoch: 4066, Len of Training loss: 18, Average loss: 1.023212217622333\n",
      "Len of Validation loss: 54, Average loss: 3.644022168936553\n",
      "Epoch: 4067, Len of Training loss: 18, Average loss: 0.9584820734130012\n",
      "Len of Validation loss: 54, Average loss: 3.36811790422157\n",
      "Epoch: 4068, Len of Training loss: 18, Average loss: 0.9007322225305769\n",
      "Len of Validation loss: 54, Average loss: 3.4951434621104487\n",
      "Epoch: 4069, Len of Training loss: 18, Average loss: 0.8633147180080414\n",
      "Len of Validation loss: 54, Average loss: 3.472088827027215\n",
      "Epoch: 4070, Len of Training loss: 18, Average loss: 0.9517594443427192\n",
      "Len of Validation loss: 54, Average loss: 3.4886280209929854\n",
      "Epoch: 4071, Len of Training loss: 18, Average loss: 1.045951783657074\n",
      "Len of Validation loss: 54, Average loss: 3.3985556684158467\n",
      "Epoch: 4072, Len of Training loss: 18, Average loss: 1.0234570072756872\n",
      "Len of Validation loss: 54, Average loss: 3.5015421355212175\n",
      "Epoch: 4073, Len of Training loss: 18, Average loss: 0.9893724554114871\n",
      "Len of Validation loss: 54, Average loss: 3.4006554859655873\n",
      "Epoch: 4074, Len of Training loss: 18, Average loss: 1.0510550737380981\n",
      "Len of Validation loss: 54, Average loss: 3.487897585939478\n",
      "Epoch: 4075, Len of Training loss: 18, Average loss: 1.0584682557317946\n",
      "Len of Validation loss: 54, Average loss: 3.570500975405728\n",
      "Epoch: 4076, Len of Training loss: 18, Average loss: 0.9982554150952233\n",
      "Len of Validation loss: 54, Average loss: 3.422432971221429\n",
      "Epoch: 4077, Len of Training loss: 18, Average loss: 1.0093891786204443\n",
      "Len of Validation loss: 54, Average loss: 3.3200390990133637\n",
      "Epoch: 4078, Len of Training loss: 18, Average loss: 1.0238456494278378\n",
      "Len of Validation loss: 54, Average loss: 3.403050121333864\n",
      "Epoch: 4079, Len of Training loss: 18, Average loss: 1.0072966582245297\n",
      "Len of Validation loss: 54, Average loss: 3.3137047125233545\n",
      "Epoch: 4080, Len of Training loss: 18, Average loss: 0.9740762147638533\n",
      "Len of Validation loss: 54, Average loss: 3.3582118694429046\n",
      "Epoch: 4081, Len of Training loss: 18, Average loss: 0.9868990017308129\n",
      "Len of Validation loss: 54, Average loss: 3.4747260532997273\n",
      "Epoch: 4082, Len of Training loss: 18, Average loss: 1.0453721781571705\n",
      "Len of Validation loss: 54, Average loss: 3.511975505837688\n",
      "Epoch: 4083, Len of Training loss: 18, Average loss: 1.088542428281572\n",
      "Len of Validation loss: 54, Average loss: 3.348484419010304\n",
      "Epoch: 4084, Len of Training loss: 18, Average loss: 1.1297860211796231\n",
      "Len of Validation loss: 54, Average loss: 3.6400601742444216\n",
      "Epoch: 4085, Len of Training loss: 18, Average loss: 1.0119189123312633\n",
      "Len of Validation loss: 54, Average loss: 3.4088028040197162\n",
      "Epoch: 4086, Len of Training loss: 18, Average loss: 0.9487813346915774\n",
      "Len of Validation loss: 54, Average loss: 3.4069557002297155\n",
      "Epoch: 4087, Len of Training loss: 18, Average loss: 0.9483365681436327\n",
      "Len of Validation loss: 54, Average loss: 3.37954369297734\n",
      "Epoch: 4088, Len of Training loss: 18, Average loss: 0.9909939799043868\n",
      "Len of Validation loss: 54, Average loss: 3.4841349124908447\n",
      "Epoch: 4089, Len of Training loss: 18, Average loss: 1.0083673033449385\n",
      "Len of Validation loss: 54, Average loss: 3.5789724433863603\n",
      "Epoch: 4090, Len of Training loss: 18, Average loss: 1.064064747757382\n",
      "Len of Validation loss: 54, Average loss: 3.376209220400563\n",
      "Epoch: 4091, Len of Training loss: 18, Average loss: 1.0820976727538638\n",
      "Len of Validation loss: 54, Average loss: 3.59470076141534\n",
      "Epoch: 4092, Len of Training loss: 18, Average loss: 1.0684067176447973\n",
      "Len of Validation loss: 54, Average loss: 3.5710583881095603\n",
      "Epoch: 4093, Len of Training loss: 18, Average loss: 0.9933049844370948\n",
      "Len of Validation loss: 54, Average loss: 3.5925679670439825\n",
      "Epoch: 4094, Len of Training loss: 18, Average loss: 0.9508248269557953\n",
      "Len of Validation loss: 54, Average loss: 3.3267047471470303\n",
      "Epoch: 4095, Len of Training loss: 18, Average loss: 0.9840159581767188\n",
      "Len of Validation loss: 54, Average loss: 3.4705018964078693\n",
      "Epoch: 4096, Len of Training loss: 18, Average loss: 1.0882879164483812\n",
      "Len of Validation loss: 54, Average loss: 3.3421077606854617\n",
      "Epoch: 4097, Len of Training loss: 18, Average loss: 0.9658992720974816\n",
      "Len of Validation loss: 54, Average loss: 3.430043811047519\n",
      "Epoch: 4098, Len of Training loss: 18, Average loss: 0.9023853374852074\n",
      "Len of Validation loss: 54, Average loss: 3.2601074145899878\n",
      "Epoch: 4099, Len of Training loss: 18, Average loss: 0.9925948083400726\n",
      "Len of Validation loss: 54, Average loss: 3.4089945256710052\n",
      "Epoch: 4100, Len of Training loss: 18, Average loss: 0.920926527844535\n",
      "Len of Validation loss: 54, Average loss: 3.5571736373283245\n",
      "Epoch: 4101, Len of Training loss: 18, Average loss: 1.1404190361499786\n",
      "Len of Validation loss: 54, Average loss: 3.325268684713929\n",
      "Epoch: 4102, Len of Training loss: 18, Average loss: 1.0346470541424222\n",
      "Len of Validation loss: 54, Average loss: 3.512595268311324\n",
      "Epoch: 4103, Len of Training loss: 18, Average loss: 1.1072777575916715\n",
      "Len of Validation loss: 54, Average loss: 3.7005277971426644\n",
      "Epoch: 4104, Len of Training loss: 18, Average loss: 1.165221220917172\n",
      "Len of Validation loss: 54, Average loss: 3.4643756493374154\n",
      "Epoch: 4105, Len of Training loss: 18, Average loss: 1.0642243358823988\n",
      "Len of Validation loss: 54, Average loss: 3.4662997270071947\n",
      "Epoch: 4106, Len of Training loss: 18, Average loss: 1.1277885105874803\n",
      "Len of Validation loss: 54, Average loss: 3.398585674939332\n",
      "Epoch: 4107, Len of Training loss: 18, Average loss: 1.0685068865617116\n",
      "Len of Validation loss: 54, Average loss: 3.3648601041899786\n",
      "Epoch: 4108, Len of Training loss: 18, Average loss: 0.988623751534356\n",
      "Len of Validation loss: 54, Average loss: 3.5466178556283317\n",
      "Epoch: 4109, Len of Training loss: 18, Average loss: 1.00636468662156\n",
      "Len of Validation loss: 54, Average loss: 3.633931237238425\n",
      "Epoch: 4110, Len of Training loss: 18, Average loss: 1.0598576433128781\n",
      "Len of Validation loss: 54, Average loss: 3.4498062277281725\n",
      "Epoch: 4111, Len of Training loss: 18, Average loss: 0.976965089639028\n",
      "Len of Validation loss: 54, Average loss: 3.3594642965881913\n",
      "Epoch: 4112, Len of Training loss: 18, Average loss: 0.9592255651950836\n",
      "Len of Validation loss: 54, Average loss: 3.319049096769757\n",
      "Epoch: 4113, Len of Training loss: 18, Average loss: 0.9242290589544508\n",
      "Len of Validation loss: 54, Average loss: 3.259833969451763\n",
      "Epoch: 4114, Len of Training loss: 18, Average loss: 0.930961012840271\n",
      "Len of Validation loss: 54, Average loss: 3.3381188456658966\n",
      "Epoch: 4115, Len of Training loss: 18, Average loss: 0.9181835419601865\n",
      "Len of Validation loss: 54, Average loss: 3.342521611187193\n",
      "Epoch: 4116, Len of Training loss: 18, Average loss: 1.0033111770947774\n",
      "Len of Validation loss: 54, Average loss: 3.4024884270297155\n",
      "Epoch: 4117, Len of Training loss: 18, Average loss: 1.0265192886193593\n",
      "Len of Validation loss: 54, Average loss: 3.4235985334272736\n",
      "Epoch: 4118, Len of Training loss: 18, Average loss: 1.207183923986223\n",
      "Len of Validation loss: 54, Average loss: 3.5668789512581296\n",
      "Epoch: 4119, Len of Training loss: 18, Average loss: 1.1557805505063798\n",
      "Len of Validation loss: 54, Average loss: 3.4427691687036446\n",
      "Epoch: 4120, Len of Training loss: 18, Average loss: 1.0599066184626684\n",
      "Len of Validation loss: 54, Average loss: 3.335912005768882\n",
      "Epoch: 4121, Len of Training loss: 18, Average loss: 1.0505459441079035\n",
      "Len of Validation loss: 54, Average loss: 3.5944656718660286\n",
      "Epoch: 4122, Len of Training loss: 18, Average loss: 0.9288866023222605\n",
      "Len of Validation loss: 54, Average loss: 3.412031587627199\n",
      "Epoch: 4123, Len of Training loss: 18, Average loss: 0.9374746249781715\n",
      "Len of Validation loss: 54, Average loss: 3.4006372105192253\n",
      "Epoch: 4124, Len of Training loss: 18, Average loss: 1.020292169517941\n",
      "Len of Validation loss: 54, Average loss: 3.3680976585105613\n",
      "Epoch: 4125, Len of Training loss: 18, Average loss: 1.0325602855947282\n",
      "Len of Validation loss: 54, Average loss: 3.512264202038447\n",
      "Epoch: 4126, Len of Training loss: 18, Average loss: 0.9953010810746087\n",
      "Len of Validation loss: 54, Average loss: 3.5324223273330264\n",
      "Epoch: 4127, Len of Training loss: 18, Average loss: 0.9588951600922478\n",
      "Len of Validation loss: 54, Average loss: 3.414566549989912\n",
      "Epoch: 4128, Len of Training loss: 18, Average loss: 0.9629241459899478\n",
      "Len of Validation loss: 54, Average loss: 3.48636061946551\n",
      "Epoch: 4129, Len of Training loss: 18, Average loss: 1.1058865785598755\n",
      "Len of Validation loss: 54, Average loss: 3.5742444981027535\n",
      "Epoch: 4130, Len of Training loss: 18, Average loss: 1.1346128781636555\n",
      "Len of Validation loss: 54, Average loss: 3.5834307273228965\n",
      "Epoch: 4131, Len of Training loss: 18, Average loss: 1.1196029583613079\n",
      "Len of Validation loss: 54, Average loss: 3.4186725594379284\n",
      "Epoch: 4132, Len of Training loss: 18, Average loss: 1.0527580347326067\n",
      "Len of Validation loss: 54, Average loss: 3.424485281661705\n",
      "Epoch: 4133, Len of Training loss: 18, Average loss: 1.010537369383706\n",
      "Len of Validation loss: 54, Average loss: 3.4035554782108024\n",
      "Epoch: 4134, Len of Training loss: 18, Average loss: 0.9947982695367601\n",
      "Len of Validation loss: 54, Average loss: 3.531806281319371\n",
      "Epoch: 4135, Len of Training loss: 18, Average loss: 1.0066782004303403\n",
      "Len of Validation loss: 54, Average loss: 3.5361398734428264\n",
      "Epoch: 4136, Len of Training loss: 18, Average loss: 0.9571331044038137\n",
      "Len of Validation loss: 54, Average loss: 3.5181925053949707\n",
      "Epoch: 4137, Len of Training loss: 18, Average loss: 0.9654850198162926\n",
      "Len of Validation loss: 54, Average loss: 3.4746935665607452\n",
      "Epoch: 4138, Len of Training loss: 18, Average loss: 0.9270964364210764\n",
      "Len of Validation loss: 54, Average loss: 3.290250445957537\n",
      "Epoch: 4139, Len of Training loss: 18, Average loss: 0.9300150904390547\n",
      "Len of Validation loss: 54, Average loss: 3.2535403381895134\n",
      "Epoch: 4140, Len of Training loss: 18, Average loss: 0.912636829747094\n",
      "Len of Validation loss: 54, Average loss: 3.1897831470878035\n",
      "Epoch: 4141, Len of Training loss: 18, Average loss: 0.8979315559069315\n",
      "Len of Validation loss: 54, Average loss: 3.618366542789671\n",
      "Epoch: 4142, Len of Training loss: 18, Average loss: 1.0255927675300174\n",
      "Len of Validation loss: 54, Average loss: 3.6274940106603832\n",
      "Epoch: 4143, Len of Training loss: 18, Average loss: 1.0364929503864713\n",
      "Len of Validation loss: 54, Average loss: 3.1981376067355827\n",
      "Epoch: 4144, Len of Training loss: 18, Average loss: 0.9304149846235911\n",
      "Len of Validation loss: 54, Average loss: 3.4943708026850664\n",
      "Epoch: 4145, Len of Training loss: 18, Average loss: 0.93439605500963\n",
      "Len of Validation loss: 54, Average loss: 3.35280943248007\n",
      "Epoch: 4146, Len of Training loss: 18, Average loss: 0.9541925655470954\n",
      "Len of Validation loss: 54, Average loss: 3.2126701319659197\n",
      "Epoch: 4147, Len of Training loss: 18, Average loss: 0.9597631494204203\n",
      "Len of Validation loss: 54, Average loss: 3.4756377460779966\n",
      "Epoch: 4148, Len of Training loss: 18, Average loss: 0.9038790431287553\n",
      "Len of Validation loss: 54, Average loss: 3.6234763949005693\n",
      "Epoch: 4149, Len of Training loss: 18, Average loss: 0.8917969001664056\n",
      "Len of Validation loss: 54, Average loss: 3.3998946955910436\n",
      "Epoch: 4150, Len of Training loss: 18, Average loss: 0.941706786553065\n",
      "Len of Validation loss: 54, Average loss: 3.4163333740499287\n",
      "Epoch: 4151, Len of Training loss: 18, Average loss: 1.0166576736503177\n",
      "Len of Validation loss: 54, Average loss: 3.3911930642746113\n",
      "Epoch: 4152, Len of Training loss: 18, Average loss: 0.9796781341234843\n",
      "Len of Validation loss: 54, Average loss: 3.3908356571639025\n",
      "Epoch: 4153, Len of Training loss: 18, Average loss: 1.124560746881697\n",
      "Len of Validation loss: 54, Average loss: 3.6257795846020735\n",
      "Epoch: 4154, Len of Training loss: 18, Average loss: 1.3323127494917975\n",
      "Len of Validation loss: 54, Average loss: 3.441791268410506\n",
      "Epoch: 4155, Len of Training loss: 18, Average loss: 1.1030287312136755\n",
      "Len of Validation loss: 54, Average loss: 3.491619755824407\n",
      "Epoch: 4156, Len of Training loss: 18, Average loss: 1.0172997183269925\n",
      "Len of Validation loss: 54, Average loss: 3.6967873904440136\n",
      "Epoch: 4157, Len of Training loss: 18, Average loss: 1.0132927397886913\n",
      "Len of Validation loss: 54, Average loss: 3.509733217733878\n",
      "Epoch: 4158, Len of Training loss: 18, Average loss: 1.2780712842941284\n",
      "Len of Validation loss: 54, Average loss: 3.559755430177406\n",
      "Epoch: 4159, Len of Training loss: 18, Average loss: 1.2530532280604045\n",
      "Len of Validation loss: 54, Average loss: 3.4539528213165425\n",
      "Epoch: 4160, Len of Training loss: 18, Average loss: 1.2785125739044614\n",
      "Len of Validation loss: 54, Average loss: 3.7095740271939173\n",
      "Epoch: 4161, Len of Training loss: 18, Average loss: 1.2535467743873596\n",
      "Len of Validation loss: 54, Average loss: 3.527579805365315\n",
      "Epoch: 4162, Len of Training loss: 18, Average loss: 1.2534789078765445\n",
      "Len of Validation loss: 54, Average loss: 3.4108394274005183\n",
      "Epoch: 4163, Len of Training loss: 18, Average loss: 1.2193928559621174\n",
      "Len of Validation loss: 54, Average loss: 3.7248406410217285\n",
      "Epoch: 4164, Len of Training loss: 18, Average loss: 1.1792560617129009\n",
      "Len of Validation loss: 54, Average loss: 3.534721090837761\n",
      "Epoch: 4165, Len of Training loss: 18, Average loss: 1.1053554250134363\n",
      "Len of Validation loss: 54, Average loss: 3.418467336230808\n",
      "Epoch: 4166, Len of Training loss: 18, Average loss: 1.011212279399236\n",
      "Len of Validation loss: 54, Average loss: 3.393996736517659\n",
      "Epoch: 4167, Len of Training loss: 18, Average loss: 1.052623000409868\n",
      "Len of Validation loss: 54, Average loss: 3.3627712770744607\n",
      "Epoch: 4168, Len of Training loss: 18, Average loss: 1.076405988799201\n",
      "Len of Validation loss: 54, Average loss: 3.4179803419996193\n",
      "Epoch: 4169, Len of Training loss: 18, Average loss: 1.015828592909707\n",
      "Len of Validation loss: 54, Average loss: 3.4657355745633445\n",
      "Epoch: 4170, Len of Training loss: 18, Average loss: 0.9846055739455752\n",
      "Len of Validation loss: 54, Average loss: 3.4838849306106567\n",
      "Epoch: 4171, Len of Training loss: 18, Average loss: 1.0013690557744768\n",
      "Len of Validation loss: 54, Average loss: 3.5385638629948653\n",
      "Epoch: 4172, Len of Training loss: 18, Average loss: 1.0408281650808122\n",
      "Len of Validation loss: 54, Average loss: 3.5420221862969576\n",
      "Epoch: 4173, Len of Training loss: 18, Average loss: 1.2026610804928675\n",
      "Len of Validation loss: 54, Average loss: 3.564723411092052\n",
      "Epoch: 4174, Len of Training loss: 18, Average loss: 1.1041685276561313\n",
      "Len of Validation loss: 54, Average loss: 3.4875788225067987\n",
      "Epoch: 4175, Len of Training loss: 18, Average loss: 1.0109700328773923\n",
      "Len of Validation loss: 54, Average loss: 3.366876104363689\n",
      "Epoch: 4176, Len of Training loss: 18, Average loss: 1.1363779140843286\n",
      "Len of Validation loss: 54, Average loss: 3.556859819977372\n",
      "Epoch: 4177, Len of Training loss: 18, Average loss: 1.0362360543674893\n",
      "Len of Validation loss: 54, Average loss: 3.3351002501116858\n",
      "Epoch: 4178, Len of Training loss: 18, Average loss: 1.257388237449858\n",
      "Len of Validation loss: 54, Average loss: 3.4570195730085724\n",
      "Epoch: 4179, Len of Training loss: 18, Average loss: 1.1473694344361622\n",
      "Len of Validation loss: 54, Average loss: 3.372143846971017\n",
      "Epoch: 4180, Len of Training loss: 18, Average loss: 1.0750072598457336\n",
      "Len of Validation loss: 54, Average loss: 3.6959614290131464\n",
      "Epoch: 4181, Len of Training loss: 18, Average loss: 1.2178562084833782\n",
      "Len of Validation loss: 54, Average loss: 3.4259908740167266\n",
      "Epoch: 4182, Len of Training loss: 18, Average loss: 1.0330451892481909\n",
      "Len of Validation loss: 54, Average loss: 3.397667211514932\n",
      "Epoch: 4183, Len of Training loss: 18, Average loss: 0.9819681876235538\n",
      "Len of Validation loss: 54, Average loss: 3.2579251759582095\n",
      "Epoch: 4184, Len of Training loss: 18, Average loss: 0.9365876581933763\n",
      "Len of Validation loss: 54, Average loss: 3.5172319842709436\n",
      "Epoch: 4185, Len of Training loss: 18, Average loss: 0.9832515782780118\n",
      "Len of Validation loss: 54, Average loss: 3.413335186463815\n",
      "Epoch: 4186, Len of Training loss: 18, Average loss: 1.0456664595339034\n",
      "Len of Validation loss: 54, Average loss: 3.5093341878166906\n",
      "Epoch: 4187, Len of Training loss: 18, Average loss: 1.0040199359258015\n",
      "Len of Validation loss: 54, Average loss: 3.475968353174351\n",
      "Epoch: 4188, Len of Training loss: 18, Average loss: 0.9709185428089566\n",
      "Len of Validation loss: 54, Average loss: 3.426369066591616\n",
      "Epoch: 4189, Len of Training loss: 18, Average loss: 0.9494190249178145\n",
      "Len of Validation loss: 54, Average loss: 3.38524533090768\n",
      "Epoch: 4190, Len of Training loss: 18, Average loss: 0.9283383786678314\n",
      "Len of Validation loss: 54, Average loss: 3.3861204187075296\n",
      "Epoch: 4191, Len of Training loss: 18, Average loss: 0.9345427089267306\n",
      "Len of Validation loss: 54, Average loss: 3.5617735849486456\n",
      "Epoch: 4192, Len of Training loss: 18, Average loss: 1.0864715112580194\n",
      "Len of Validation loss: 54, Average loss: 3.350886098764561\n",
      "Epoch: 4193, Len of Training loss: 18, Average loss: 1.2742244137658014\n",
      "Len of Validation loss: 54, Average loss: 3.5635608942420394\n",
      "Epoch: 4194, Len of Training loss: 18, Average loss: 1.175376209947798\n",
      "Len of Validation loss: 54, Average loss: 3.5057416690720453\n",
      "Epoch: 4195, Len of Training loss: 18, Average loss: 1.2087040543556213\n",
      "Len of Validation loss: 54, Average loss: 3.676564320370003\n",
      "Epoch: 4196, Len of Training loss: 18, Average loss: 1.1621705161200628\n",
      "Len of Validation loss: 54, Average loss: 3.5355570294238903\n",
      "Epoch: 4197, Len of Training loss: 18, Average loss: 1.150478012031979\n",
      "Len of Validation loss: 54, Average loss: 3.3394387419576996\n",
      "Epoch: 4198, Len of Training loss: 18, Average loss: 1.0139003064897325\n",
      "Len of Validation loss: 54, Average loss: 3.311428126361635\n",
      "Epoch: 4199, Len of Training loss: 18, Average loss: 1.0838160481717851\n",
      "Len of Validation loss: 54, Average loss: 3.3940119875801935\n",
      "Epoch: 4200, Len of Training loss: 18, Average loss: 0.9746328956551022\n",
      "Len of Validation loss: 54, Average loss: 3.5889726877212524\n",
      "Epoch: 4201, Len of Training loss: 18, Average loss: 0.938653932677375\n",
      "Len of Validation loss: 54, Average loss: 3.2493085353462785\n",
      "Epoch: 4202, Len of Training loss: 18, Average loss: 0.9779643913110098\n",
      "Len of Validation loss: 54, Average loss: 3.6661828690105014\n",
      "Epoch: 4203, Len of Training loss: 18, Average loss: 1.0812154048018985\n",
      "Len of Validation loss: 54, Average loss: 3.5371759787753776\n",
      "Epoch: 4204, Len of Training loss: 18, Average loss: 1.0230307181676228\n",
      "Len of Validation loss: 54, Average loss: 3.3220687762454704\n",
      "Epoch: 4205, Len of Training loss: 18, Average loss: 0.9053205450375875\n",
      "Len of Validation loss: 54, Average loss: 3.393130895164278\n",
      "Epoch: 4206, Len of Training loss: 18, Average loss: 0.8845097886191474\n",
      "Len of Validation loss: 54, Average loss: 3.5077674697946617\n",
      "Epoch: 4207, Len of Training loss: 18, Average loss: 0.9662514825661978\n",
      "Len of Validation loss: 54, Average loss: 3.383762691859846\n",
      "Epoch: 4208, Len of Training loss: 18, Average loss: 0.8968826134999593\n",
      "Len of Validation loss: 54, Average loss: 3.2968057040815\n",
      "Epoch: 4209, Len of Training loss: 18, Average loss: 0.9296424786249796\n",
      "Len of Validation loss: 54, Average loss: 3.4911346159599446\n",
      "Epoch: 4210, Len of Training loss: 18, Average loss: 0.9546898073620267\n",
      "Len of Validation loss: 54, Average loss: 3.2551666917624296\n",
      "Epoch: 4211, Len of Training loss: 18, Average loss: 1.3872598210970561\n",
      "Len of Validation loss: 54, Average loss: 3.8546371393733554\n",
      "Epoch: 4212, Len of Training loss: 18, Average loss: 1.204913682407803\n",
      "Len of Validation loss: 54, Average loss: 3.53364477775715\n",
      "Epoch: 4213, Len of Training loss: 18, Average loss: 1.086940950817532\n",
      "Len of Validation loss: 54, Average loss: 3.6437192183953746\n",
      "Epoch: 4214, Len of Training loss: 18, Average loss: 1.0495229628351\n",
      "Len of Validation loss: 54, Average loss: 3.5461208014576524\n",
      "Epoch: 4215, Len of Training loss: 18, Average loss: 0.9723836349116431\n",
      "Len of Validation loss: 54, Average loss: 3.4917511034894875\n",
      "Epoch: 4216, Len of Training loss: 18, Average loss: 0.9324877791934543\n",
      "Len of Validation loss: 54, Average loss: 3.4472939096115254\n",
      "Epoch: 4217, Len of Training loss: 18, Average loss: 0.9554021656513214\n",
      "Len of Validation loss: 54, Average loss: 3.3719243815651647\n",
      "Epoch: 4218, Len of Training loss: 18, Average loss: 0.8909261425336202\n",
      "Len of Validation loss: 54, Average loss: 3.46996592702689\n",
      "Epoch: 4219, Len of Training loss: 18, Average loss: 0.8917297124862671\n",
      "Len of Validation loss: 54, Average loss: 3.303637425104777\n",
      "Epoch: 4220, Len of Training loss: 18, Average loss: 0.9036603702439202\n",
      "Len of Validation loss: 54, Average loss: 3.501963730211611\n",
      "Epoch: 4221, Len of Training loss: 18, Average loss: 0.9195789198080698\n",
      "Len of Validation loss: 54, Average loss: 3.3289771455305592\n",
      "Epoch: 4222, Len of Training loss: 18, Average loss: 1.0486671494113073\n",
      "Len of Validation loss: 54, Average loss: 3.7138908450250274\n",
      "Epoch: 4223, Len of Training loss: 18, Average loss: 1.083438903093338\n",
      "Len of Validation loss: 54, Average loss: 3.42680882524561\n",
      "Epoch: 4224, Len of Training loss: 18, Average loss: 0.9858408239152696\n",
      "Len of Validation loss: 54, Average loss: 3.430731372700797\n",
      "Epoch: 4225, Len of Training loss: 18, Average loss: 0.9458430078294542\n",
      "Len of Validation loss: 54, Average loss: 3.4826114895167173\n",
      "Epoch: 4226, Len of Training loss: 18, Average loss: 1.017036063803567\n",
      "Len of Validation loss: 54, Average loss: 3.6683074876114175\n",
      "Epoch: 4227, Len of Training loss: 18, Average loss: 1.1981378727489047\n",
      "Len of Validation loss: 54, Average loss: 3.7636068099074893\n",
      "Epoch: 4228, Len of Training loss: 18, Average loss: 1.1140715314282312\n",
      "Len of Validation loss: 54, Average loss: 3.525029276256208\n",
      "Epoch: 4229, Len of Training loss: 18, Average loss: 1.2117109133137598\n",
      "Len of Validation loss: 54, Average loss: 3.7966950679266893\n",
      "Epoch: 4230, Len of Training loss: 18, Average loss: 1.4230164951748319\n",
      "Len of Validation loss: 54, Average loss: 3.548120457817007\n",
      "Epoch: 4231, Len of Training loss: 18, Average loss: 1.1615599592526753\n",
      "Len of Validation loss: 54, Average loss: 3.590063018931283\n",
      "Epoch: 4232, Len of Training loss: 18, Average loss: 1.0806581576665242\n",
      "Len of Validation loss: 54, Average loss: 3.4346811550634877\n",
      "Epoch: 4233, Len of Training loss: 18, Average loss: 1.1810378564728632\n",
      "Len of Validation loss: 54, Average loss: 3.4026018138285035\n",
      "Epoch: 4234, Len of Training loss: 18, Average loss: 1.1492467721303303\n",
      "Len of Validation loss: 54, Average loss: 3.4742981636965715\n",
      "Epoch: 4235, Len of Training loss: 18, Average loss: 1.0795981056160397\n",
      "Len of Validation loss: 54, Average loss: 3.4620177304303206\n",
      "Epoch: 4236, Len of Training loss: 18, Average loss: 1.043411397271686\n",
      "Len of Validation loss: 54, Average loss: 3.4750835994879403\n",
      "Epoch: 4237, Len of Training loss: 18, Average loss: 1.0143261783652835\n",
      "Len of Validation loss: 54, Average loss: 3.4774120405868247\n",
      "Epoch: 4238, Len of Training loss: 18, Average loss: 1.021630883216858\n",
      "Len of Validation loss: 54, Average loss: 3.4518700352421514\n",
      "Epoch: 4239, Len of Training loss: 18, Average loss: 0.9739959504869249\n",
      "Len of Validation loss: 54, Average loss: 3.4325704166182764\n",
      "Epoch: 4240, Len of Training loss: 18, Average loss: 1.0050297611289554\n",
      "Len of Validation loss: 54, Average loss: 3.337621985762208\n",
      "Epoch: 4241, Len of Training loss: 18, Average loss: 1.0237219201193914\n",
      "Len of Validation loss: 54, Average loss: 3.7712559081889965\n",
      "Epoch: 4242, Len of Training loss: 18, Average loss: 1.3056079281700983\n",
      "Len of Validation loss: 54, Average loss: 3.5645696035137884\n",
      "Epoch: 4243, Len of Training loss: 18, Average loss: 1.0337868763340845\n",
      "Len of Validation loss: 54, Average loss: 3.451193429805614\n",
      "Epoch: 4244, Len of Training loss: 18, Average loss: 1.164114193783866\n",
      "Len of Validation loss: 54, Average loss: 3.8507740210603782\n",
      "Epoch: 4245, Len of Training loss: 18, Average loss: 1.2578600578837924\n",
      "Len of Validation loss: 54, Average loss: 3.6461599789283894\n",
      "Epoch: 4246, Len of Training loss: 18, Average loss: 1.1471142801973555\n",
      "Len of Validation loss: 54, Average loss: 3.5009395899596036\n",
      "Epoch: 4247, Len of Training loss: 18, Average loss: 0.9499206874105666\n",
      "Len of Validation loss: 54, Average loss: 3.235082706919423\n",
      "Epoch: 4248, Len of Training loss: 18, Average loss: 0.9982503751913706\n",
      "Len of Validation loss: 54, Average loss: 3.512272708945804\n",
      "Epoch: 4249, Len of Training loss: 18, Average loss: 1.2772454288270738\n",
      "Len of Validation loss: 54, Average loss: 3.4406505790021686\n",
      "Epoch: 4250, Len of Training loss: 18, Average loss: 1.1499167051580217\n",
      "Len of Validation loss: 54, Average loss: 3.5707581683441445\n",
      "Epoch: 4251, Len of Training loss: 18, Average loss: 0.9718134668138292\n",
      "Len of Validation loss: 54, Average loss: 3.433487923056991\n",
      "Epoch: 4252, Len of Training loss: 18, Average loss: 0.9322000675731235\n",
      "Len of Validation loss: 54, Average loss: 3.332770006524192\n",
      "Epoch: 4253, Len of Training loss: 18, Average loss: 1.1071520613299475\n",
      "Len of Validation loss: 54, Average loss: 3.309367311221582\n",
      "Epoch: 4254, Len of Training loss: 18, Average loss: 1.0412628981802199\n",
      "Len of Validation loss: 54, Average loss: 3.352721458231961\n",
      "Epoch: 4255, Len of Training loss: 18, Average loss: 1.0071649220254686\n",
      "Len of Validation loss: 54, Average loss: 3.5204781028959484\n",
      "Epoch: 4256, Len of Training loss: 18, Average loss: 1.0290241340796153\n",
      "Len of Validation loss: 54, Average loss: 3.5486161278353796\n",
      "Epoch: 4257, Len of Training loss: 18, Average loss: 1.011439926094479\n",
      "Len of Validation loss: 54, Average loss: 3.4572204737751573\n",
      "Epoch: 4258, Len of Training loss: 18, Average loss: 0.9094823035928938\n",
      "Len of Validation loss: 54, Average loss: 3.3785084143832878\n",
      "Epoch: 4259, Len of Training loss: 18, Average loss: 0.8910667233996921\n",
      "Len of Validation loss: 54, Average loss: 3.206566338185911\n",
      "Epoch: 4260, Len of Training loss: 18, Average loss: 1.0065583520465426\n",
      "Len of Validation loss: 54, Average loss: 3.5956439519370043\n",
      "Epoch: 4261, Len of Training loss: 18, Average loss: 0.9996123048994277\n",
      "Len of Validation loss: 54, Average loss: 3.468272425510265\n",
      "Epoch: 4262, Len of Training loss: 18, Average loss: 1.0319070286220975\n",
      "Len of Validation loss: 54, Average loss: 3.646874475258368\n",
      "Epoch: 4263, Len of Training loss: 18, Average loss: 0.9967782265610166\n",
      "Len of Validation loss: 54, Average loss: 3.3905164202054343\n",
      "Epoch: 4264, Len of Training loss: 18, Average loss: 0.9698556760946909\n",
      "Len of Validation loss: 54, Average loss: 3.3713342116938696\n",
      "Epoch: 4265, Len of Training loss: 18, Average loss: 0.9104525115754869\n",
      "Len of Validation loss: 54, Average loss: 3.2767861043965376\n",
      "Epoch: 4266, Len of Training loss: 18, Average loss: 0.882709241575665\n",
      "Len of Validation loss: 54, Average loss: 3.462572604417801\n",
      "Epoch: 4267, Len of Training loss: 18, Average loss: 0.8946178522374895\n",
      "Len of Validation loss: 54, Average loss: 3.397071498411673\n",
      "Epoch: 4268, Len of Training loss: 18, Average loss: 1.01093457142512\n",
      "Len of Validation loss: 54, Average loss: 3.398440182209015\n",
      "Epoch: 4269, Len of Training loss: 18, Average loss: 0.941840668519338\n",
      "Len of Validation loss: 54, Average loss: 3.4475465129922935\n",
      "Epoch: 4270, Len of Training loss: 18, Average loss: 0.9618149035506778\n",
      "Len of Validation loss: 54, Average loss: 3.444275089988002\n",
      "Epoch: 4271, Len of Training loss: 18, Average loss: 1.0006223188506231\n",
      "Len of Validation loss: 54, Average loss: 3.5800449307318085\n",
      "Epoch: 4272, Len of Training loss: 18, Average loss: 0.9692483378781213\n",
      "Len of Validation loss: 54, Average loss: 3.4154089667178966\n",
      "Epoch: 4273, Len of Training loss: 18, Average loss: 1.0134579804208543\n",
      "Len of Validation loss: 54, Average loss: 3.3755464840818337\n",
      "Epoch: 4274, Len of Training loss: 18, Average loss: 0.9845128191841973\n",
      "Len of Validation loss: 54, Average loss: 3.5985643057911485\n",
      "Epoch: 4275, Len of Training loss: 18, Average loss: 0.9455356664127774\n",
      "Len of Validation loss: 54, Average loss: 3.577427938028618\n",
      "Epoch: 4276, Len of Training loss: 18, Average loss: 0.9288769000106387\n",
      "Len of Validation loss: 54, Average loss: 3.500454835317753\n",
      "Epoch: 4277, Len of Training loss: 18, Average loss: 0.9412596358193291\n",
      "Len of Validation loss: 54, Average loss: 3.533921558547903\n",
      "Epoch: 4278, Len of Training loss: 18, Average loss: 0.945236454407374\n",
      "Len of Validation loss: 54, Average loss: 3.434793825502749\n",
      "Epoch: 4279, Len of Training loss: 18, Average loss: 1.0225656926631927\n",
      "Len of Validation loss: 54, Average loss: 3.3642004606900393\n",
      "Epoch: 4280, Len of Training loss: 18, Average loss: 0.9753548502922058\n",
      "Len of Validation loss: 54, Average loss: 3.541785188295223\n",
      "Epoch: 4281, Len of Training loss: 18, Average loss: 1.0653596288628049\n",
      "Len of Validation loss: 54, Average loss: 3.377419787424582\n",
      "Epoch: 4282, Len of Training loss: 18, Average loss: 0.9645345277256436\n",
      "Len of Validation loss: 54, Average loss: 3.4241836380075523\n",
      "Epoch: 4283, Len of Training loss: 18, Average loss: 0.9281079471111298\n",
      "Len of Validation loss: 54, Average loss: 3.3903913608303777\n",
      "Epoch: 4284, Len of Training loss: 18, Average loss: 1.0336180561118655\n",
      "Len of Validation loss: 54, Average loss: 3.5207201176219516\n",
      "Epoch: 4285, Len of Training loss: 18, Average loss: 1.0532391005092197\n",
      "Len of Validation loss: 54, Average loss: 3.5277780537252075\n",
      "Epoch: 4286, Len of Training loss: 18, Average loss: 1.0982853372891743\n",
      "Len of Validation loss: 54, Average loss: 3.487227645185259\n",
      "Epoch: 4287, Len of Training loss: 18, Average loss: 0.9831573963165283\n",
      "Len of Validation loss: 54, Average loss: 3.387434596264804\n",
      "Epoch: 4288, Len of Training loss: 18, Average loss: 1.0288625823126898\n",
      "Len of Validation loss: 54, Average loss: 3.2767742772897086\n",
      "Epoch: 4289, Len of Training loss: 18, Average loss: 1.1971077852778964\n",
      "Len of Validation loss: 54, Average loss: 3.4425011453805148\n",
      "Epoch: 4290, Len of Training loss: 18, Average loss: 1.0758203234937456\n",
      "Len of Validation loss: 54, Average loss: 3.421677961393639\n",
      "Epoch: 4291, Len of Training loss: 18, Average loss: 1.0131271481513977\n",
      "Len of Validation loss: 54, Average loss: 3.5608252463517367\n",
      "Epoch: 4292, Len of Training loss: 18, Average loss: 1.0329128073321447\n",
      "Len of Validation loss: 54, Average loss: 3.5225345812461994\n",
      "Epoch: 4293, Len of Training loss: 18, Average loss: 1.1366004414028592\n",
      "Len of Validation loss: 54, Average loss: 3.413691227082853\n",
      "Epoch: 4294, Len of Training loss: 18, Average loss: 1.034835673040814\n",
      "Len of Validation loss: 54, Average loss: 3.5252769633575722\n",
      "Epoch: 4295, Len of Training loss: 18, Average loss: 0.9944631258646647\n",
      "Len of Validation loss: 54, Average loss: 3.4015222092469535\n",
      "Epoch: 4296, Len of Training loss: 18, Average loss: 0.9921708305676779\n",
      "Len of Validation loss: 54, Average loss: 3.159136786505028\n",
      "Epoch: 4297, Len of Training loss: 18, Average loss: 0.919895350933075\n",
      "Len of Validation loss: 54, Average loss: 3.343875077035692\n",
      "Epoch: 4298, Len of Training loss: 18, Average loss: 0.9141261610719893\n",
      "Len of Validation loss: 54, Average loss: 3.4527168571949005\n",
      "Epoch: 4299, Len of Training loss: 18, Average loss: 0.9714184072282579\n",
      "Len of Validation loss: 54, Average loss: 3.4956019575949067\n",
      "Epoch: 4300, Len of Training loss: 18, Average loss: 0.8998360733191172\n",
      "Len of Validation loss: 54, Average loss: 3.401504388561955\n",
      "Epoch: 4301, Len of Training loss: 18, Average loss: 1.0148259666230943\n",
      "Len of Validation loss: 54, Average loss: 3.5942538612418704\n",
      "Epoch: 4302, Len of Training loss: 18, Average loss: 0.966320640510983\n",
      "Len of Validation loss: 54, Average loss: 3.3649777902497187\n",
      "Epoch: 4303, Len of Training loss: 18, Average loss: 0.9716476566261716\n",
      "Len of Validation loss: 54, Average loss: 3.693539704437609\n",
      "Epoch: 4304, Len of Training loss: 18, Average loss: 0.916135311126709\n",
      "Len of Validation loss: 54, Average loss: 3.5951436735965587\n",
      "Epoch: 4305, Len of Training loss: 18, Average loss: 0.8665426638391283\n",
      "Len of Validation loss: 54, Average loss: 3.263320329012694\n",
      "Epoch: 4306, Len of Training loss: 18, Average loss: 0.89410643113984\n",
      "Len of Validation loss: 54, Average loss: 3.439763640915906\n",
      "Epoch: 4307, Len of Training loss: 18, Average loss: 1.083549244536294\n",
      "Len of Validation loss: 54, Average loss: 3.7713674108187356\n",
      "Epoch: 4308, Len of Training loss: 18, Average loss: 1.2379673189587064\n",
      "Len of Validation loss: 54, Average loss: 3.382914806957598\n",
      "Epoch: 4309, Len of Training loss: 18, Average loss: 1.0921134948730469\n",
      "Len of Validation loss: 54, Average loss: 3.466956942169755\n",
      "Epoch: 4310, Len of Training loss: 18, Average loss: 0.9947507613235049\n",
      "Len of Validation loss: 54, Average loss: 3.4497223021807493\n",
      "Epoch: 4311, Len of Training loss: 18, Average loss: 0.9521171649297079\n",
      "Len of Validation loss: 54, Average loss: 3.4140043037909047\n",
      "Epoch: 4312, Len of Training loss: 18, Average loss: 0.9659945501221551\n",
      "Len of Validation loss: 54, Average loss: 3.587975255869053\n",
      "Epoch: 4313, Len of Training loss: 18, Average loss: 0.9573001364866892\n",
      "Len of Validation loss: 54, Average loss: 3.60108929210239\n",
      "Epoch: 4314, Len of Training loss: 18, Average loss: 1.1875828272766538\n",
      "Len of Validation loss: 54, Average loss: 3.4859350511321314\n",
      "Epoch: 4315, Len of Training loss: 18, Average loss: 1.1439931624465518\n",
      "Len of Validation loss: 54, Average loss: 3.5950836263321064\n",
      "Epoch: 4316, Len of Training loss: 18, Average loss: 1.1987599465582106\n",
      "Len of Validation loss: 54, Average loss: 3.62809505837935\n",
      "Epoch: 4317, Len of Training loss: 18, Average loss: 1.1387264132499695\n",
      "Len of Validation loss: 54, Average loss: 3.5659657286273108\n",
      "Epoch: 4318, Len of Training loss: 18, Average loss: 1.056012597348955\n",
      "Len of Validation loss: 54, Average loss: 3.4653115371863046\n",
      "Epoch: 4319, Len of Training loss: 18, Average loss: 0.9537369244628482\n",
      "Len of Validation loss: 54, Average loss: 3.31013121869829\n",
      "Epoch: 4320, Len of Training loss: 18, Average loss: 0.8860492606957754\n",
      "Len of Validation loss: 54, Average loss: 3.2847235202789307\n",
      "Epoch: 4321, Len of Training loss: 18, Average loss: 0.8707789348231422\n",
      "Len of Validation loss: 54, Average loss: 3.328310447710532\n",
      "Epoch: 4322, Len of Training loss: 18, Average loss: 0.8798863225513034\n",
      "Len of Validation loss: 54, Average loss: 3.445026989336367\n",
      "Epoch: 4323, Len of Training loss: 18, Average loss: 0.934284034702513\n",
      "Len of Validation loss: 54, Average loss: 3.475618310548641\n",
      "Epoch: 4324, Len of Training loss: 18, Average loss: 1.1795395910739899\n",
      "Len of Validation loss: 54, Average loss: 3.544104159982116\n",
      "Epoch: 4325, Len of Training loss: 18, Average loss: 1.4948216213120356\n",
      "Len of Validation loss: 54, Average loss: 3.6346311370531716\n",
      "Epoch: 4326, Len of Training loss: 18, Average loss: 1.3641531136300828\n",
      "Len of Validation loss: 54, Average loss: 3.4310285062701613\n",
      "Epoch: 4327, Len of Training loss: 18, Average loss: 1.4184039566251967\n",
      "Len of Validation loss: 54, Average loss: 3.5899001713152283\n",
      "Epoch: 4328, Len of Training loss: 18, Average loss: 1.2898226380348206\n",
      "Len of Validation loss: 54, Average loss: 3.776436514324612\n",
      "Epoch: 4329, Len of Training loss: 18, Average loss: 1.148663494322035\n",
      "Len of Validation loss: 54, Average loss: 3.548334921951647\n",
      "Epoch: 4330, Len of Training loss: 18, Average loss: 1.2886361744668748\n",
      "Len of Validation loss: 54, Average loss: 3.6388284166653952\n",
      "Epoch: 4331, Len of Training loss: 18, Average loss: 1.1823858353826735\n",
      "Len of Validation loss: 54, Average loss: 3.537202180535705\n",
      "Epoch: 4332, Len of Training loss: 18, Average loss: 1.0583451787630718\n",
      "Len of Validation loss: 54, Average loss: 3.451236351772591\n",
      "Epoch: 4333, Len of Training loss: 18, Average loss: 0.9976885351869795\n",
      "Len of Validation loss: 54, Average loss: 3.3538238439295025\n",
      "Epoch: 4334, Len of Training loss: 18, Average loss: 0.9452727006541358\n",
      "Len of Validation loss: 54, Average loss: 3.4941685265964932\n",
      "Epoch: 4335, Len of Training loss: 18, Average loss: 0.9253317614396414\n",
      "Len of Validation loss: 54, Average loss: 3.452159849581895\n",
      "Epoch: 4336, Len of Training loss: 18, Average loss: 1.007999963230557\n",
      "Len of Validation loss: 54, Average loss: 3.543681697713004\n",
      "Epoch: 4337, Len of Training loss: 18, Average loss: 0.9714504314793481\n",
      "Len of Validation loss: 54, Average loss: 3.5419246973814786\n",
      "Epoch: 4338, Len of Training loss: 18, Average loss: 0.9981511566374037\n",
      "Len of Validation loss: 54, Average loss: 3.6348341548884355\n",
      "Epoch: 4339, Len of Training loss: 18, Average loss: 1.0607557793458302\n",
      "Len of Validation loss: 54, Average loss: 3.4785697327719793\n",
      "Epoch: 4340, Len of Training loss: 18, Average loss: 1.3277883066071405\n",
      "Len of Validation loss: 54, Average loss: 3.791624184007998\n",
      "Epoch: 4341, Len of Training loss: 18, Average loss: 1.2906410495440166\n",
      "Len of Validation loss: 54, Average loss: 3.6479180918799505\n",
      "Epoch: 4342, Len of Training loss: 18, Average loss: 1.0793599486351013\n",
      "Len of Validation loss: 54, Average loss: 3.44030111807364\n",
      "Epoch: 4343, Len of Training loss: 18, Average loss: 1.0141737394862704\n",
      "Len of Validation loss: 54, Average loss: 3.2661857185540377\n",
      "Epoch: 4344, Len of Training loss: 18, Average loss: 1.0738744172785017\n",
      "Len of Validation loss: 54, Average loss: 3.789232447191521\n",
      "Epoch: 4345, Len of Training loss: 18, Average loss: 1.0666836533281538\n",
      "Len of Validation loss: 54, Average loss: 3.2987192648428456\n",
      "Epoch: 4346, Len of Training loss: 18, Average loss: 0.9300516247749329\n",
      "Len of Validation loss: 54, Average loss: 3.3574054031460374\n",
      "Epoch: 4347, Len of Training loss: 18, Average loss: 0.9301597575346628\n",
      "Len of Validation loss: 54, Average loss: 3.2951631590172097\n",
      "Epoch: 4348, Len of Training loss: 18, Average loss: 0.9550124009450277\n",
      "Len of Validation loss: 54, Average loss: 3.4145903289318085\n",
      "Epoch: 4349, Len of Training loss: 18, Average loss: 0.9029805825816261\n",
      "Len of Validation loss: 54, Average loss: 3.450515062720687\n",
      "Epoch: 4350, Len of Training loss: 18, Average loss: 0.9449880487389035\n",
      "Len of Validation loss: 54, Average loss: 3.6160516838232675\n",
      "Epoch: 4351, Len of Training loss: 18, Average loss: 1.011412137084537\n",
      "Len of Validation loss: 54, Average loss: 3.4875063598155975\n",
      "Epoch: 4352, Len of Training loss: 18, Average loss: 0.9923466874493493\n",
      "Len of Validation loss: 54, Average loss: 3.6960099471939936\n",
      "Epoch: 4353, Len of Training loss: 18, Average loss: 1.0333061748080783\n",
      "Len of Validation loss: 54, Average loss: 3.518853172107979\n",
      "Epoch: 4354, Len of Training loss: 18, Average loss: 0.9580444859133826\n",
      "Len of Validation loss: 54, Average loss: 3.469793251267186\n",
      "Epoch: 4355, Len of Training loss: 18, Average loss: 0.9257642991012998\n",
      "Len of Validation loss: 54, Average loss: 3.406416767173343\n",
      "Epoch: 4356, Len of Training loss: 18, Average loss: 0.9696219232347276\n",
      "Len of Validation loss: 54, Average loss: 3.6614771010699094\n",
      "Epoch: 4357, Len of Training loss: 18, Average loss: 0.8904701438215044\n",
      "Len of Validation loss: 54, Average loss: 3.453828183589158\n",
      "Epoch: 4358, Len of Training loss: 18, Average loss: 0.894748310248057\n",
      "Len of Validation loss: 54, Average loss: 3.4696292766818293\n",
      "Epoch: 4359, Len of Training loss: 18, Average loss: 0.9811988406711154\n",
      "Len of Validation loss: 54, Average loss: 3.441126752782751\n",
      "Epoch: 4360, Len of Training loss: 18, Average loss: 1.037517484691408\n",
      "Len of Validation loss: 54, Average loss: 3.405183180614754\n",
      "Epoch: 4361, Len of Training loss: 18, Average loss: 0.9615293674998813\n",
      "Len of Validation loss: 54, Average loss: 3.3817622341491558\n",
      "Epoch: 4362, Len of Training loss: 18, Average loss: 1.008096804221471\n",
      "Len of Validation loss: 54, Average loss: 3.5146234565311008\n",
      "Epoch: 4363, Len of Training loss: 18, Average loss: 1.0164648559358385\n",
      "Len of Validation loss: 54, Average loss: 3.376518647979807\n",
      "Epoch: 4364, Len of Training loss: 18, Average loss: 0.963915593094296\n",
      "Len of Validation loss: 54, Average loss: 3.639316591951582\n",
      "Epoch: 4365, Len of Training loss: 18, Average loss: 0.9311011864079369\n",
      "Len of Validation loss: 54, Average loss: 3.4207401397051633\n",
      "Epoch: 4366, Len of Training loss: 18, Average loss: 0.9778024819162157\n",
      "Len of Validation loss: 54, Average loss: 3.346366388930215\n",
      "Epoch: 4367, Len of Training loss: 18, Average loss: 0.9590043491787381\n",
      "Len of Validation loss: 54, Average loss: 3.386310366568742\n",
      "Epoch: 4368, Len of Training loss: 18, Average loss: 0.9952854017416636\n",
      "Len of Validation loss: 54, Average loss: 3.3841172655423484\n",
      "Epoch: 4369, Len of Training loss: 18, Average loss: 1.015910569164488\n",
      "Len of Validation loss: 54, Average loss: 3.5546532107724085\n",
      "Epoch: 4370, Len of Training loss: 18, Average loss: 1.0365587539143033\n",
      "Len of Validation loss: 54, Average loss: 3.586267865366406\n",
      "Epoch: 4371, Len of Training loss: 18, Average loss: 1.063307950894038\n",
      "Len of Validation loss: 54, Average loss: 3.427828526055371\n",
      "Epoch: 4372, Len of Training loss: 18, Average loss: 1.0676152176327176\n",
      "Len of Validation loss: 54, Average loss: 3.405791730792434\n",
      "Epoch: 4373, Len of Training loss: 18, Average loss: 0.9731894036134084\n",
      "Len of Validation loss: 54, Average loss: 3.275634396959234\n",
      "Epoch: 4374, Len of Training loss: 18, Average loss: 0.9765260020891825\n",
      "Len of Validation loss: 54, Average loss: 3.475281059741974\n",
      "Epoch: 4375, Len of Training loss: 18, Average loss: 0.9338542057408227\n",
      "Len of Validation loss: 54, Average loss: 3.234179902959753\n",
      "Epoch: 4376, Len of Training loss: 18, Average loss: 0.9915424187978109\n",
      "Len of Validation loss: 54, Average loss: 3.679882021965804\n",
      "Epoch: 4377, Len of Training loss: 18, Average loss: 1.0342443684736888\n",
      "Len of Validation loss: 54, Average loss: 3.33151595570423\n",
      "Epoch: 4378, Len of Training loss: 18, Average loss: 0.9532153871324327\n",
      "Len of Validation loss: 54, Average loss: 3.380144606033961\n",
      "Epoch: 4379, Len of Training loss: 18, Average loss: 0.8931375841299692\n",
      "Len of Validation loss: 54, Average loss: 3.2278885786180145\n",
      "Epoch: 4380, Len of Training loss: 18, Average loss: 0.8646759490172068\n",
      "Len of Validation loss: 54, Average loss: 3.4742936193943024\n",
      "Epoch: 4381, Len of Training loss: 18, Average loss: 0.8875810835096571\n",
      "Len of Validation loss: 54, Average loss: 3.5469256098623627\n",
      "Epoch: 4382, Len of Training loss: 18, Average loss: 0.9535290598869324\n",
      "Len of Validation loss: 54, Average loss: 3.3691401426438934\n",
      "Epoch: 4383, Len of Training loss: 18, Average loss: 0.9580818679597642\n",
      "Len of Validation loss: 54, Average loss: 3.488155061448062\n",
      "Epoch: 4384, Len of Training loss: 18, Average loss: 0.9467825227313571\n",
      "Len of Validation loss: 54, Average loss: 3.4813594310371965\n",
      "Epoch: 4385, Len of Training loss: 18, Average loss: 0.9818530678749084\n",
      "Len of Validation loss: 54, Average loss: 3.307414984261548\n",
      "Epoch: 4386, Len of Training loss: 18, Average loss: 0.9173167016771104\n",
      "Len of Validation loss: 54, Average loss: 3.377650829376998\n",
      "Epoch: 4387, Len of Training loss: 18, Average loss: 0.9452482395701938\n",
      "Len of Validation loss: 54, Average loss: 3.3633211531020977\n",
      "Epoch: 4388, Len of Training loss: 18, Average loss: 0.9248291187816196\n",
      "Len of Validation loss: 54, Average loss: 3.377528210481008\n",
      "Epoch: 4389, Len of Training loss: 18, Average loss: 0.9595593942536248\n",
      "Len of Validation loss: 54, Average loss: 3.4340284014189684\n",
      "Epoch: 4390, Len of Training loss: 18, Average loss: 1.0109054015742407\n",
      "Len of Validation loss: 54, Average loss: 3.608377148707708\n",
      "Epoch: 4391, Len of Training loss: 18, Average loss: 1.0066763228840299\n",
      "Len of Validation loss: 54, Average loss: 3.3986481339843184\n",
      "Epoch: 4392, Len of Training loss: 18, Average loss: 1.0263813568486109\n",
      "Len of Validation loss: 54, Average loss: 3.4568876800713717\n",
      "Epoch: 4393, Len of Training loss: 18, Average loss: 1.003526724047131\n",
      "Len of Validation loss: 54, Average loss: 3.3614338051389763\n",
      "Epoch: 4394, Len of Training loss: 18, Average loss: 1.0788675083054438\n",
      "Len of Validation loss: 54, Average loss: 3.573649799382245\n",
      "Epoch: 4395, Len of Training loss: 18, Average loss: 1.1582599216037326\n",
      "Len of Validation loss: 54, Average loss: 3.379931769989155\n",
      "Epoch: 4396, Len of Training loss: 18, Average loss: 1.0813773804240756\n",
      "Len of Validation loss: 54, Average loss: 3.5077397701916873\n",
      "Epoch: 4397, Len of Training loss: 18, Average loss: 1.160997211933136\n",
      "Len of Validation loss: 54, Average loss: 3.38945902939196\n",
      "Epoch: 4398, Len of Training loss: 18, Average loss: 1.1120516955852509\n",
      "Len of Validation loss: 54, Average loss: 3.2838345114831573\n",
      "Epoch: 4399, Len of Training loss: 18, Average loss: 0.9630136092503866\n",
      "Len of Validation loss: 54, Average loss: 3.57652927769555\n",
      "Epoch: 4400, Len of Training loss: 18, Average loss: 0.9342984325355954\n",
      "Len of Validation loss: 54, Average loss: 3.4326876534356012\n",
      "Epoch: 4401, Len of Training loss: 18, Average loss: 0.9278854860199822\n",
      "Len of Validation loss: 54, Average loss: 3.425456440007245\n",
      "Epoch: 4402, Len of Training loss: 18, Average loss: 0.8557817108101315\n",
      "Len of Validation loss: 54, Average loss: 3.348174434017252\n",
      "Epoch: 4403, Len of Training loss: 18, Average loss: 0.8304013510545095\n",
      "Len of Validation loss: 54, Average loss: 3.1613995686725334\n",
      "Epoch: 4404, Len of Training loss: 18, Average loss: 0.8429906566937765\n",
      "Len of Validation loss: 54, Average loss: 3.30010395248731\n",
      "Epoch: 4405, Len of Training loss: 18, Average loss: 0.8469996088080936\n",
      "Len of Validation loss: 54, Average loss: 3.2690872936337083\n",
      "Epoch: 4406, Len of Training loss: 18, Average loss: 0.9427478346559737\n",
      "Len of Validation loss: 54, Average loss: 3.571036458015442\n",
      "Epoch: 4407, Len of Training loss: 18, Average loss: 0.9078115059269799\n",
      "Len of Validation loss: 54, Average loss: 3.5026065674093037\n",
      "Epoch: 4408, Len of Training loss: 18, Average loss: 0.939839416080051\n",
      "Len of Validation loss: 54, Average loss: 3.5034082708535372\n",
      "Epoch: 4409, Len of Training loss: 18, Average loss: 0.9405663112799326\n",
      "Len of Validation loss: 54, Average loss: 3.454878522290124\n",
      "Epoch: 4410, Len of Training loss: 18, Average loss: 1.1860620048311021\n",
      "Len of Validation loss: 54, Average loss: 3.693884256813261\n",
      "Epoch: 4411, Len of Training loss: 18, Average loss: 1.2595529291364882\n",
      "Len of Validation loss: 54, Average loss: 3.60194155242708\n",
      "Epoch: 4412, Len of Training loss: 18, Average loss: 1.0879257453812494\n",
      "Len of Validation loss: 54, Average loss: 3.4419571967036635\n",
      "Epoch: 4413, Len of Training loss: 18, Average loss: 1.0625220239162445\n",
      "Len of Validation loss: 54, Average loss: 3.2908658904057964\n",
      "Epoch: 4414, Len of Training loss: 18, Average loss: 0.9895762867397733\n",
      "Len of Validation loss: 54, Average loss: 3.5433125142697937\n",
      "Epoch: 4415, Len of Training loss: 18, Average loss: 0.935199605094062\n",
      "Len of Validation loss: 54, Average loss: 3.4817168337327464\n",
      "Epoch: 4416, Len of Training loss: 18, Average loss: 0.9360270102818807\n",
      "Len of Validation loss: 54, Average loss: 3.4359734003190643\n",
      "Epoch: 4417, Len of Training loss: 18, Average loss: 0.9324225121074252\n",
      "Len of Validation loss: 54, Average loss: 3.286609629789988\n",
      "Epoch: 4418, Len of Training loss: 18, Average loss: 0.9019417895211114\n",
      "Len of Validation loss: 54, Average loss: 3.3334572502860316\n",
      "Epoch: 4419, Len of Training loss: 18, Average loss: 0.9549498524930742\n",
      "Len of Validation loss: 54, Average loss: 3.425945325030221\n",
      "Epoch: 4420, Len of Training loss: 18, Average loss: 0.9513126048776839\n",
      "Len of Validation loss: 54, Average loss: 3.457677021070763\n",
      "Epoch: 4421, Len of Training loss: 18, Average loss: 0.9902012844880422\n",
      "Len of Validation loss: 54, Average loss: 3.3648326926761203\n",
      "Epoch: 4422, Len of Training loss: 18, Average loss: 1.2190951175159879\n",
      "Len of Validation loss: 54, Average loss: 3.5444442475283586\n",
      "Epoch: 4423, Len of Training loss: 18, Average loss: 1.3973329663276672\n",
      "Len of Validation loss: 54, Average loss: 3.581403299614235\n",
      "Epoch: 4424, Len of Training loss: 18, Average loss: 1.1037527951929305\n",
      "Len of Validation loss: 54, Average loss: 3.7131531591768616\n",
      "Epoch: 4425, Len of Training loss: 18, Average loss: 1.0214387145307329\n",
      "Len of Validation loss: 54, Average loss: 3.4368007293453924\n",
      "Epoch: 4426, Len of Training loss: 18, Average loss: 1.1020329528384738\n",
      "Len of Validation loss: 54, Average loss: 3.4564183641363075\n",
      "Epoch: 4427, Len of Training loss: 18, Average loss: 1.004198173681895\n",
      "Len of Validation loss: 54, Average loss: 3.655502971675661\n",
      "Epoch: 4428, Len of Training loss: 18, Average loss: 1.1211685207155015\n",
      "Len of Validation loss: 54, Average loss: 3.5007442434628806\n",
      "Epoch: 4429, Len of Training loss: 18, Average loss: 1.256521119011773\n",
      "Len of Validation loss: 54, Average loss: 3.459919096143157\n",
      "Epoch: 4430, Len of Training loss: 18, Average loss: 1.1560238533549838\n",
      "Len of Validation loss: 54, Average loss: 3.521340212336293\n",
      "Epoch: 4431, Len of Training loss: 18, Average loss: 1.0397802392641704\n",
      "Len of Validation loss: 54, Average loss: 3.3583268512178353\n",
      "Epoch: 4432, Len of Training loss: 18, Average loss: 0.9981805880864462\n",
      "Len of Validation loss: 54, Average loss: 3.411197579569287\n",
      "Epoch: 4433, Len of Training loss: 18, Average loss: 0.9854655464490255\n",
      "Len of Validation loss: 54, Average loss: 3.412032472866553\n",
      "Epoch: 4434, Len of Training loss: 18, Average loss: 0.956885771618949\n",
      "Len of Validation loss: 54, Average loss: 3.50231342514356\n",
      "Epoch: 4435, Len of Training loss: 18, Average loss: 1.0110032558441162\n",
      "Len of Validation loss: 54, Average loss: 3.297789866173709\n",
      "Epoch: 4436, Len of Training loss: 18, Average loss: 0.8787566920121511\n",
      "Len of Validation loss: 54, Average loss: 3.5158856875366635\n",
      "Epoch: 4437, Len of Training loss: 18, Average loss: 1.033140864637163\n",
      "Len of Validation loss: 54, Average loss: 3.5012557561750763\n",
      "Epoch: 4438, Len of Training loss: 18, Average loss: 0.9493094881375631\n",
      "Len of Validation loss: 54, Average loss: 3.3452688256899514\n",
      "Epoch: 4439, Len of Training loss: 18, Average loss: 0.9930958284272088\n",
      "Len of Validation loss: 54, Average loss: 3.395256264342202\n",
      "Epoch: 4440, Len of Training loss: 18, Average loss: 1.073424498240153\n",
      "Len of Validation loss: 54, Average loss: 3.3685636233400413\n",
      "Epoch: 4441, Len of Training loss: 18, Average loss: 0.9640739957491556\n",
      "Len of Validation loss: 54, Average loss: 3.546502756851691\n",
      "Epoch: 4442, Len of Training loss: 18, Average loss: 0.9131979611184862\n",
      "Len of Validation loss: 54, Average loss: 3.339943995078405\n",
      "Epoch: 4443, Len of Training loss: 18, Average loss: 0.9009621640046438\n",
      "Len of Validation loss: 54, Average loss: 3.3988101957020938\n",
      "Epoch: 4444, Len of Training loss: 18, Average loss: 0.8852680457962884\n",
      "Len of Validation loss: 54, Average loss: 3.3180137640900083\n",
      "Epoch: 4445, Len of Training loss: 18, Average loss: 0.9754200412167443\n",
      "Len of Validation loss: 54, Average loss: 3.1724890304936304\n",
      "Epoch: 4446, Len of Training loss: 18, Average loss: 0.9323515527778201\n",
      "Len of Validation loss: 54, Average loss: 3.526096784406238\n",
      "Epoch: 4447, Len of Training loss: 18, Average loss: 1.137528853283988\n",
      "Len of Validation loss: 54, Average loss: 3.3654930039688393\n",
      "Epoch: 4448, Len of Training loss: 18, Average loss: 1.1281682418452368\n",
      "Len of Validation loss: 54, Average loss: 3.5566941223762654\n",
      "Epoch: 4449, Len of Training loss: 18, Average loss: 1.052199085553487\n",
      "Len of Validation loss: 54, Average loss: 3.420628672396695\n",
      "Epoch: 4450, Len of Training loss: 18, Average loss: 1.0480507512887318\n",
      "Len of Validation loss: 54, Average loss: 3.410269554014559\n",
      "Epoch: 4451, Len of Training loss: 18, Average loss: 0.952098419268926\n",
      "Len of Validation loss: 54, Average loss: 3.294386723527202\n",
      "Epoch: 4452, Len of Training loss: 18, Average loss: 0.9274350768989987\n",
      "Len of Validation loss: 54, Average loss: 3.405059059460958\n",
      "Epoch: 4453, Len of Training loss: 18, Average loss: 0.8562341266208224\n",
      "Len of Validation loss: 54, Average loss: 3.2594035974255315\n",
      "Epoch: 4454, Len of Training loss: 18, Average loss: 0.8890111280812157\n",
      "Len of Validation loss: 54, Average loss: 3.2866722731678575\n",
      "Epoch: 4455, Len of Training loss: 18, Average loss: 0.9993061621983846\n",
      "Len of Validation loss: 54, Average loss: 3.519798692729738\n",
      "Epoch: 4456, Len of Training loss: 18, Average loss: 1.1208528180917103\n",
      "Len of Validation loss: 54, Average loss: 3.44117722136003\n",
      "Epoch: 4457, Len of Training loss: 18, Average loss: 1.007581353187561\n",
      "Len of Validation loss: 54, Average loss: 3.434732386359462\n",
      "Epoch: 4458, Len of Training loss: 18, Average loss: 0.9626455704371134\n",
      "Len of Validation loss: 54, Average loss: 3.83989625175794\n",
      "Epoch: 4459, Len of Training loss: 18, Average loss: 0.9737575219737159\n",
      "Len of Validation loss: 54, Average loss: 3.319976528485616\n",
      "Epoch: 4460, Len of Training loss: 18, Average loss: 0.9743751254346635\n",
      "Len of Validation loss: 54, Average loss: 3.4464247988329992\n",
      "Epoch: 4461, Len of Training loss: 18, Average loss: 0.8901581168174744\n",
      "Len of Validation loss: 54, Average loss: 3.365803259390372\n",
      "Epoch: 4462, Len of Training loss: 18, Average loss: 0.9840627875592973\n",
      "Len of Validation loss: 54, Average loss: 3.3866233461432986\n",
      "Epoch: 4463, Len of Training loss: 18, Average loss: 0.9130882951948378\n",
      "Len of Validation loss: 54, Average loss: 3.3278494996053203\n",
      "Epoch: 4464, Len of Training loss: 18, Average loss: 0.879325701130761\n",
      "Len of Validation loss: 54, Average loss: 3.2871690381456307\n",
      "Epoch: 4465, Len of Training loss: 18, Average loss: 0.9660616417725881\n",
      "Len of Validation loss: 54, Average loss: 3.377533354141094\n",
      "Epoch: 4466, Len of Training loss: 18, Average loss: 1.1273084713353052\n",
      "Len of Validation loss: 54, Average loss: 3.3596920801533594\n",
      "Epoch: 4467, Len of Training loss: 18, Average loss: 1.040967520740297\n",
      "Len of Validation loss: 54, Average loss: 3.5493996695235923\n",
      "Epoch: 4468, Len of Training loss: 18, Average loss: 1.3500639332665338\n",
      "Len of Validation loss: 54, Average loss: 3.581658998021373\n",
      "Epoch: 4469, Len of Training loss: 18, Average loss: 1.3403401242362127\n",
      "Len of Validation loss: 54, Average loss: 3.478303598033057\n",
      "Epoch: 4470, Len of Training loss: 18, Average loss: 1.2071573734283447\n",
      "Len of Validation loss: 54, Average loss: 3.4657233357429504\n",
      "Epoch: 4471, Len of Training loss: 18, Average loss: 1.111481507619222\n",
      "Len of Validation loss: 54, Average loss: 3.3869263529777527\n",
      "Epoch: 4472, Len of Training loss: 18, Average loss: 1.1040779683325026\n",
      "Len of Validation loss: 54, Average loss: 3.504994628606019\n",
      "Epoch: 4473, Len of Training loss: 18, Average loss: 1.159801506333881\n",
      "Len of Validation loss: 54, Average loss: 3.4821683720306114\n",
      "Epoch: 4474, Len of Training loss: 18, Average loss: 1.048596183458964\n",
      "Len of Validation loss: 54, Average loss: 3.469301723771625\n",
      "Epoch: 4475, Len of Training loss: 18, Average loss: 0.9387473695807986\n",
      "Len of Validation loss: 54, Average loss: 3.322609086831411\n",
      "Epoch: 4476, Len of Training loss: 18, Average loss: 0.9234445492426554\n",
      "Len of Validation loss: 54, Average loss: 3.4486114283402762\n",
      "Epoch: 4477, Len of Training loss: 18, Average loss: 1.0812436673376296\n",
      "Len of Validation loss: 54, Average loss: 3.3852697169339216\n",
      "Epoch: 4478, Len of Training loss: 18, Average loss: 1.0778316060702007\n",
      "Len of Validation loss: 54, Average loss: 3.607126655402007\n",
      "Epoch: 4479, Len of Training loss: 18, Average loss: 1.0778020686573453\n",
      "Len of Validation loss: 54, Average loss: 3.616165037508364\n",
      "Epoch: 4480, Len of Training loss: 18, Average loss: 0.9972619381215837\n",
      "Len of Validation loss: 54, Average loss: 3.3177640747140953\n",
      "Epoch: 4481, Len of Training loss: 18, Average loss: 0.8953107429875268\n",
      "Len of Validation loss: 54, Average loss: 3.347646184541561\n",
      "Epoch: 4482, Len of Training loss: 18, Average loss: 0.8698336415820651\n",
      "Len of Validation loss: 54, Average loss: 3.6481925745805106\n",
      "Epoch: 4483, Len of Training loss: 18, Average loss: 0.9190396169821421\n",
      "Len of Validation loss: 54, Average loss: 3.3928608695665994\n",
      "Epoch: 4484, Len of Training loss: 18, Average loss: 0.9461568097273508\n",
      "Len of Validation loss: 54, Average loss: 3.5015633746429726\n",
      "Epoch: 4485, Len of Training loss: 18, Average loss: 0.94900068309572\n",
      "Len of Validation loss: 54, Average loss: 3.561761732454653\n",
      "Epoch: 4486, Len of Training loss: 18, Average loss: 0.9555972781446245\n",
      "Len of Validation loss: 54, Average loss: 3.509866225498694\n",
      "Epoch: 4487, Len of Training loss: 18, Average loss: 0.9178392456637489\n",
      "Len of Validation loss: 54, Average loss: 3.277506678192704\n",
      "Epoch: 4488, Len of Training loss: 18, Average loss: 0.9380582968393961\n",
      "Len of Validation loss: 54, Average loss: 3.5513108284385115\n",
      "Epoch: 4489, Len of Training loss: 18, Average loss: 0.9909345540735457\n",
      "Len of Validation loss: 54, Average loss: 3.482417041504825\n",
      "Epoch: 4490, Len of Training loss: 18, Average loss: 0.939520173602634\n",
      "Len of Validation loss: 54, Average loss: 3.3145067780106157\n",
      "Epoch: 4491, Len of Training loss: 18, Average loss: 0.9033517340819041\n",
      "Len of Validation loss: 54, Average loss: 3.4499927130010395\n",
      "Epoch: 4492, Len of Training loss: 18, Average loss: 0.9104342924224006\n",
      "Len of Validation loss: 54, Average loss: 3.6579755268715046\n",
      "Epoch: 4493, Len of Training loss: 18, Average loss: 1.0068302651246388\n",
      "Len of Validation loss: 54, Average loss: 3.4444649009792894\n",
      "Epoch: 4494, Len of Training loss: 18, Average loss: 1.2992109623220232\n",
      "Len of Validation loss: 54, Average loss: 3.8648333107983626\n",
      "Epoch: 4495, Len of Training loss: 18, Average loss: 1.1126224497954051\n",
      "Len of Validation loss: 54, Average loss: 3.4774953722953796\n",
      "Epoch: 4496, Len of Training loss: 18, Average loss: 1.021975686152776\n",
      "Len of Validation loss: 54, Average loss: 3.5070805748303733\n",
      "Epoch: 4497, Len of Training loss: 18, Average loss: 0.9613668421904246\n",
      "Len of Validation loss: 54, Average loss: 3.358731711352313\n",
      "Epoch: 4498, Len of Training loss: 18, Average loss: 0.9109492500623068\n",
      "Len of Validation loss: 54, Average loss: 3.402609207012035\n",
      "Epoch: 4499, Len of Training loss: 18, Average loss: 0.8885905179712508\n",
      "Len of Validation loss: 54, Average loss: 3.3846990212246224\n",
      "Epoch: 4500, Len of Training loss: 18, Average loss: 0.8962100578678979\n",
      "Len of Validation loss: 54, Average loss: 3.470517458739104\n",
      "Epoch: 4501, Len of Training loss: 18, Average loss: 0.9305383231904771\n",
      "Len of Validation loss: 54, Average loss: 3.44329051066328\n",
      "Epoch: 4502, Len of Training loss: 18, Average loss: 0.9377798868550195\n",
      "Len of Validation loss: 54, Average loss: 3.3091377075071686\n",
      "Epoch: 4503, Len of Training loss: 18, Average loss: 0.8849574128786722\n",
      "Len of Validation loss: 54, Average loss: 3.373480248230475\n",
      "Epoch: 4504, Len of Training loss: 18, Average loss: 0.8553393383820852\n",
      "Len of Validation loss: 54, Average loss: 3.551120158698824\n",
      "Epoch: 4505, Len of Training loss: 18, Average loss: 0.8467141588528951\n",
      "Len of Validation loss: 54, Average loss: 3.325710706136845\n",
      "Epoch: 4506, Len of Training loss: 18, Average loss: 0.8639839424027337\n",
      "Len of Validation loss: 54, Average loss: 3.3045470913251243\n",
      "Epoch: 4507, Len of Training loss: 18, Average loss: 0.8419432176484002\n",
      "Len of Validation loss: 54, Average loss: 3.6956389899607056\n",
      "Epoch: 4508, Len of Training loss: 18, Average loss: 0.9722201890415616\n",
      "Len of Validation loss: 54, Average loss: 3.354269396375727\n",
      "Epoch: 4509, Len of Training loss: 18, Average loss: 1.0573769973384008\n",
      "Len of Validation loss: 54, Average loss: 3.40544565960213\n",
      "Epoch: 4510, Len of Training loss: 18, Average loss: 0.9471263223224216\n",
      "Len of Validation loss: 54, Average loss: 3.399980104631848\n",
      "Epoch: 4511, Len of Training loss: 18, Average loss: 0.9099005824989743\n",
      "Len of Validation loss: 54, Average loss: 3.2844289000387543\n",
      "Epoch: 4512, Len of Training loss: 18, Average loss: 1.0014306240611606\n",
      "Len of Validation loss: 54, Average loss: 3.451732156453309\n",
      "Epoch: 4513, Len of Training loss: 18, Average loss: 0.9278876880804697\n",
      "Len of Validation loss: 54, Average loss: 3.3333150148391724\n",
      "Epoch: 4514, Len of Training loss: 18, Average loss: 0.9078704251183404\n",
      "Len of Validation loss: 54, Average loss: 3.277085840702057\n",
      "Epoch: 4515, Len of Training loss: 18, Average loss: 0.9196079936292436\n",
      "Len of Validation loss: 54, Average loss: 3.3635361393292746\n",
      "Epoch: 4516, Len of Training loss: 18, Average loss: 0.8876985808213552\n",
      "Len of Validation loss: 54, Average loss: 3.4075891645970167\n",
      "Epoch: 4517, Len of Training loss: 18, Average loss: 0.8902421759234534\n",
      "Len of Validation loss: 54, Average loss: 3.4395318781888045\n",
      "Epoch: 4518, Len of Training loss: 18, Average loss: 0.9766582681073083\n",
      "Len of Validation loss: 54, Average loss: 3.4403324988153248\n",
      "Epoch: 4519, Len of Training loss: 18, Average loss: 1.1266390085220337\n",
      "Len of Validation loss: 54, Average loss: 3.6321715734623097\n",
      "Epoch: 4520, Len of Training loss: 18, Average loss: 1.2260802785555522\n",
      "Len of Validation loss: 54, Average loss: 3.5154742201169333\n",
      "Epoch: 4521, Len of Training loss: 18, Average loss: 1.1106499201721616\n",
      "Len of Validation loss: 54, Average loss: 3.274539898943018\n",
      "Epoch: 4522, Len of Training loss: 18, Average loss: 1.0579911934004889\n",
      "Len of Validation loss: 54, Average loss: 3.613883445660273\n",
      "Epoch: 4523, Len of Training loss: 18, Average loss: 0.9455494781335195\n",
      "Len of Validation loss: 54, Average loss: 3.5675979013796204\n",
      "Epoch: 4524, Len of Training loss: 18, Average loss: 0.9276751412285699\n",
      "Len of Validation loss: 54, Average loss: 3.3887885864134186\n",
      "Epoch: 4525, Len of Training loss: 18, Average loss: 0.89687294099066\n",
      "Len of Validation loss: 54, Average loss: 3.4741194402730025\n",
      "Epoch: 4526, Len of Training loss: 18, Average loss: 0.9142116639349196\n",
      "Len of Validation loss: 54, Average loss: 3.4549779483565577\n",
      "Epoch: 4527, Len of Training loss: 18, Average loss: 0.9267467690838708\n",
      "Len of Validation loss: 54, Average loss: 3.274463909643668\n",
      "Epoch: 4528, Len of Training loss: 18, Average loss: 1.0177859001689487\n",
      "Len of Validation loss: 54, Average loss: 3.4697771193804563\n",
      "Epoch: 4529, Len of Training loss: 18, Average loss: 1.055192510286967\n",
      "Len of Validation loss: 54, Average loss: 3.4493054957301528\n",
      "Epoch: 4530, Len of Training loss: 18, Average loss: 1.0655834509266748\n",
      "Len of Validation loss: 54, Average loss: 3.469867980038678\n",
      "Epoch: 4531, Len of Training loss: 18, Average loss: 1.0565616654025183\n",
      "Len of Validation loss: 54, Average loss: 3.552279891791167\n",
      "Epoch: 4532, Len of Training loss: 18, Average loss: 0.979404393169615\n",
      "Len of Validation loss: 54, Average loss: 3.269471400313907\n",
      "Epoch: 4533, Len of Training loss: 18, Average loss: 0.9290655387772454\n",
      "Len of Validation loss: 54, Average loss: 3.5029869145817227\n",
      "Epoch: 4534, Len of Training loss: 18, Average loss: 0.8557396100627052\n",
      "Len of Validation loss: 54, Average loss: 3.419551439859249\n",
      "Epoch: 4535, Len of Training loss: 18, Average loss: 0.928247763050927\n",
      "Len of Validation loss: 54, Average loss: 3.195147398445341\n",
      "Epoch: 4536, Len of Training loss: 18, Average loss: 0.9202168451415168\n",
      "Len of Validation loss: 54, Average loss: 3.4908057782385082\n",
      "Epoch: 4537, Len of Training loss: 18, Average loss: 0.9977878563933902\n",
      "Len of Validation loss: 54, Average loss: 3.455772046689634\n",
      "Epoch: 4538, Len of Training loss: 18, Average loss: 1.027951757113139\n",
      "Len of Validation loss: 54, Average loss: 3.7326134619889437\n",
      "Epoch: 4539, Len of Training loss: 18, Average loss: 1.204126947455936\n",
      "Len of Validation loss: 54, Average loss: 3.5278283688757153\n",
      "Epoch: 4540, Len of Training loss: 18, Average loss: 1.0696422557036083\n",
      "Len of Validation loss: 54, Average loss: 3.351645705876527\n",
      "Epoch: 4541, Len of Training loss: 18, Average loss: 0.9946539203325907\n",
      "Len of Validation loss: 54, Average loss: 3.2859803482338235\n",
      "Epoch: 4542, Len of Training loss: 18, Average loss: 0.9599250786834292\n",
      "Len of Validation loss: 54, Average loss: 3.4138368323997215\n",
      "Epoch: 4543, Len of Training loss: 18, Average loss: 0.9070284565289816\n",
      "Len of Validation loss: 54, Average loss: 3.5076258922064745\n",
      "Epoch: 4544, Len of Training loss: 18, Average loss: 0.9864878753821055\n",
      "Len of Validation loss: 54, Average loss: 3.4147158044355885\n",
      "Epoch: 4545, Len of Training loss: 18, Average loss: 0.9315354327360789\n",
      "Len of Validation loss: 54, Average loss: 3.3274392353163824\n",
      "Epoch: 4546, Len of Training loss: 18, Average loss: 0.9552312692006429\n",
      "Len of Validation loss: 54, Average loss: 3.433023941737634\n",
      "Epoch: 4547, Len of Training loss: 18, Average loss: 0.9662003086672889\n",
      "Len of Validation loss: 54, Average loss: 3.621188579886048\n",
      "Epoch: 4548, Len of Training loss: 18, Average loss: 1.0477912823359172\n",
      "Len of Validation loss: 54, Average loss: 3.5198403000831604\n",
      "Epoch: 4549, Len of Training loss: 18, Average loss: 1.0380291773213282\n",
      "Len of Validation loss: 54, Average loss: 3.369048493879813\n",
      "Epoch: 4550, Len of Training loss: 18, Average loss: 0.989302224583096\n",
      "Len of Validation loss: 54, Average loss: 3.4418257695657237\n",
      "Epoch: 4551, Len of Training loss: 18, Average loss: 0.9887999395529429\n",
      "Len of Validation loss: 54, Average loss: 3.799743374188741\n",
      "Epoch: 4552, Len of Training loss: 18, Average loss: 1.4481107857492235\n",
      "Len of Validation loss: 54, Average loss: 4.00101320611106\n",
      "Epoch: 4553, Len of Training loss: 18, Average loss: 2.2782554229100547\n",
      "Len of Validation loss: 54, Average loss: 4.140536456196396\n",
      "Epoch: 4554, Len of Training loss: 18, Average loss: 1.9693402780426874\n",
      "Len of Validation loss: 54, Average loss: 4.104058563709259\n",
      "Epoch: 4555, Len of Training loss: 18, Average loss: 1.5583180454042223\n",
      "Len of Validation loss: 54, Average loss: 4.01153544805668\n",
      "Epoch: 4556, Len of Training loss: 18, Average loss: 2.9746860928005643\n",
      "Len of Validation loss: 54, Average loss: 4.604950677465509\n",
      "Epoch: 4557, Len of Training loss: 18, Average loss: 2.5755118396547108\n",
      "Len of Validation loss: 54, Average loss: 4.309647487269507\n",
      "Epoch: 4558, Len of Training loss: 18, Average loss: 2.1114337378078036\n",
      "Len of Validation loss: 54, Average loss: 3.8982529176606073\n",
      "Epoch: 4559, Len of Training loss: 18, Average loss: 1.6736260056495667\n",
      "Len of Validation loss: 54, Average loss: 3.7291098722705134\n",
      "Epoch: 4560, Len of Training loss: 18, Average loss: 1.452170153458913\n",
      "Len of Validation loss: 54, Average loss: 3.516746152330328\n",
      "Epoch: 4561, Len of Training loss: 18, Average loss: 1.361026366551717\n",
      "Len of Validation loss: 54, Average loss: 3.6847553473931773\n",
      "Epoch: 4562, Len of Training loss: 18, Average loss: 1.3315001792377896\n",
      "Len of Validation loss: 54, Average loss: 3.8727425866656833\n",
      "Epoch: 4563, Len of Training loss: 18, Average loss: 1.358043180571662\n",
      "Len of Validation loss: 54, Average loss: 3.642206828903269\n",
      "Epoch: 4564, Len of Training loss: 18, Average loss: 1.3550907969474792\n",
      "Len of Validation loss: 54, Average loss: 3.720614548082705\n",
      "Epoch: 4565, Len of Training loss: 18, Average loss: 1.2276537517706554\n",
      "Len of Validation loss: 54, Average loss: 3.4191230513431408\n",
      "Epoch: 4566, Len of Training loss: 18, Average loss: 1.0709548029634688\n",
      "Len of Validation loss: 54, Average loss: 3.458228798928084\n",
      "Epoch: 4567, Len of Training loss: 18, Average loss: 1.0031823184755113\n",
      "Len of Validation loss: 54, Average loss: 3.357448474124626\n",
      "Epoch: 4568, Len of Training loss: 18, Average loss: 1.0832835369639926\n",
      "Len of Validation loss: 54, Average loss: 3.3856017655796475\n",
      "Epoch: 4569, Len of Training loss: 18, Average loss: 1.1267456048064761\n",
      "Len of Validation loss: 54, Average loss: 3.58054268249759\n",
      "Epoch: 4570, Len of Training loss: 18, Average loss: 1.116639604171117\n",
      "Len of Validation loss: 54, Average loss: 3.456676251358456\n",
      "Epoch: 4571, Len of Training loss: 18, Average loss: 1.0224135087596045\n",
      "Len of Validation loss: 54, Average loss: 3.4377742928487285\n",
      "Epoch: 4572, Len of Training loss: 18, Average loss: 1.0101668768458896\n",
      "Len of Validation loss: 54, Average loss: 3.254609096933294\n",
      "Epoch: 4573, Len of Training loss: 18, Average loss: 1.0130540761682723\n",
      "Len of Validation loss: 54, Average loss: 3.359631074799432\n",
      "Epoch: 4574, Len of Training loss: 18, Average loss: 1.2380108667744532\n",
      "Len of Validation loss: 54, Average loss: 3.6583486420136913\n",
      "Epoch: 4575, Len of Training loss: 18, Average loss: 1.3416499098141987\n",
      "Len of Validation loss: 54, Average loss: 3.370388949358905\n",
      "Epoch: 4576, Len of Training loss: 18, Average loss: 1.1108806961112552\n",
      "Len of Validation loss: 54, Average loss: 3.5496202574835882\n",
      "Epoch: 4577, Len of Training loss: 18, Average loss: 1.0204459296332464\n",
      "Len of Validation loss: 54, Average loss: 3.4498950805928974\n",
      "Epoch: 4578, Len of Training loss: 18, Average loss: 1.0986266103055742\n",
      "Len of Validation loss: 54, Average loss: 3.528528956351457\n",
      "Epoch: 4579, Len of Training loss: 18, Average loss: 1.2423994806077745\n",
      "Len of Validation loss: 54, Average loss: 3.6897054932735585\n",
      "Epoch: 4580, Len of Training loss: 18, Average loss: 1.16975004474322\n",
      "Len of Validation loss: 54, Average loss: 3.5157754641992076\n",
      "Epoch: 4581, Len of Training loss: 18, Average loss: 1.0194130539894104\n",
      "Len of Validation loss: 54, Average loss: 3.437824152134083\n",
      "Epoch: 4582, Len of Training loss: 18, Average loss: 0.9861678447988298\n",
      "Len of Validation loss: 54, Average loss: 3.5026603517708956\n",
      "Epoch: 4583, Len of Training loss: 18, Average loss: 0.973424474398295\n",
      "Len of Validation loss: 54, Average loss: 3.2821317105381578\n",
      "Epoch: 4584, Len of Training loss: 18, Average loss: 0.9110830492443509\n",
      "Len of Validation loss: 54, Average loss: 3.523365785678228\n",
      "Epoch: 4585, Len of Training loss: 18, Average loss: 0.8924805786874559\n",
      "Len of Validation loss: 54, Average loss: 3.6907551332756325\n",
      "Epoch: 4586, Len of Training loss: 18, Average loss: 0.8879681825637817\n",
      "Len of Validation loss: 54, Average loss: 3.4768285387092166\n",
      "Epoch: 4587, Len of Training loss: 18, Average loss: 0.9321342408657074\n",
      "Len of Validation loss: 54, Average loss: 3.28725767466757\n",
      "Epoch: 4588, Len of Training loss: 18, Average loss: 0.8968538575702243\n",
      "Len of Validation loss: 54, Average loss: 3.2527140080928802\n",
      "Epoch: 4589, Len of Training loss: 18, Average loss: 0.9169389671749539\n",
      "Len of Validation loss: 54, Average loss: 3.4566507273250155\n",
      "Epoch: 4590, Len of Training loss: 18, Average loss: 1.0300386349360149\n",
      "Len of Validation loss: 54, Average loss: 3.2856698212800204\n",
      "Epoch: 4591, Len of Training loss: 18, Average loss: 0.9223414096567366\n",
      "Len of Validation loss: 54, Average loss: 3.5293340992044517\n",
      "Epoch: 4592, Len of Training loss: 18, Average loss: 0.9306892686420016\n",
      "Len of Validation loss: 54, Average loss: 3.443786523960255\n",
      "Epoch: 4593, Len of Training loss: 18, Average loss: 0.9502047035429213\n",
      "Len of Validation loss: 54, Average loss: 3.436055522274088\n",
      "Epoch: 4594, Len of Training loss: 18, Average loss: 1.039771791961458\n",
      "Len of Validation loss: 54, Average loss: 3.5146657316773027\n",
      "Epoch: 4595, Len of Training loss: 18, Average loss: 1.0072104036808014\n",
      "Len of Validation loss: 54, Average loss: 3.419101471150363\n",
      "Epoch: 4596, Len of Training loss: 18, Average loss: 1.0012400216526456\n",
      "Len of Validation loss: 54, Average loss: 3.403085134647511\n",
      "Epoch: 4597, Len of Training loss: 18, Average loss: 1.308245774772432\n",
      "Len of Validation loss: 54, Average loss: 3.62438651808986\n",
      "Epoch: 4598, Len of Training loss: 18, Average loss: 1.3305400941107008\n",
      "Len of Validation loss: 54, Average loss: 3.555744562987928\n",
      "Epoch: 4599, Len of Training loss: 18, Average loss: 1.1609806716442108\n",
      "Len of Validation loss: 54, Average loss: 3.622000295806814\n",
      "Epoch: 4600, Len of Training loss: 18, Average loss: 1.107021517223782\n",
      "Len of Validation loss: 54, Average loss: 3.398221955255226\n",
      "Epoch: 4601, Len of Training loss: 18, Average loss: 1.0757021771536932\n",
      "Len of Validation loss: 54, Average loss: 3.328891470476433\n",
      "Epoch: 4602, Len of Training loss: 18, Average loss: 1.003050880299674\n",
      "Len of Validation loss: 54, Average loss: 3.446559404885327\n",
      "Epoch: 4603, Len of Training loss: 18, Average loss: 1.1527603997124567\n",
      "Len of Validation loss: 54, Average loss: 3.712112272227252\n",
      "Epoch: 4604, Len of Training loss: 18, Average loss: 1.127843290567398\n",
      "Len of Validation loss: 54, Average loss: 3.6130985065742776\n",
      "Epoch: 4605, Len of Training loss: 18, Average loss: 1.0455119378036923\n",
      "Len of Validation loss: 54, Average loss: 3.318075777203948\n",
      "Epoch: 4606, Len of Training loss: 18, Average loss: 1.045306921005249\n",
      "Len of Validation loss: 54, Average loss: 3.5045287752593004\n",
      "Epoch: 4607, Len of Training loss: 18, Average loss: 0.9818849696053399\n",
      "Len of Validation loss: 54, Average loss: 3.552102792042273\n",
      "Epoch: 4608, Len of Training loss: 18, Average loss: 0.9595228168699477\n",
      "Len of Validation loss: 54, Average loss: 3.365777126065007\n",
      "Epoch: 4609, Len of Training loss: 18, Average loss: 0.9170245793130662\n",
      "Len of Validation loss: 54, Average loss: 3.4364872111214533\n",
      "Epoch: 4610, Len of Training loss: 18, Average loss: 0.9309168855349222\n",
      "Len of Validation loss: 54, Average loss: 3.401637781549383\n",
      "Epoch: 4611, Len of Training loss: 18, Average loss: 1.0315660138924916\n",
      "Len of Validation loss: 54, Average loss: 3.2999071147706776\n",
      "Epoch: 4612, Len of Training loss: 18, Average loss: 0.9124426808622148\n",
      "Len of Validation loss: 54, Average loss: 3.251839139947185\n",
      "Epoch: 4613, Len of Training loss: 18, Average loss: 0.8532727857430776\n",
      "Len of Validation loss: 54, Average loss: 3.3548159025333546\n",
      "Epoch: 4614, Len of Training loss: 18, Average loss: 0.8165432976351844\n",
      "Len of Validation loss: 54, Average loss: 3.2074976850439003\n",
      "Epoch: 4615, Len of Training loss: 18, Average loss: 0.8740454945299361\n",
      "Len of Validation loss: 54, Average loss: 3.499318503671222\n",
      "Epoch: 4616, Len of Training loss: 18, Average loss: 0.930312348736657\n",
      "Len of Validation loss: 54, Average loss: 3.1957730739204973\n",
      "Epoch: 4617, Len of Training loss: 18, Average loss: 0.8763126366668277\n",
      "Len of Validation loss: 54, Average loss: 3.210017857728181\n",
      "Epoch: 4618, Len of Training loss: 18, Average loss: 0.8395189344882965\n",
      "Len of Validation loss: 54, Average loss: 3.2925503430543124\n",
      "Epoch: 4619, Len of Training loss: 18, Average loss: 0.8899771769841512\n",
      "Len of Validation loss: 54, Average loss: 3.381962318111349\n",
      "Epoch: 4620, Len of Training loss: 18, Average loss: 1.0046786235438452\n",
      "Len of Validation loss: 54, Average loss: 3.3773519220175565\n",
      "Epoch: 4621, Len of Training loss: 18, Average loss: 0.9519206749068366\n",
      "Len of Validation loss: 54, Average loss: 3.4069362426245653\n",
      "Epoch: 4622, Len of Training loss: 18, Average loss: 0.9338671962420145\n",
      "Len of Validation loss: 54, Average loss: 3.3691420709645308\n",
      "Epoch: 4623, Len of Training loss: 18, Average loss: 0.9202408161428239\n",
      "Len of Validation loss: 54, Average loss: 3.298741077935254\n",
      "Epoch: 4624, Len of Training loss: 18, Average loss: 0.9732252359390259\n",
      "Len of Validation loss: 54, Average loss: 3.4396718133378914\n",
      "Epoch: 4625, Len of Training loss: 18, Average loss: 0.9231317275100284\n",
      "Len of Validation loss: 54, Average loss: 3.4546495499434293\n",
      "Epoch: 4626, Len of Training loss: 18, Average loss: 0.9463424185911814\n",
      "Len of Validation loss: 54, Average loss: 3.5032194002910897\n",
      "Epoch: 4627, Len of Training loss: 18, Average loss: 0.953569985098309\n",
      "Len of Validation loss: 54, Average loss: 3.504868576924006\n",
      "Epoch: 4628, Len of Training loss: 18, Average loss: 0.9062242243025038\n",
      "Len of Validation loss: 54, Average loss: 3.4490346952720925\n",
      "Epoch: 4629, Len of Training loss: 18, Average loss: 0.8885902199480269\n",
      "Len of Validation loss: 54, Average loss: 3.6209132185688726\n",
      "Epoch: 4630, Len of Training loss: 18, Average loss: 0.9916354616483053\n",
      "Len of Validation loss: 54, Average loss: 3.350537294590915\n",
      "Epoch: 4631, Len of Training loss: 18, Average loss: 0.9635569221443601\n",
      "Len of Validation loss: 54, Average loss: 3.2489225765069327\n",
      "Epoch: 4632, Len of Training loss: 18, Average loss: 0.8942843907409244\n",
      "Len of Validation loss: 54, Average loss: 3.2633051055449025\n",
      "Epoch: 4633, Len of Training loss: 18, Average loss: 0.9073266718122694\n",
      "Len of Validation loss: 54, Average loss: 3.3085651916486247\n",
      "Epoch: 4634, Len of Training loss: 18, Average loss: 0.9862088130580055\n",
      "Len of Validation loss: 54, Average loss: 3.3507844750527984\n",
      "Epoch: 4635, Len of Training loss: 18, Average loss: 1.0040839380688138\n",
      "Len of Validation loss: 54, Average loss: 3.230651158977438\n",
      "Epoch: 4636, Len of Training loss: 18, Average loss: 0.9997719890541501\n",
      "Len of Validation loss: 54, Average loss: 3.381475461853875\n",
      "Epoch: 4637, Len of Training loss: 18, Average loss: 1.1117297907670338\n",
      "Len of Validation loss: 54, Average loss: 3.7556652890311346\n",
      "Epoch: 4638, Len of Training loss: 18, Average loss: 1.334499888949924\n",
      "Len of Validation loss: 54, Average loss: 3.581769553599534\n",
      "Epoch: 4639, Len of Training loss: 18, Average loss: 1.1199461619059246\n",
      "Len of Validation loss: 54, Average loss: 3.582761063619896\n",
      "Epoch: 4640, Len of Training loss: 18, Average loss: 1.2073067294226751\n",
      "Len of Validation loss: 54, Average loss: 3.5584928040151245\n",
      "Epoch: 4641, Len of Training loss: 18, Average loss: 1.0765975217024486\n",
      "Len of Validation loss: 54, Average loss: 3.325884032028693\n",
      "Epoch: 4642, Len of Training loss: 18, Average loss: 0.9258174929353926\n",
      "Len of Validation loss: 54, Average loss: 3.2846221614767006\n",
      "Epoch: 4643, Len of Training loss: 18, Average loss: 0.8861357271671295\n",
      "Len of Validation loss: 54, Average loss: 3.307837975245935\n",
      "Epoch: 4644, Len of Training loss: 18, Average loss: 0.9168423281775581\n",
      "Len of Validation loss: 54, Average loss: 3.3258423882502095\n",
      "Epoch: 4645, Len of Training loss: 18, Average loss: 0.9078361060884264\n",
      "Len of Validation loss: 54, Average loss: 3.4550835777212074\n",
      "Epoch: 4646, Len of Training loss: 18, Average loss: 0.8797105451424917\n",
      "Len of Validation loss: 54, Average loss: 3.3232174074208296\n",
      "Epoch: 4647, Len of Training loss: 18, Average loss: 0.9629782173368666\n",
      "Len of Validation loss: 54, Average loss: 3.4459532366858587\n",
      "Epoch: 4648, Len of Training loss: 18, Average loss: 0.9380428459909227\n",
      "Len of Validation loss: 54, Average loss: 3.278584678967794\n",
      "Epoch: 4649, Len of Training loss: 18, Average loss: 0.9122260279125638\n",
      "Len of Validation loss: 54, Average loss: 3.4454660294232546\n",
      "Epoch: 4650, Len of Training loss: 18, Average loss: 0.905298113822937\n",
      "Len of Validation loss: 54, Average loss: 3.2956164103967174\n",
      "Epoch: 4651, Len of Training loss: 18, Average loss: 0.9007584551970164\n",
      "Len of Validation loss: 54, Average loss: 3.219170360653489\n",
      "Epoch: 4652, Len of Training loss: 18, Average loss: 0.880746023522483\n",
      "Len of Validation loss: 54, Average loss: 3.4500861962636313\n",
      "Epoch: 4653, Len of Training loss: 18, Average loss: 0.8587039808432261\n",
      "Len of Validation loss: 54, Average loss: 3.4887636458432234\n",
      "Epoch: 4654, Len of Training loss: 18, Average loss: 1.0246010290251837\n",
      "Len of Validation loss: 54, Average loss: 3.3141933944490223\n",
      "Epoch: 4655, Len of Training loss: 18, Average loss: 0.9851849344041612\n",
      "Len of Validation loss: 54, Average loss: 3.2837166245336884\n",
      "Epoch: 4656, Len of Training loss: 18, Average loss: 0.9157982832855649\n",
      "Len of Validation loss: 54, Average loss: 3.4626930631973125\n",
      "Epoch: 4657, Len of Training loss: 18, Average loss: 0.9339171846707662\n",
      "Len of Validation loss: 54, Average loss: 3.4672010044256845\n",
      "Epoch: 4658, Len of Training loss: 18, Average loss: 1.1305169761180878\n",
      "Len of Validation loss: 54, Average loss: 3.441165311468972\n",
      "Epoch: 4659, Len of Training loss: 18, Average loss: 1.139904671245151\n",
      "Len of Validation loss: 54, Average loss: 3.57232857743899\n",
      "Epoch: 4660, Len of Training loss: 18, Average loss: 1.069727275106642\n",
      "Len of Validation loss: 54, Average loss: 3.5881650425769664\n",
      "Epoch: 4661, Len of Training loss: 18, Average loss: 1.0990110668871138\n",
      "Len of Validation loss: 54, Average loss: 3.547976478382393\n",
      "Epoch: 4662, Len of Training loss: 18, Average loss: 1.115864935848448\n",
      "Len of Validation loss: 54, Average loss: 3.628705483895761\n",
      "Epoch: 4663, Len of Training loss: 18, Average loss: 1.0536246332857344\n",
      "Len of Validation loss: 54, Average loss: 3.5031843428258544\n",
      "Epoch: 4664, Len of Training loss: 18, Average loss: 0.9799345764848921\n",
      "Len of Validation loss: 54, Average loss: 3.585749285088645\n",
      "Epoch: 4665, Len of Training loss: 18, Average loss: 0.9078858130507999\n",
      "Len of Validation loss: 54, Average loss: 3.4160203127949327\n",
      "Epoch: 4666, Len of Training loss: 18, Average loss: 0.8632698059082031\n",
      "Len of Validation loss: 54, Average loss: 3.394122149105425\n",
      "Epoch: 4667, Len of Training loss: 18, Average loss: 0.9717871215608385\n",
      "Len of Validation loss: 54, Average loss: 3.5169682414443404\n",
      "Epoch: 4668, Len of Training loss: 18, Average loss: 0.9351701372199588\n",
      "Len of Validation loss: 54, Average loss: 3.4457473810072297\n",
      "Epoch: 4669, Len of Training loss: 18, Average loss: 0.9156495167149438\n",
      "Len of Validation loss: 54, Average loss: 3.356065304191024\n",
      "Epoch: 4670, Len of Training loss: 18, Average loss: 1.0763140155209436\n",
      "Len of Validation loss: 54, Average loss: 3.357943801968186\n",
      "Epoch: 4671, Len of Training loss: 18, Average loss: 1.3383763432502747\n",
      "Len of Validation loss: 54, Average loss: 3.8938141719058708\n",
      "Epoch: 4672, Len of Training loss: 18, Average loss: 1.2246314022276137\n",
      "Len of Validation loss: 54, Average loss: 3.581143226888445\n",
      "Epoch: 4673, Len of Training loss: 18, Average loss: 1.1405550969971552\n",
      "Len of Validation loss: 54, Average loss: 3.5281708670987024\n",
      "Epoch: 4674, Len of Training loss: 18, Average loss: 1.132969018485811\n",
      "Len of Validation loss: 54, Average loss: 3.51667258143425\n",
      "Epoch: 4675, Len of Training loss: 18, Average loss: 1.1063938173982832\n",
      "Len of Validation loss: 54, Average loss: 3.537003028172034\n",
      "Epoch: 4676, Len of Training loss: 18, Average loss: 1.0354092518488567\n",
      "Len of Validation loss: 54, Average loss: 3.4023421307404837\n",
      "Epoch: 4677, Len of Training loss: 18, Average loss: 0.9124090373516083\n",
      "Len of Validation loss: 54, Average loss: 3.366655738265426\n",
      "Epoch: 4678, Len of Training loss: 18, Average loss: 0.9093160364362929\n",
      "Len of Validation loss: 54, Average loss: 3.3347654629636696\n",
      "Epoch: 4679, Len of Training loss: 18, Average loss: 0.9541631804572211\n",
      "Len of Validation loss: 54, Average loss: 3.2579489140598863\n",
      "Epoch: 4680, Len of Training loss: 18, Average loss: 0.9013418356577555\n",
      "Len of Validation loss: 54, Average loss: 3.250964085261027\n",
      "Epoch: 4681, Len of Training loss: 18, Average loss: 0.9285515480571322\n",
      "Len of Validation loss: 54, Average loss: 3.23489092014454\n",
      "Epoch: 4682, Len of Training loss: 18, Average loss: 0.9235929979218377\n",
      "Len of Validation loss: 54, Average loss: 3.393998864624235\n",
      "Epoch: 4683, Len of Training loss: 18, Average loss: 0.9847437838713328\n",
      "Len of Validation loss: 54, Average loss: 3.3529340194331274\n",
      "Epoch: 4684, Len of Training loss: 18, Average loss: 0.9874388608667586\n",
      "Len of Validation loss: 54, Average loss: 3.5067046924873635\n",
      "Epoch: 4685, Len of Training loss: 18, Average loss: 0.9169079230891334\n",
      "Len of Validation loss: 54, Average loss: 3.4826757113138833\n",
      "Epoch: 4686, Len of Training loss: 18, Average loss: 0.8960395389133029\n",
      "Len of Validation loss: 54, Average loss: 3.5131254527303906\n",
      "Epoch: 4687, Len of Training loss: 18, Average loss: 1.0480555726422205\n",
      "Len of Validation loss: 54, Average loss: 3.5006612252306053\n",
      "Epoch: 4688, Len of Training loss: 18, Average loss: 0.9149847957823012\n",
      "Len of Validation loss: 54, Average loss: 3.3432521908371537\n",
      "Epoch: 4689, Len of Training loss: 18, Average loss: 0.8944751189814674\n",
      "Len of Validation loss: 54, Average loss: 3.3252213773904025\n",
      "Epoch: 4690, Len of Training loss: 18, Average loss: 0.943736755185657\n",
      "Len of Validation loss: 54, Average loss: 3.3995305354948395\n",
      "Epoch: 4691, Len of Training loss: 18, Average loss: 0.9609222842587365\n",
      "Len of Validation loss: 54, Average loss: 3.0745051337613\n",
      "Epoch: 4692, Len of Training loss: 18, Average loss: 0.9554397563139597\n",
      "Len of Validation loss: 54, Average loss: 3.476474407646391\n",
      "Epoch: 4693, Len of Training loss: 18, Average loss: 0.9839441842503018\n",
      "Len of Validation loss: 54, Average loss: 3.6669639393135354\n",
      "Epoch: 4694, Len of Training loss: 18, Average loss: 0.9729226993189918\n",
      "Len of Validation loss: 54, Average loss: 3.332112609236329\n",
      "Epoch: 4695, Len of Training loss: 18, Average loss: 0.8975619408819411\n",
      "Len of Validation loss: 54, Average loss: 3.425457052610539\n",
      "Epoch: 4696, Len of Training loss: 18, Average loss: 1.0466340978940327\n",
      "Len of Validation loss: 54, Average loss: 3.4053133372907287\n",
      "Epoch: 4697, Len of Training loss: 18, Average loss: 0.9632514946990542\n",
      "Len of Validation loss: 54, Average loss: 3.4835224427558757\n",
      "Epoch: 4698, Len of Training loss: 18, Average loss: 0.9803224272198148\n",
      "Len of Validation loss: 54, Average loss: 3.329009504229934\n",
      "Epoch: 4699, Len of Training loss: 18, Average loss: 0.9141981965965695\n",
      "Len of Validation loss: 54, Average loss: 3.375932640499539\n",
      "Epoch: 4700, Len of Training loss: 18, Average loss: 0.9047337538666196\n",
      "Len of Validation loss: 54, Average loss: 3.4128131568431854\n",
      "Epoch: 4701, Len of Training loss: 18, Average loss: 0.9476171599494086\n",
      "Len of Validation loss: 54, Average loss: 3.244936484981466\n",
      "Epoch: 4702, Len of Training loss: 18, Average loss: 0.8716612160205841\n",
      "Len of Validation loss: 54, Average loss: 3.264963534143236\n",
      "Epoch: 4703, Len of Training loss: 18, Average loss: 0.8677382734086778\n",
      "Len of Validation loss: 54, Average loss: 3.5596637107707836\n",
      "Epoch: 4704, Len of Training loss: 18, Average loss: 0.8853056298361884\n",
      "Len of Validation loss: 54, Average loss: 3.407481433064849\n",
      "Epoch: 4705, Len of Training loss: 18, Average loss: 0.9471078978644477\n",
      "Len of Validation loss: 54, Average loss: 3.2462878271385476\n",
      "Epoch: 4706, Len of Training loss: 18, Average loss: 0.9414174291822646\n",
      "Len of Validation loss: 54, Average loss: 3.261200221600356\n",
      "Epoch: 4707, Len of Training loss: 18, Average loss: 0.9709199931886461\n",
      "Len of Validation loss: 54, Average loss: 3.4096340470843844\n",
      "Epoch: 4708, Len of Training loss: 18, Average loss: 1.0134104357825384\n",
      "Len of Validation loss: 54, Average loss: 3.455360420324184\n",
      "Epoch: 4709, Len of Training loss: 18, Average loss: 0.9259070853392283\n",
      "Len of Validation loss: 54, Average loss: 3.278661913341946\n",
      "Epoch: 4710, Len of Training loss: 18, Average loss: 0.9086470968193479\n",
      "Len of Validation loss: 54, Average loss: 3.384683229305126\n",
      "Epoch: 4711, Len of Training loss: 18, Average loss: 0.9231261445416344\n",
      "Len of Validation loss: 54, Average loss: 3.4523554576767816\n",
      "Epoch: 4712, Len of Training loss: 18, Average loss: 0.8728063570128547\n",
      "Len of Validation loss: 54, Average loss: 3.390211961887501\n",
      "Epoch: 4713, Len of Training loss: 18, Average loss: 0.8283896015750037\n",
      "Len of Validation loss: 54, Average loss: 3.1502199945626437\n",
      "Epoch: 4714, Len of Training loss: 18, Average loss: 0.9645895395014021\n",
      "Len of Validation loss: 54, Average loss: 3.347425934341219\n",
      "Epoch: 4715, Len of Training loss: 18, Average loss: 0.9663399424817827\n",
      "Len of Validation loss: 54, Average loss: 3.419723672999276\n",
      "Epoch: 4716, Len of Training loss: 18, Average loss: 0.9692440761460198\n",
      "Len of Validation loss: 54, Average loss: 3.4990247476983956\n",
      "Epoch: 4717, Len of Training loss: 18, Average loss: 0.9461312625143263\n",
      "Len of Validation loss: 54, Average loss: 3.398130612240897\n",
      "Epoch: 4718, Len of Training loss: 18, Average loss: 0.9537052545282576\n",
      "Len of Validation loss: 54, Average loss: 3.646665801604589\n",
      "Epoch: 4719, Len of Training loss: 18, Average loss: 1.0244673656092749\n",
      "Len of Validation loss: 54, Average loss: 3.301481494197139\n",
      "Epoch: 4720, Len of Training loss: 18, Average loss: 0.9952248500453101\n",
      "Len of Validation loss: 54, Average loss: 3.33424632968726\n",
      "Epoch: 4721, Len of Training loss: 18, Average loss: 1.0400136013825734\n",
      "Len of Validation loss: 54, Average loss: 3.423206747682006\n",
      "Epoch: 4722, Len of Training loss: 18, Average loss: 1.0203645063771143\n",
      "Len of Validation loss: 54, Average loss: 3.574008485785237\n",
      "Epoch: 4723, Len of Training loss: 18, Average loss: 1.0782245794932048\n",
      "Len of Validation loss: 54, Average loss: 3.576403977694335\n",
      "Epoch: 4724, Len of Training loss: 18, Average loss: 1.1151286529170141\n",
      "Len of Validation loss: 54, Average loss: 3.363153667361648\n",
      "Epoch: 4725, Len of Training loss: 18, Average loss: 1.1352369354830847\n",
      "Len of Validation loss: 54, Average loss: 3.4892779566623546\n",
      "Epoch: 4726, Len of Training loss: 18, Average loss: 1.0973175134923723\n",
      "Len of Validation loss: 54, Average loss: 3.53257726960712\n",
      "Epoch: 4727, Len of Training loss: 18, Average loss: 1.0434579352537792\n",
      "Len of Validation loss: 54, Average loss: 3.4425924298939883\n",
      "Epoch: 4728, Len of Training loss: 18, Average loss: 0.9825281666384803\n",
      "Len of Validation loss: 54, Average loss: 3.3214029989860676\n",
      "Epoch: 4729, Len of Training loss: 18, Average loss: 0.9557447334130605\n",
      "Len of Validation loss: 54, Average loss: 3.518848986537368\n",
      "Epoch: 4730, Len of Training loss: 18, Average loss: 0.9198356370131174\n",
      "Len of Validation loss: 54, Average loss: 3.3509112706890813\n",
      "Epoch: 4731, Len of Training loss: 18, Average loss: 1.046672327650918\n",
      "Len of Validation loss: 54, Average loss: 3.475322178116551\n",
      "Epoch: 4732, Len of Training loss: 18, Average loss: 1.1307650340927973\n",
      "Len of Validation loss: 54, Average loss: 3.6371936433845096\n",
      "Epoch: 4733, Len of Training loss: 18, Average loss: 1.3615155551168654\n",
      "Len of Validation loss: 54, Average loss: 3.688195018856614\n",
      "Epoch: 4734, Len of Training loss: 18, Average loss: 1.3345336980289884\n",
      "Len of Validation loss: 54, Average loss: 3.896500616161912\n",
      "Epoch: 4735, Len of Training loss: 18, Average loss: 1.4752829339769151\n",
      "Len of Validation loss: 54, Average loss: 3.6761888287685536\n",
      "Epoch: 4736, Len of Training loss: 18, Average loss: 1.1461985343032413\n",
      "Len of Validation loss: 54, Average loss: 3.5647230965119823\n",
      "Epoch: 4737, Len of Training loss: 18, Average loss: 1.0352005064487457\n",
      "Len of Validation loss: 54, Average loss: 3.385530706908968\n",
      "Epoch: 4738, Len of Training loss: 18, Average loss: 0.9386589527130127\n",
      "Len of Validation loss: 54, Average loss: 3.632384105964943\n",
      "Epoch: 4739, Len of Training loss: 18, Average loss: 0.8629877070585886\n",
      "Len of Validation loss: 54, Average loss: 3.4274039500289493\n",
      "Epoch: 4740, Len of Training loss: 18, Average loss: 0.829808748430676\n",
      "Len of Validation loss: 54, Average loss: 3.358759731054306\n",
      "Epoch: 4741, Len of Training loss: 18, Average loss: 0.8290344542927213\n",
      "Len of Validation loss: 54, Average loss: 3.350445141394933\n",
      "Epoch: 4742, Len of Training loss: 18, Average loss: 0.8815654052628411\n",
      "Len of Validation loss: 54, Average loss: 3.501341018411848\n",
      "Epoch: 4743, Len of Training loss: 18, Average loss: 0.9044077330165439\n",
      "Len of Validation loss: 54, Average loss: 3.54912829067972\n",
      "Epoch: 4744, Len of Training loss: 18, Average loss: 0.9103621476226382\n",
      "Len of Validation loss: 54, Average loss: 3.413244099528701\n",
      "Epoch: 4745, Len of Training loss: 18, Average loss: 1.0269313388400607\n",
      "Len of Validation loss: 54, Average loss: 3.3916408485836453\n",
      "Epoch: 4746, Len of Training loss: 18, Average loss: 0.9452034036318461\n",
      "Len of Validation loss: 54, Average loss: 3.2772216421586498\n",
      "Epoch: 4747, Len of Training loss: 18, Average loss: 1.0390032364262476\n",
      "Len of Validation loss: 54, Average loss: 3.3930404550499387\n",
      "Epoch: 4748, Len of Training loss: 18, Average loss: 1.1157428920269012\n",
      "Len of Validation loss: 54, Average loss: 3.978185898727841\n",
      "Epoch: 4749, Len of Training loss: 18, Average loss: 1.5527076522509258\n",
      "Len of Validation loss: 54, Average loss: 3.687934245224352\n",
      "Epoch: 4750, Len of Training loss: 18, Average loss: 1.2733808623419867\n",
      "Len of Validation loss: 54, Average loss: 3.565656030619586\n",
      "Epoch: 4751, Len of Training loss: 18, Average loss: 1.0719441539711423\n",
      "Len of Validation loss: 54, Average loss: 3.4318111251901695\n",
      "Epoch: 4752, Len of Training loss: 18, Average loss: 1.048170119524002\n",
      "Len of Validation loss: 54, Average loss: 3.4263705854062683\n",
      "Epoch: 4753, Len of Training loss: 18, Average loss: 1.0802451703283522\n",
      "Len of Validation loss: 54, Average loss: 3.369865749721174\n",
      "Epoch: 4754, Len of Training loss: 18, Average loss: 1.111583732896381\n",
      "Len of Validation loss: 54, Average loss: 3.4251550833384194\n",
      "Epoch: 4755, Len of Training loss: 18, Average loss: 1.0358362661467657\n",
      "Len of Validation loss: 54, Average loss: 3.5508366657627954\n",
      "Epoch: 4756, Len of Training loss: 18, Average loss: 1.022928340567483\n",
      "Len of Validation loss: 54, Average loss: 3.67764033542739\n",
      "Epoch: 4757, Len of Training loss: 18, Average loss: 1.1146887838840485\n",
      "Len of Validation loss: 54, Average loss: 3.839088970864261\n",
      "Epoch: 4758, Len of Training loss: 18, Average loss: 1.1564195156097412\n",
      "Len of Validation loss: 54, Average loss: 3.33531822319384\n",
      "Epoch: 4759, Len of Training loss: 18, Average loss: 1.1205367512173123\n",
      "Len of Validation loss: 54, Average loss: 3.6295046221326897\n",
      "Epoch: 4760, Len of Training loss: 18, Average loss: 1.0935717225074768\n",
      "Len of Validation loss: 54, Average loss: 3.524237022355751\n",
      "Epoch: 4761, Len of Training loss: 18, Average loss: 0.9957619905471802\n",
      "Len of Validation loss: 54, Average loss: 3.51671623300623\n",
      "Epoch: 4762, Len of Training loss: 18, Average loss: 0.9716502527395884\n",
      "Len of Validation loss: 54, Average loss: 3.371406517646931\n",
      "Epoch: 4763, Len of Training loss: 18, Average loss: 0.9680787126223246\n",
      "Len of Validation loss: 54, Average loss: 3.384684071496681\n",
      "Epoch: 4764, Len of Training loss: 18, Average loss: 0.9661232696639167\n",
      "Len of Validation loss: 54, Average loss: 3.3504854517954366\n",
      "Epoch: 4765, Len of Training loss: 18, Average loss: 0.9202033314439986\n",
      "Len of Validation loss: 54, Average loss: 3.243646045525869\n",
      "Epoch: 4766, Len of Training loss: 18, Average loss: 0.8620333009295993\n",
      "Len of Validation loss: 54, Average loss: 3.1902179695941784\n",
      "Epoch: 4767, Len of Training loss: 18, Average loss: 0.8876161244180467\n",
      "Len of Validation loss: 54, Average loss: 3.4438043623058885\n",
      "Epoch: 4768, Len of Training loss: 18, Average loss: 1.0500664677884843\n",
      "Len of Validation loss: 54, Average loss: 3.703677922487259\n",
      "Epoch: 4769, Len of Training loss: 18, Average loss: 1.2446497943666246\n",
      "Len of Validation loss: 54, Average loss: 3.4544082758603274\n",
      "Epoch: 4770, Len of Training loss: 18, Average loss: 1.0979108446174197\n",
      "Len of Validation loss: 54, Average loss: 3.5866924711951502\n",
      "Epoch: 4771, Len of Training loss: 18, Average loss: 0.9341582225428687\n",
      "Len of Validation loss: 54, Average loss: 3.2852981090545654\n",
      "Epoch: 4772, Len of Training loss: 18, Average loss: 0.880454765425788\n",
      "Len of Validation loss: 54, Average loss: 3.398951592268767\n",
      "Epoch: 4773, Len of Training loss: 18, Average loss: 0.9062281449635824\n",
      "Len of Validation loss: 54, Average loss: 3.259738411064501\n",
      "Epoch: 4774, Len of Training loss: 18, Average loss: 0.9323417974842919\n",
      "Len of Validation loss: 54, Average loss: 3.412291681324994\n",
      "Epoch: 4775, Len of Training loss: 18, Average loss: 0.8640057245890299\n",
      "Len of Validation loss: 54, Average loss: 3.3273532225026026\n",
      "Epoch: 4776, Len of Training loss: 18, Average loss: 0.8851958082781898\n",
      "Len of Validation loss: 54, Average loss: 3.410729846468678\n",
      "Epoch: 4777, Len of Training loss: 18, Average loss: 0.9870747394031949\n",
      "Len of Validation loss: 54, Average loss: 3.5978550171410597\n",
      "Epoch: 4778, Len of Training loss: 18, Average loss: 1.2031095557742648\n",
      "Len of Validation loss: 54, Average loss: 3.5203816769299685\n",
      "Epoch: 4779, Len of Training loss: 18, Average loss: 1.0345842639605205\n",
      "Len of Validation loss: 54, Average loss: 3.5481843760720007\n",
      "Epoch: 4780, Len of Training loss: 18, Average loss: 1.0720526377360027\n",
      "Len of Validation loss: 54, Average loss: 3.611280721646768\n",
      "Epoch: 4781, Len of Training loss: 18, Average loss: 1.1507028639316559\n",
      "Len of Validation loss: 54, Average loss: 3.431383928766957\n",
      "Epoch: 4782, Len of Training loss: 18, Average loss: 1.0755771034293704\n",
      "Len of Validation loss: 54, Average loss: 3.465879215134515\n",
      "Epoch: 4783, Len of Training loss: 18, Average loss: 1.0497430331177182\n",
      "Len of Validation loss: 54, Average loss: 3.3842061769079277\n",
      "Epoch: 4784, Len of Training loss: 18, Average loss: 0.9610924356513553\n",
      "Len of Validation loss: 54, Average loss: 3.4409389385470637\n",
      "Epoch: 4785, Len of Training loss: 18, Average loss: 0.9077132940292358\n",
      "Len of Validation loss: 54, Average loss: 3.56497084763315\n",
      "Epoch: 4786, Len of Training loss: 18, Average loss: 0.9171188043223487\n",
      "Len of Validation loss: 54, Average loss: 3.3979791458006257\n",
      "Epoch: 4787, Len of Training loss: 18, Average loss: 0.9794445733229319\n",
      "Len of Validation loss: 54, Average loss: 3.382143450004083\n",
      "Epoch: 4788, Len of Training loss: 18, Average loss: 0.9249573747316996\n",
      "Len of Validation loss: 54, Average loss: 3.3047110460422657\n",
      "Epoch: 4789, Len of Training loss: 18, Average loss: 0.8543318344487084\n",
      "Len of Validation loss: 54, Average loss: 3.490665031803979\n",
      "Epoch: 4790, Len of Training loss: 18, Average loss: 0.9313897755410936\n",
      "Len of Validation loss: 54, Average loss: 3.620669604451568\n",
      "Epoch: 4791, Len of Training loss: 18, Average loss: 1.183940827846527\n",
      "Len of Validation loss: 54, Average loss: 3.4852787399733507\n",
      "Epoch: 4792, Len of Training loss: 18, Average loss: 1.0829162465201483\n",
      "Len of Validation loss: 54, Average loss: 3.8176913548398903\n",
      "Epoch: 4793, Len of Training loss: 18, Average loss: 1.0583395957946777\n",
      "Len of Validation loss: 54, Average loss: 3.4007836679617562\n",
      "Epoch: 4794, Len of Training loss: 18, Average loss: 0.9925075338946449\n",
      "Len of Validation loss: 54, Average loss: 3.7039193974600897\n",
      "Epoch: 4795, Len of Training loss: 18, Average loss: 0.9701316257317861\n",
      "Len of Validation loss: 54, Average loss: 3.45120773602415\n",
      "Epoch: 4796, Len of Training loss: 18, Average loss: 0.9496804277102152\n",
      "Len of Validation loss: 54, Average loss: 3.3830218138518156\n",
      "Epoch: 4797, Len of Training loss: 18, Average loss: 0.9057892031139798\n",
      "Len of Validation loss: 54, Average loss: 3.530982340927477\n",
      "Epoch: 4798, Len of Training loss: 18, Average loss: 0.8941049145327674\n",
      "Len of Validation loss: 54, Average loss: 3.5244511939861156\n",
      "Epoch: 4799, Len of Training loss: 18, Average loss: 0.9439332187175751\n",
      "Len of Validation loss: 54, Average loss: 3.4037248922718897\n",
      "Epoch: 4800, Len of Training loss: 18, Average loss: 0.9466621975104014\n",
      "Len of Validation loss: 54, Average loss: 3.397145045024377\n",
      "Epoch: 4801, Len of Training loss: 18, Average loss: 1.0843679110209148\n",
      "Len of Validation loss: 54, Average loss: 3.423124983354851\n",
      "Epoch: 4802, Len of Training loss: 18, Average loss: 1.1840429306030273\n",
      "Len of Validation loss: 54, Average loss: 3.5901766496675984\n",
      "Epoch: 4803, Len of Training loss: 18, Average loss: 1.0723125437895458\n",
      "Len of Validation loss: 54, Average loss: 3.5279339253902435\n",
      "Epoch: 4804, Len of Training loss: 18, Average loss: 1.053277889887492\n",
      "Len of Validation loss: 54, Average loss: 3.1515620240458735\n",
      "Epoch: 4805, Len of Training loss: 18, Average loss: 1.0648805366622076\n",
      "Len of Validation loss: 54, Average loss: 3.4178862428223646\n",
      "Epoch: 4806, Len of Training loss: 18, Average loss: 0.9453273448679183\n",
      "Len of Validation loss: 54, Average loss: 3.4265727742954537\n",
      "Epoch: 4807, Len of Training loss: 18, Average loss: 0.8841284082995521\n",
      "Len of Validation loss: 54, Average loss: 3.3244909357141563\n",
      "Epoch: 4808, Len of Training loss: 18, Average loss: 0.8228084246317545\n",
      "Len of Validation loss: 54, Average loss: 3.361964322902538\n",
      "Epoch: 4809, Len of Training loss: 18, Average loss: 0.8369802037874857\n",
      "Len of Validation loss: 54, Average loss: 3.211539755264918\n",
      "Epoch: 4810, Len of Training loss: 18, Average loss: 0.8441114326318105\n",
      "Len of Validation loss: 54, Average loss: 3.4129307115519487\n",
      "Epoch: 4811, Len of Training loss: 18, Average loss: 0.9938310086727142\n",
      "Len of Validation loss: 54, Average loss: 3.551648446807155\n",
      "Epoch: 4812, Len of Training loss: 18, Average loss: 1.1235785451200273\n",
      "Len of Validation loss: 54, Average loss: 3.500541622991915\n",
      "Epoch: 4813, Len of Training loss: 18, Average loss: 1.1194313334094153\n",
      "Len of Validation loss: 54, Average loss: 3.4450909517429493\n",
      "Epoch: 4814, Len of Training loss: 18, Average loss: 0.9906711611482832\n",
      "Len of Validation loss: 54, Average loss: 3.3407346076435513\n",
      "Epoch: 4815, Len of Training loss: 18, Average loss: 0.952249162726932\n",
      "Len of Validation loss: 54, Average loss: 3.384604811668396\n",
      "Epoch: 4816, Len of Training loss: 18, Average loss: 0.8777517113420699\n",
      "Len of Validation loss: 54, Average loss: 3.428775980516716\n",
      "Epoch: 4817, Len of Training loss: 18, Average loss: 0.8479458954599168\n",
      "Len of Validation loss: 54, Average loss: 3.169297759179716\n",
      "Epoch: 4818, Len of Training loss: 18, Average loss: 0.8496104975541433\n",
      "Len of Validation loss: 54, Average loss: 3.5120505039338714\n",
      "Epoch: 4819, Len of Training loss: 18, Average loss: 0.8969352543354034\n",
      "Len of Validation loss: 54, Average loss: 3.517953754575164\n",
      "Epoch: 4820, Len of Training loss: 18, Average loss: 0.9234123031298319\n",
      "Len of Validation loss: 54, Average loss: 3.52061715832463\n",
      "Epoch: 4821, Len of Training loss: 18, Average loss: 0.9141006039248573\n",
      "Len of Validation loss: 54, Average loss: 3.339857535229789\n",
      "Epoch: 4822, Len of Training loss: 18, Average loss: 1.0965286195278168\n",
      "Len of Validation loss: 54, Average loss: 3.273208319037049\n",
      "Epoch: 4823, Len of Training loss: 18, Average loss: 1.1708083649476368\n",
      "Len of Validation loss: 54, Average loss: 3.6323005888197155\n",
      "Epoch: 4824, Len of Training loss: 18, Average loss: 1.2312030361758337\n",
      "Len of Validation loss: 54, Average loss: 3.4842417427787074\n",
      "Epoch: 4825, Len of Training loss: 18, Average loss: 1.009820842080646\n",
      "Len of Validation loss: 54, Average loss: 3.37836699574082\n",
      "Epoch: 4826, Len of Training loss: 18, Average loss: 0.9294139014350044\n",
      "Len of Validation loss: 54, Average loss: 3.328527858963719\n",
      "Epoch: 4827, Len of Training loss: 18, Average loss: 0.8871667053964403\n",
      "Len of Validation loss: 54, Average loss: 3.511258820692698\n",
      "Epoch: 4828, Len of Training loss: 18, Average loss: 0.8914504183663262\n",
      "Len of Validation loss: 54, Average loss: 3.377703005516971\n",
      "Epoch: 4829, Len of Training loss: 18, Average loss: 0.9569346772299873\n",
      "Len of Validation loss: 54, Average loss: 3.409266742291274\n",
      "Epoch: 4830, Len of Training loss: 18, Average loss: 0.9207581844594743\n",
      "Len of Validation loss: 54, Average loss: 3.403854289540538\n",
      "Epoch: 4831, Len of Training loss: 18, Average loss: 0.8862076236142052\n",
      "Len of Validation loss: 54, Average loss: 3.3106047842237682\n",
      "Epoch: 4832, Len of Training loss: 18, Average loss: 0.8633937107192146\n",
      "Len of Validation loss: 54, Average loss: 3.5078027182155185\n",
      "Epoch: 4833, Len of Training loss: 18, Average loss: 0.9429531792799631\n",
      "Len of Validation loss: 54, Average loss: 3.2930853830443487\n",
      "Epoch: 4834, Len of Training loss: 18, Average loss: 0.8673082987467448\n",
      "Len of Validation loss: 54, Average loss: 3.455590890513526\n",
      "Epoch: 4835, Len of Training loss: 18, Average loss: 0.8926923672358195\n",
      "Len of Validation loss: 54, Average loss: 3.435062760556186\n",
      "Epoch: 4836, Len of Training loss: 18, Average loss: 0.9160816603236728\n",
      "Len of Validation loss: 54, Average loss: 3.358857483775527\n",
      "Epoch: 4837, Len of Training loss: 18, Average loss: 0.8735646307468414\n",
      "Len of Validation loss: 54, Average loss: 3.222048156791263\n",
      "Epoch: 4838, Len of Training loss: 18, Average loss: 0.9369377990563711\n",
      "Len of Validation loss: 54, Average loss: 3.4167080411204585\n",
      "Epoch: 4839, Len of Training loss: 18, Average loss: 0.9546576340993246\n",
      "Len of Validation loss: 54, Average loss: 3.5216825251226074\n",
      "Epoch: 4840, Len of Training loss: 18, Average loss: 0.9871779481569926\n",
      "Len of Validation loss: 54, Average loss: 3.38907109035386\n",
      "Epoch: 4841, Len of Training loss: 18, Average loss: 1.0274266103903453\n",
      "Len of Validation loss: 54, Average loss: 3.51855707499716\n",
      "Epoch: 4842, Len of Training loss: 18, Average loss: 1.0598417752318912\n",
      "Len of Validation loss: 54, Average loss: 3.3501023893003112\n",
      "Epoch: 4843, Len of Training loss: 18, Average loss: 0.9065668019983504\n",
      "Len of Validation loss: 54, Average loss: 3.309632710836552\n",
      "Epoch: 4844, Len of Training loss: 18, Average loss: 1.0139872928460438\n",
      "Len of Validation loss: 54, Average loss: 3.6373623642656536\n",
      "Epoch: 4845, Len of Training loss: 18, Average loss: 1.1341841684447393\n",
      "Len of Validation loss: 54, Average loss: 3.5102133563271276\n",
      "Epoch: 4846, Len of Training loss: 18, Average loss: 0.9272905952400632\n",
      "Len of Validation loss: 54, Average loss: 3.349530142766458\n",
      "Epoch: 4847, Len of Training loss: 18, Average loss: 0.8792501654889848\n",
      "Len of Validation loss: 54, Average loss: 3.397308964420248\n",
      "Epoch: 4848, Len of Training loss: 18, Average loss: 0.9111840162012312\n",
      "Len of Validation loss: 54, Average loss: 3.500508828295602\n",
      "Epoch: 4849, Len of Training loss: 18, Average loss: 1.0677922997209761\n",
      "Len of Validation loss: 54, Average loss: 3.374918081142284\n",
      "Epoch: 4850, Len of Training loss: 18, Average loss: 1.1308720409870148\n",
      "Len of Validation loss: 54, Average loss: 3.5038617286417217\n",
      "Epoch: 4851, Len of Training loss: 18, Average loss: 0.9965419967969259\n",
      "Len of Validation loss: 54, Average loss: 3.6412405128832215\n",
      "Epoch: 4852, Len of Training loss: 18, Average loss: 1.057422290245692\n",
      "Len of Validation loss: 54, Average loss: 3.4584784220766136\n",
      "Epoch: 4853, Len of Training loss: 18, Average loss: 0.9629603193865882\n",
      "Len of Validation loss: 54, Average loss: 3.6103246686635195\n",
      "Epoch: 4854, Len of Training loss: 18, Average loss: 0.9542848103576236\n",
      "Len of Validation loss: 54, Average loss: 3.328071882327398\n",
      "Epoch: 4855, Len of Training loss: 18, Average loss: 0.9450728595256805\n",
      "Len of Validation loss: 54, Average loss: 3.252373896263264\n",
      "Epoch: 4856, Len of Training loss: 18, Average loss: 0.9312650958697001\n",
      "Len of Validation loss: 54, Average loss: 3.421129062219902\n",
      "Epoch: 4857, Len of Training loss: 18, Average loss: 0.8982563118139902\n",
      "Len of Validation loss: 54, Average loss: 3.2694045614313194\n",
      "Epoch: 4858, Len of Training loss: 18, Average loss: 0.8840412464406755\n",
      "Len of Validation loss: 54, Average loss: 3.209986359984786\n",
      "Epoch: 4859, Len of Training loss: 18, Average loss: 0.9281966355111864\n",
      "Len of Validation loss: 54, Average loss: 3.310847842031055\n",
      "Epoch: 4860, Len of Training loss: 18, Average loss: 1.0316289232836828\n",
      "Len of Validation loss: 54, Average loss: 3.436219580747463\n",
      "Epoch: 4861, Len of Training loss: 18, Average loss: 1.1066462165779538\n",
      "Len of Validation loss: 54, Average loss: 3.350930534027241\n",
      "Epoch: 4862, Len of Training loss: 18, Average loss: 1.037989467382431\n",
      "Len of Validation loss: 54, Average loss: 3.668735525122395\n",
      "Epoch: 4863, Len of Training loss: 18, Average loss: 0.961986329820421\n",
      "Len of Validation loss: 54, Average loss: 3.3597791813038014\n",
      "Epoch: 4864, Len of Training loss: 18, Average loss: 0.9424798819753859\n",
      "Len of Validation loss: 54, Average loss: 3.3376074179455086\n",
      "Epoch: 4865, Len of Training loss: 18, Average loss: 0.9457747042179108\n",
      "Len of Validation loss: 54, Average loss: 3.323802061654903\n",
      "Epoch: 4866, Len of Training loss: 18, Average loss: 0.9554029504458109\n",
      "Len of Validation loss: 54, Average loss: 3.323767061586733\n",
      "Epoch: 4867, Len of Training loss: 18, Average loss: 0.9907363454500834\n",
      "Len of Validation loss: 54, Average loss: 3.5553663726206177\n",
      "Epoch: 4868, Len of Training loss: 18, Average loss: 0.9777966174814436\n",
      "Len of Validation loss: 54, Average loss: 3.606875224245919\n",
      "Epoch: 4869, Len of Training loss: 18, Average loss: 1.2703730397754245\n",
      "Len of Validation loss: 54, Average loss: 3.4927845508963973\n",
      "Epoch: 4870, Len of Training loss: 18, Average loss: 1.2991987268129985\n",
      "Len of Validation loss: 54, Average loss: 3.6587912340958915\n",
      "Epoch: 4871, Len of Training loss: 18, Average loss: 1.1567957136366103\n",
      "Len of Validation loss: 54, Average loss: 3.5475592414538064\n",
      "Epoch: 4872, Len of Training loss: 18, Average loss: 1.0984110136826832\n",
      "Len of Validation loss: 54, Average loss: 3.3069471054606967\n",
      "Epoch: 4873, Len of Training loss: 18, Average loss: 0.9969130820698209\n",
      "Len of Validation loss: 54, Average loss: 3.500284287664625\n",
      "Epoch: 4874, Len of Training loss: 18, Average loss: 0.9615088767475553\n",
      "Len of Validation loss: 54, Average loss: 3.5654384670434176\n",
      "Epoch: 4875, Len of Training loss: 18, Average loss: 0.9317255152596368\n",
      "Len of Validation loss: 54, Average loss: 3.6385289861096277\n",
      "Epoch: 4876, Len of Training loss: 18, Average loss: 0.9271368483702341\n",
      "Len of Validation loss: 54, Average loss: 3.4403321820276753\n",
      "Epoch: 4877, Len of Training loss: 18, Average loss: 0.9077932569715712\n",
      "Len of Validation loss: 54, Average loss: 3.236624154779646\n",
      "Epoch: 4878, Len of Training loss: 18, Average loss: 0.8516885638237\n",
      "Len of Validation loss: 54, Average loss: 3.3781988841516\n",
      "Epoch: 4879, Len of Training loss: 18, Average loss: 0.8235225545035468\n",
      "Len of Validation loss: 54, Average loss: 3.3255768241705717\n",
      "Epoch: 4880, Len of Training loss: 18, Average loss: 0.897623383336597\n",
      "Len of Validation loss: 54, Average loss: 3.5094430755685875\n",
      "Epoch: 4881, Len of Training loss: 18, Average loss: 0.8941846191883087\n",
      "Len of Validation loss: 54, Average loss: 3.288644077601256\n",
      "Epoch: 4882, Len of Training loss: 18, Average loss: 0.982642650604248\n",
      "Len of Validation loss: 54, Average loss: 3.4991033750551717\n",
      "Epoch: 4883, Len of Training loss: 18, Average loss: 1.0411695705519781\n",
      "Len of Validation loss: 54, Average loss: 3.4476233654552035\n",
      "Epoch: 4884, Len of Training loss: 18, Average loss: 0.9696219464143118\n",
      "Len of Validation loss: 54, Average loss: 3.5688869368146965\n",
      "Epoch: 4885, Len of Training loss: 18, Average loss: 1.000476727883021\n",
      "Len of Validation loss: 54, Average loss: 3.2895518099820173\n",
      "Epoch: 4886, Len of Training loss: 18, Average loss: 0.9225859940052032\n",
      "Len of Validation loss: 54, Average loss: 3.593855639298757\n",
      "Epoch: 4887, Len of Training loss: 18, Average loss: 1.209497527943717\n",
      "Len of Validation loss: 54, Average loss: 3.365595548241227\n",
      "Epoch: 4888, Len of Training loss: 18, Average loss: 1.1455733610524073\n",
      "Len of Validation loss: 54, Average loss: 3.5196048573211387\n",
      "Epoch: 4889, Len of Training loss: 18, Average loss: 1.0427379773722754\n",
      "Len of Validation loss: 54, Average loss: 3.4646916367389538\n",
      "Epoch: 4890, Len of Training loss: 18, Average loss: 1.0008257627487183\n",
      "Len of Validation loss: 54, Average loss: 3.613033257148884\n",
      "Epoch: 4891, Len of Training loss: 18, Average loss: 1.0567553440729778\n",
      "Len of Validation loss: 54, Average loss: 3.4183761885872594\n",
      "Epoch: 4892, Len of Training loss: 18, Average loss: 0.9415371550454034\n",
      "Len of Validation loss: 54, Average loss: 3.5527545037092985\n",
      "Epoch: 4893, Len of Training loss: 18, Average loss: 1.1233489877647824\n",
      "Len of Validation loss: 54, Average loss: 3.630852723563159\n",
      "Epoch: 4894, Len of Training loss: 18, Average loss: 1.2128996716605291\n",
      "Len of Validation loss: 54, Average loss: 3.5269950727621713\n",
      "Epoch: 4895, Len of Training loss: 18, Average loss: 1.0672893987761602\n",
      "Len of Validation loss: 54, Average loss: 3.3184917127644575\n",
      "Epoch: 4896, Len of Training loss: 18, Average loss: 0.9924794137477875\n",
      "Len of Validation loss: 54, Average loss: 3.6079750524626837\n",
      "Epoch: 4897, Len of Training loss: 18, Average loss: 0.9408100214269426\n",
      "Len of Validation loss: 54, Average loss: 3.490589827299118\n",
      "Epoch: 4898, Len of Training loss: 18, Average loss: 0.8620880908436246\n",
      "Len of Validation loss: 54, Average loss: 3.386124813998187\n",
      "Epoch: 4899, Len of Training loss: 18, Average loss: 0.9090941382779015\n",
      "Len of Validation loss: 54, Average loss: 3.2445457125151598\n",
      "Epoch: 4900, Len of Training loss: 18, Average loss: 0.9018024967776405\n",
      "Len of Validation loss: 54, Average loss: 3.485342522462209\n",
      "Epoch: 4901, Len of Training loss: 18, Average loss: 0.8975047104888492\n",
      "Len of Validation loss: 54, Average loss: 3.6628185543749066\n",
      "Epoch: 4902, Len of Training loss: 18, Average loss: 0.9269284307956696\n",
      "Len of Validation loss: 54, Average loss: 3.2456676728195615\n",
      "Epoch: 4903, Len of Training loss: 18, Average loss: 0.9694125751654307\n",
      "Len of Validation loss: 54, Average loss: 3.2958513189245155\n",
      "Epoch: 4904, Len of Training loss: 18, Average loss: 0.968647708495458\n",
      "Len of Validation loss: 54, Average loss: 3.471115016274982\n",
      "Epoch: 4905, Len of Training loss: 18, Average loss: 0.9392790761258867\n",
      "Len of Validation loss: 54, Average loss: 3.317719788463027\n",
      "Epoch: 4906, Len of Training loss: 18, Average loss: 0.9338216417365603\n",
      "Len of Validation loss: 54, Average loss: 3.3103565076986947\n",
      "Epoch: 4907, Len of Training loss: 18, Average loss: 0.9215278195010291\n",
      "Len of Validation loss: 54, Average loss: 3.3278987805048623\n",
      "Epoch: 4908, Len of Training loss: 18, Average loss: 0.9492185711860657\n",
      "Len of Validation loss: 54, Average loss: 3.4952433583913027\n",
      "Epoch: 4909, Len of Training loss: 18, Average loss: 1.1280484232637618\n",
      "Len of Validation loss: 54, Average loss: 3.511534520873317\n",
      "Epoch: 4910, Len of Training loss: 18, Average loss: 1.1061859130859375\n",
      "Len of Validation loss: 54, Average loss: 3.5624146395259433\n",
      "Epoch: 4911, Len of Training loss: 18, Average loss: 0.9935594730907016\n",
      "Len of Validation loss: 54, Average loss: 3.590253014255453\n",
      "Epoch: 4912, Len of Training loss: 18, Average loss: 0.8781395256519318\n",
      "Len of Validation loss: 54, Average loss: 3.111926778599068\n",
      "Epoch: 4913, Len of Training loss: 18, Average loss: 0.8647869361771477\n",
      "Len of Validation loss: 54, Average loss: 3.5660719330664032\n",
      "Epoch: 4914, Len of Training loss: 18, Average loss: 0.8850423561202155\n",
      "Len of Validation loss: 54, Average loss: 3.4928069799034684\n",
      "Epoch: 4915, Len of Training loss: 18, Average loss: 0.8500812550385793\n",
      "Len of Validation loss: 54, Average loss: 3.360028107961019\n",
      "Epoch: 4916, Len of Training loss: 18, Average loss: 0.8789122237099541\n",
      "Len of Validation loss: 54, Average loss: 3.497345628561797\n",
      "Epoch: 4917, Len of Training loss: 18, Average loss: 0.8307001060909696\n",
      "Len of Validation loss: 54, Average loss: 3.273250452898167\n",
      "Epoch: 4918, Len of Training loss: 18, Average loss: 0.8200681308905283\n",
      "Len of Validation loss: 54, Average loss: 3.3778526098639876\n",
      "Epoch: 4919, Len of Training loss: 18, Average loss: 0.883759296602673\n",
      "Len of Validation loss: 54, Average loss: 3.4401199089156256\n",
      "Epoch: 4920, Len of Training loss: 18, Average loss: 0.9457436005274454\n",
      "Len of Validation loss: 54, Average loss: 3.3858162297142878\n",
      "Epoch: 4921, Len of Training loss: 18, Average loss: 0.9033518731594086\n",
      "Len of Validation loss: 54, Average loss: 3.2162338804315636\n",
      "Epoch: 4922, Len of Training loss: 18, Average loss: 0.9526337153381772\n",
      "Len of Validation loss: 54, Average loss: 3.639650946414029\n",
      "Epoch: 4923, Len of Training loss: 18, Average loss: 0.9816603230105506\n",
      "Len of Validation loss: 54, Average loss: 3.3885173289864152\n",
      "Epoch: 4924, Len of Training loss: 18, Average loss: 0.9820663068029616\n",
      "Len of Validation loss: 54, Average loss: 3.496711846854952\n",
      "Epoch: 4925, Len of Training loss: 18, Average loss: 1.118295477496253\n",
      "Len of Validation loss: 54, Average loss: 3.5095325266873396\n",
      "Epoch: 4926, Len of Training loss: 18, Average loss: 1.057695809337828\n",
      "Len of Validation loss: 54, Average loss: 3.509744762270539\n",
      "Epoch: 4927, Len of Training loss: 18, Average loss: 1.0368468463420868\n",
      "Len of Validation loss: 54, Average loss: 3.4291360124393746\n",
      "Epoch: 4928, Len of Training loss: 18, Average loss: 0.9922630124621921\n",
      "Len of Validation loss: 54, Average loss: 3.686595073452702\n",
      "Epoch: 4929, Len of Training loss: 18, Average loss: 0.9186691873603396\n",
      "Len of Validation loss: 54, Average loss: 3.4482170564156993\n",
      "Epoch: 4930, Len of Training loss: 18, Average loss: 0.8985093434651693\n",
      "Len of Validation loss: 54, Average loss: 3.417187671970438\n",
      "Epoch: 4931, Len of Training loss: 18, Average loss: 0.8496964706314935\n",
      "Len of Validation loss: 54, Average loss: 3.5097579845675715\n",
      "Epoch: 4932, Len of Training loss: 18, Average loss: 0.8973778784275055\n",
      "Len of Validation loss: 54, Average loss: 3.335317294906687\n",
      "Epoch: 4933, Len of Training loss: 18, Average loss: 0.92739447289043\n",
      "Len of Validation loss: 54, Average loss: 3.3878784764696053\n",
      "Epoch: 4934, Len of Training loss: 18, Average loss: 1.0801472432083554\n",
      "Len of Validation loss: 54, Average loss: 3.520236517544146\n",
      "Epoch: 4935, Len of Training loss: 18, Average loss: 1.0338718758689032\n",
      "Len of Validation loss: 54, Average loss: 3.6563368384484893\n",
      "Epoch: 4936, Len of Training loss: 18, Average loss: 1.1584490405188665\n",
      "Len of Validation loss: 54, Average loss: 3.3482198737285755\n",
      "Epoch: 4937, Len of Training loss: 18, Average loss: 1.0969022810459137\n",
      "Len of Validation loss: 54, Average loss: 3.3694475845054344\n",
      "Epoch: 4938, Len of Training loss: 18, Average loss: 1.0314130816194746\n",
      "Len of Validation loss: 54, Average loss: 3.3597232942227966\n",
      "Epoch: 4939, Len of Training loss: 18, Average loss: 0.9778541293409135\n",
      "Len of Validation loss: 54, Average loss: 3.495681866451546\n",
      "Epoch: 4940, Len of Training loss: 18, Average loss: 0.8792667090892792\n",
      "Len of Validation loss: 54, Average loss: 3.361286642374816\n",
      "Epoch: 4941, Len of Training loss: 18, Average loss: 0.9714014099703895\n",
      "Len of Validation loss: 54, Average loss: 3.638450253892828\n",
      "Epoch: 4942, Len of Training loss: 18, Average loss: 0.9902411434385512\n",
      "Len of Validation loss: 54, Average loss: 3.450362369970039\n",
      "Epoch: 4943, Len of Training loss: 18, Average loss: 0.9299467669592963\n",
      "Len of Validation loss: 54, Average loss: 3.3951780045474016\n",
      "Epoch: 4944, Len of Training loss: 18, Average loss: 0.9178644782967038\n",
      "Len of Validation loss: 54, Average loss: 3.3087940083609686\n",
      "Epoch: 4945, Len of Training loss: 18, Average loss: 0.8617651959260305\n",
      "Len of Validation loss: 54, Average loss: 3.633449916486387\n",
      "Epoch: 4946, Len of Training loss: 18, Average loss: 0.9487921728028191\n",
      "Len of Validation loss: 54, Average loss: 3.3653812805811563\n",
      "Epoch: 4947, Len of Training loss: 18, Average loss: 0.9444468352529738\n",
      "Len of Validation loss: 54, Average loss: 3.3108639419078827\n",
      "Epoch: 4948, Len of Training loss: 18, Average loss: 0.9090087016423544\n",
      "Len of Validation loss: 54, Average loss: 3.3310872128716222\n",
      "Epoch: 4949, Len of Training loss: 18, Average loss: 0.8827027380466461\n",
      "Len of Validation loss: 54, Average loss: 3.348905223387259\n",
      "Epoch: 4950, Len of Training loss: 18, Average loss: 0.9183215896288554\n",
      "Len of Validation loss: 54, Average loss: 3.18956791361173\n",
      "Epoch: 4951, Len of Training loss: 18, Average loss: 0.8574514786402384\n",
      "Len of Validation loss: 54, Average loss: 3.330692030765392\n",
      "Epoch: 4952, Len of Training loss: 18, Average loss: 0.8854760030905405\n",
      "Len of Validation loss: 54, Average loss: 3.2637072525642536\n",
      "Epoch: 4953, Len of Training loss: 18, Average loss: 0.9749227298630608\n",
      "Len of Validation loss: 54, Average loss: 3.423412640889486\n",
      "Epoch: 4954, Len of Training loss: 18, Average loss: 0.9375763138135275\n",
      "Len of Validation loss: 54, Average loss: 3.4486545699614064\n",
      "Epoch: 4955, Len of Training loss: 18, Average loss: 1.0223142902056377\n",
      "Len of Validation loss: 54, Average loss: 3.2222148135856346\n",
      "Epoch: 4956, Len of Training loss: 18, Average loss: 1.229304098420673\n",
      "Len of Validation loss: 54, Average loss: 3.2659531467490726\n",
      "Epoch: 4957, Len of Training loss: 18, Average loss: 1.16469782922003\n",
      "Len of Validation loss: 54, Average loss: 3.319036599662569\n",
      "Epoch: 4958, Len of Training loss: 18, Average loss: 0.984189079867469\n",
      "Len of Validation loss: 54, Average loss: 3.491209622886446\n",
      "Epoch: 4959, Len of Training loss: 18, Average loss: 1.0750190483199225\n",
      "Len of Validation loss: 54, Average loss: 3.4443972971704273\n",
      "Epoch: 4960, Len of Training loss: 18, Average loss: 0.976971126264996\n",
      "Len of Validation loss: 54, Average loss: 3.351650795450917\n",
      "Epoch: 4961, Len of Training loss: 18, Average loss: 0.89670819706387\n",
      "Len of Validation loss: 54, Average loss: 3.5707532906973802\n",
      "Epoch: 4962, Len of Training loss: 18, Average loss: 0.913190268807941\n",
      "Len of Validation loss: 54, Average loss: 3.328897522555457\n",
      "Epoch: 4963, Len of Training loss: 18, Average loss: 1.0286053684022691\n",
      "Len of Validation loss: 54, Average loss: 3.2983633864808968\n",
      "Epoch: 4964, Len of Training loss: 18, Average loss: 0.9054868751102023\n",
      "Len of Validation loss: 54, Average loss: 3.408365766207377\n",
      "Epoch: 4965, Len of Training loss: 18, Average loss: 0.8955690662066141\n",
      "Len of Validation loss: 54, Average loss: 3.272775485559746\n",
      "Epoch: 4966, Len of Training loss: 18, Average loss: 0.8856036563714346\n",
      "Len of Validation loss: 54, Average loss: 3.433361189232932\n",
      "Epoch: 4967, Len of Training loss: 18, Average loss: 0.923461871014701\n",
      "Len of Validation loss: 54, Average loss: 3.2584083444542356\n",
      "Epoch: 4968, Len of Training loss: 18, Average loss: 0.8417192200819651\n",
      "Len of Validation loss: 54, Average loss: 3.3781297063386\n",
      "Epoch: 4969, Len of Training loss: 18, Average loss: 0.9548008541266123\n",
      "Len of Validation loss: 54, Average loss: 3.604788886176215\n",
      "Epoch: 4970, Len of Training loss: 18, Average loss: 0.9668444163269467\n",
      "Len of Validation loss: 54, Average loss: 3.472071420263361\n",
      "Epoch: 4971, Len of Training loss: 18, Average loss: 1.1733563972844019\n",
      "Len of Validation loss: 54, Average loss: 3.6909503671858044\n",
      "Epoch: 4972, Len of Training loss: 18, Average loss: 1.2010858654975891\n",
      "Len of Validation loss: 54, Average loss: 3.3133880153850273\n",
      "Epoch: 4973, Len of Training loss: 18, Average loss: 1.1097930901580386\n",
      "Len of Validation loss: 54, Average loss: 3.5157647695806293\n",
      "Epoch: 4974, Len of Training loss: 18, Average loss: 1.000174836979972\n",
      "Len of Validation loss: 54, Average loss: 3.620926949712965\n",
      "Epoch: 4975, Len of Training loss: 18, Average loss: 1.0517176157898374\n",
      "Len of Validation loss: 54, Average loss: 3.3750476395642317\n",
      "Epoch: 4976, Len of Training loss: 18, Average loss: 0.9274412724706862\n",
      "Len of Validation loss: 54, Average loss: 3.4418737634464547\n",
      "Epoch: 4977, Len of Training loss: 18, Average loss: 0.8867768711513944\n",
      "Len of Validation loss: 54, Average loss: 3.449054272086532\n",
      "Epoch: 4978, Len of Training loss: 18, Average loss: 0.9419660965601603\n",
      "Len of Validation loss: 54, Average loss: 3.3522764797563904\n",
      "Epoch: 4979, Len of Training loss: 18, Average loss: 1.0620722671349843\n",
      "Len of Validation loss: 54, Average loss: 3.5212067365646362\n",
      "Epoch: 4980, Len of Training loss: 18, Average loss: 0.9183878898620605\n",
      "Len of Validation loss: 54, Average loss: 3.450509497412929\n",
      "Epoch: 4981, Len of Training loss: 18, Average loss: 0.8872636126147376\n",
      "Len of Validation loss: 54, Average loss: 3.4977178695025266\n",
      "Epoch: 4982, Len of Training loss: 18, Average loss: 0.8378325700759888\n",
      "Len of Validation loss: 54, Average loss: 3.379034161567688\n",
      "Epoch: 4983, Len of Training loss: 18, Average loss: 0.8528945015536414\n",
      "Len of Validation loss: 54, Average loss: 3.377652030300211\n",
      "Epoch: 4984, Len of Training loss: 18, Average loss: 1.0385446416007147\n",
      "Len of Validation loss: 54, Average loss: 3.551625104965987\n",
      "Epoch: 4985, Len of Training loss: 18, Average loss: 1.1177569528420765\n",
      "Len of Validation loss: 54, Average loss: 3.525740944676929\n",
      "Epoch: 4986, Len of Training loss: 18, Average loss: 1.0950861242082384\n",
      "Len of Validation loss: 54, Average loss: 3.6359975194489516\n",
      "Epoch: 4987, Len of Training loss: 18, Average loss: 1.0213079750537872\n",
      "Len of Validation loss: 54, Average loss: 3.446811968529666\n",
      "Epoch: 4988, Len of Training loss: 18, Average loss: 1.009119772248798\n",
      "Len of Validation loss: 54, Average loss: 3.296760141849518\n",
      "Epoch: 4989, Len of Training loss: 18, Average loss: 0.8636681404378679\n",
      "Len of Validation loss: 54, Average loss: 3.3877819743421345\n",
      "Epoch: 4990, Len of Training loss: 18, Average loss: 0.9341503580411276\n",
      "Len of Validation loss: 54, Average loss: 3.4551099980318987\n",
      "Epoch: 4991, Len of Training loss: 18, Average loss: 1.193355358309216\n",
      "Len of Validation loss: 54, Average loss: 3.5685372529206454\n",
      "Epoch: 4992, Len of Training loss: 18, Average loss: 1.0925818516148462\n",
      "Len of Validation loss: 54, Average loss: 3.250679182785529\n",
      "Epoch: 4993, Len of Training loss: 18, Average loss: 0.9790463546911875\n",
      "Len of Validation loss: 54, Average loss: 3.1765282032666384\n",
      "Epoch: 4994, Len of Training loss: 18, Average loss: 1.0003866851329803\n",
      "Len of Validation loss: 54, Average loss: 3.3729418725879103\n",
      "Epoch: 4995, Len of Training loss: 18, Average loss: 0.8986647261513604\n",
      "Len of Validation loss: 54, Average loss: 3.2735814065844924\n",
      "Epoch: 4996, Len of Training loss: 18, Average loss: 0.8968872759077284\n",
      "Len of Validation loss: 54, Average loss: 3.599261243034292\n",
      "Epoch: 4997, Len of Training loss: 18, Average loss: 0.8450954655806223\n",
      "Len of Validation loss: 54, Average loss: 3.3787250805784157\n",
      "Epoch: 4998, Len of Training loss: 18, Average loss: 0.8397279414865706\n",
      "Len of Validation loss: 54, Average loss: 3.4252119218861616\n",
      "Epoch: 4999, Len of Training loss: 18, Average loss: 0.8512518240345849\n",
      "Len of Validation loss: 54, Average loss: 3.3745038796354225\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Metrics recorder per epoch.\n",
    "train_losses = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_losses_corrected = []\n",
    "\n",
    "# Training loop.\n",
    "model.train()\n",
    "for epoch in range(5000):\n",
    "    # Train.\n",
    "    train_epoch_losses = train(train_loader)\n",
    "    print(f\"Epoch: {epoch}, Len of Training loss: {len(train_epoch_losses)}, Average loss: {float(np.sum(train_epoch_losses))/len(train_epoch_losses)}\")\n",
    "    train_losses.append(np.mean(train_epoch_losses))\n",
    "\n",
    "    valid_epoch_losses= evaluate(valid_loader)\n",
    "    print(f\"Len of Validation loss: {len(valid_epoch_losses)}, Average loss: {float(np.sum(valid_epoch_losses))/len(valid_epoch_losses)}\")\n",
    "    valid_losses.append(np.mean(valid_epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "## Metrics recorder per epoch.\n",
    "#train_losses = []\n",
    "#\n",
    "#valid_losses = []\n",
    "#valid_losses_corrected = []\n",
    "#\n",
    "## Training loop.\n",
    "#model.train()\n",
    "#for epoch in range(3000):\n",
    "#    # Train.\n",
    "#    train_epoch_losses = train(train_loader)\n",
    "#    print(f\"Epoch: {epoch}, Len of Training loss: {len(train_epoch_losses)}, Average loss: {float(np.sum(train_epoch_losses))/len(train_epoch_losses)}\")\n",
    "#    train_losses.append(np.mean(train_epoch_losses))\n",
    "#\n",
    "#    valid_epoch_losses= evaluate(valid_loader)\n",
    "#    print(f\"Len of Validation loss: {len(valid_epoch_losses)}, Average loss: {float(np.sum(valid_epoch_losses))/len(valid_epoch_losses)}\")\n",
    "#    valid_losses.append(np.mean(valid_epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "## Metrics recorder per epoch.\n",
    "#train_losses = []\n",
    "#\n",
    "#valid_losses = []\n",
    "#valid_losses_corrected = []\n",
    "#\n",
    "## Training loop.\n",
    "#model.train()\n",
    "#for epoch in range(3000):\n",
    "#    # Train.\n",
    "#    train_epoch_losses = train(train_loader)\n",
    "#    print(f\"Epoch: {epoch}, Len of Training loss: {len(train_epoch_losses)}, Average loss: {float(np.sum(train_epoch_losses))/len(train_epoch_losses)}\")\n",
    "#    train_losses.append(np.mean(train_epoch_losses))\n",
    "#\n",
    "#    valid_epoch_losses= evaluate(valid_loader)\n",
    "#    print(f\"Len of Validation loss: {len(valid_epoch_losses)}, Average loss: {float(np.sum(valid_epoch_losses))/len(valid_epoch_losses)}\")\n",
    "#    valid_losses.append(np.mean(valid_epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:500], label=\"Training loss\")\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:500], label=\"Testing loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:1000], label=\"Training loss\")\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:1000], label=\"Testing loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 3000\n",
    "\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:3000], label=\"Training loss\")\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:3000], label=\"Testing loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:5000], label=\"Training loss\")\n",
    "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:5000], label=\"Testing loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>min_channel_allowed</th>\n",
       "      <th>max_channel_allowed</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>SINR</th>\n",
       "      <th>throughput</th>\n",
       "      <th>average_airtime</th>\n",
       "      <th>average_interference</th>\n",
       "      <th>wlan_code_index</th>\n",
       "      <th>deployment</th>\n",
       "      <th>int_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44482</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-60.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-60.880000</td>\n",
       "      <td>25.11</td>\n",
       "      <td>4.30</td>\n",
       "      <td>42.32250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>232.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16635</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-66.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-66.540000</td>\n",
       "      <td>14.94</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.46750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20040</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-61.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-61.800000</td>\n",
       "      <td>16.91</td>\n",
       "      <td>0.61</td>\n",
       "      <td>7.97500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76988</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-63.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-63.550000</td>\n",
       "      <td>15.95</td>\n",
       "      <td>1.38</td>\n",
       "      <td>16.87625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>584.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44786</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-48.520000</td>\n",
       "      <td>35.54</td>\n",
       "      <td>6.76</td>\n",
       "      <td>20.48875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>233.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87904</th>\n",
       "      <td>48368</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-48.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-48.390000</td>\n",
       "      <td>39.49</td>\n",
       "      <td>15.59</td>\n",
       "      <td>42.73625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>252.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87905</th>\n",
       "      <td>36779</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-64.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-64.900000</td>\n",
       "      <td>24.61</td>\n",
       "      <td>4.22</td>\n",
       "      <td>21.88250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>192.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87906</th>\n",
       "      <td>49773</td>\n",
       "      <td>-97.33</td>\n",
       "      <td>-87.05</td>\n",
       "      <td>-91.31</td>\n",
       "      <td>-104.10</td>\n",
       "      <td>-79.43</td>\n",
       "      <td>-70.58</td>\n",
       "      <td>-82.44</td>\n",
       "      <td>-99.68</td>\n",
       "      <td>-75.93</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-54.071875</td>\n",
       "      <td>29.13</td>\n",
       "      <td>126.83</td>\n",
       "      <td>82.01500</td>\n",
       "      <td>-86.768182</td>\n",
       "      <td>9</td>\n",
       "      <td>259.0</td>\n",
       "      <td>[-97.33, -87.05, -91.31, -104.1, -79.43, -70.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87907</th>\n",
       "      <td>73045</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-60.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>-60.640000</td>\n",
       "      <td>32.34</td>\n",
       "      <td>5.81</td>\n",
       "      <td>84.73500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>526.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87908</th>\n",
       "      <td>41070</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-60.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-60.130000</td>\n",
       "      <td>32.25</td>\n",
       "      <td>1.03</td>\n",
       "      <td>7.11000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>214.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87909 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.2      0      1      2       3      4      5      6      7  \\\n",
       "0             44482   0.00   0.00   0.00  -60.88   0.00   0.00   0.00   0.00   \n",
       "1             16635   0.00   0.00   0.00    0.00   0.00 -66.54   0.00   0.00   \n",
       "2             20040   0.00   0.00   0.00    0.00   0.00   0.00 -61.80   0.00   \n",
       "3             76988   0.00   0.00   0.00    0.00 -63.55   0.00   0.00   0.00   \n",
       "4             44786   0.00   0.00   0.00    0.00   0.00   0.00   0.00   0.00   \n",
       "...             ...    ...    ...    ...     ...    ...    ...    ...    ...   \n",
       "87904         48368   0.00   0.00   0.00  -48.39   0.00   0.00   0.00   0.00   \n",
       "87905         36779   0.00   0.00 -64.90    0.00   0.00   0.00   0.00   0.00   \n",
       "87906         49773 -97.33 -87.05 -91.31 -104.10 -79.43 -70.58 -82.44 -99.68   \n",
       "87907         73045   0.00   0.00   0.00    0.00 -60.64   0.00   0.00   0.00   \n",
       "87908         41070   0.00   0.00   0.00    0.00   0.00   0.00 -60.13   0.00   \n",
       "\n",
       "           8  ...  min_channel_allowed  max_channel_allowed       RSSI   SINR  \\\n",
       "0       0.00  ...                    0                    3 -60.880000  25.11   \n",
       "1       0.00  ...                    0                    3 -66.540000  14.94   \n",
       "2       0.00  ...                    0                    3 -61.800000  16.91   \n",
       "3       0.00  ...                    0                    7 -63.550000  15.95   \n",
       "4       0.00  ...                    0                    7 -48.520000  35.54   \n",
       "...      ...  ...                  ...                  ...        ...    ...   \n",
       "87904   0.00  ...                    0                    7 -48.390000  39.49   \n",
       "87905   0.00  ...                    0                    3 -64.900000  24.61   \n",
       "87906 -75.93  ...                    4                    5 -54.071875  29.13   \n",
       "87907   0.00  ...                    6                    7 -60.640000  32.34   \n",
       "87908   0.00  ...                    0                    1 -60.130000  32.25   \n",
       "\n",
       "       throughput average_airtime  average_interference  wlan_code_index  \\\n",
       "0            4.30        42.32250              0.000000                3   \n",
       "1            0.23         2.46750              0.000000                5   \n",
       "2            0.61         7.97500              0.000000                6   \n",
       "3            1.38        16.87625              0.000000                4   \n",
       "4            6.76        20.48875              0.000000               10   \n",
       "...           ...             ...                   ...              ...   \n",
       "87904       15.59        42.73625              0.000000                3   \n",
       "87905        4.22        21.88250              0.000000                2   \n",
       "87906      126.83        82.01500            -86.768182                9   \n",
       "87907        5.81        84.73500              0.000000                4   \n",
       "87908        1.03         7.11000              0.000000                6   \n",
       "\n",
       "       deployment                                            int_map  \n",
       "0           232.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1            86.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2           104.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3           584.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4           233.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...           ...                                                ...  \n",
       "87904       252.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "87905       192.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "87906       259.0  [-97.33, -87.05, -91.31, -104.1, -79.43, -70.5...  \n",
       "87907       526.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "87908       214.0               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[87909 rows x 30 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\shrey\\\\OneDrive\\\\Documents\\\\.PES\\\\PIL\\\\sta-int_map.csv\")\n",
    "# Shuffle the different deployments in the dataset while keeping all the rows within a particular deployment in thw same order.\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df\n",
    "#groups = [df.sample(frac = 1) for _,df in df.groupby('deployment')]\n",
    "#df = pd.concat(groups)\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Testing loss: 80, Average loss: 3.418294843286276\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Calculate validation losses.\n",
    "        out = model(batch)\n",
    "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
    "\n",
    "        # Metric logging.\n",
    "        losses.append(loss.item())\n",
    "        #rmse.append(rmse_batch.item())\n",
    "        if(i == 79): break\n",
    "    print(f\"Len of Testing loss: {len(losses)}, Average loss: {float(np.sum(losses))/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
